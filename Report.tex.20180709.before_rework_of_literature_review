\documentclass[12pt,oneside,openany,a4paper, %... Layout
english, %... Global lang drivers{}
masters-t, goldenblock]{usthesis} 
\usepackage{float} %for images
\usepackage{graphicx}
\usepackage{cite}
% \usepackage[colorinlistoftodos,prependcaption,textsize=small,disable]{todonotes}
\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes}
\usepackage{pgfgantt}
\usepackage{float} %so that it can use the 'f' float option
\usepackage{pdflscape} %allow you to make landscape pages
\usepackage{amsmath}
\newcommand*\mean[1]{\bar{#1}} % to create an overbar for symbols
% \usepackage{cite}
\usepackage{soul}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{standalone}
\usepackage{subcaption} %for multiple images in one figure
% \usepackage{subfig} %for multiple images in one figure
\usepackage{preview}
\usepackage{mathtools}
\usepackage{pdfpages}
\usepackage{enumitem}
\setlistdepth{9}
% \usepackage[sort&compress]{natbib} %able to cite 2 reference at once

% \usepackage{gensymb} %for degree symbol
% \usepackage{hyperref} %to hyperlink the equations

%packages required by matlab2tikz function
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
% \usepackage{amsmath}


\usepackage{listings} %For code in appendix
\lstset
{ %Formatting for code in appendix
    language=Matlab,
    basicstyle=\footnotesize,
    numbers=left,
    stepnumber=1,
    showstringspaces=false,
    tabsize=1,
    breaklines=true,
    breakatwhitespace=false,
}

\newganttlinktype{drur}{
\ganttsetstartanchor{on bottom=0.75}
\ganttsetendanchor{on left}
\draw [/pgfgantt/link]
% first segment (down)
(\xLeft, \yUpper) --
% second segment (right)
(\xLeft, \yUpper -
\ganttvalueof{link bulge} * \ganttvalueof{y unit chart}) --
% link label
node [pos=.5, /pgfgantt/link label anchor] {\ganttlinklabel}
% third segment (up)
($(\xLeft,
\yUpper -
\ganttvalueof{link bulge} * \ganttvalueof{y unit chart})!%
\ganttvalueof{link mid}!%
(\xRight,
\yUpper -
\ganttvalueof{link bulge} * \ganttvalueof{y unit chart})$) --
% last segment (right again)
($(\xLeft, \yLower)!%
\ganttvalueof{link mid}!%
(\xRight, \yLower)$) --
(\xRight, \yLower);
}

\newganttlinktype{rdldr*}{%
  \draw [/pgfgantt/link]
    (\xLeft, \yUpper) --
    (\xLeft + \ganttvalueof{link bulge 1} * \ganttvalueof{x unit},
      \yUpper) --
    ($(\xLeft + \ganttvalueof{link bulge 1} * \ganttvalueof{x unit},
      \yUpper)!%
      \ganttvalueof{link mid}!%
      (\xLeft + \ganttvalueof{link bulge 1} * \ganttvalueof{x unit},
      \yLower)$) --
    ($(\xRight - \ganttvalueof{link bulge 2} * \ganttvalueof{x unit},
      \yUpper)!%
      \ganttvalueof{link mid}!%
      (\xRight - \ganttvalueof{link bulge 2} * \ganttvalueof{x unit},
      \yLower)$) --
    (\xRight - \ganttvalueof{link bulge 2} * \ganttvalueof{x unit},
      \yLower) --
    (\xRight, \yLower);%
}
\ganttset{
  link bulge 1/.link=/pgfgantt/link bulge,
  link bulge 2/.link=/pgfgantt/link bulge}

\begin{document}
\includepdf{Cover.pdf}
% \includepdf{TitlePage.pdf}
\includepdf{plagiarism_signed.pdf}
% \chapter*{Abstract}
% Digital Image Correlation (DIC) and Digital Volume Correlation (DVC) are becoming widely used tools, in the field of material science, to measure the displacements and deformations of specimens. However the high cost of commercial DIC and DVC software and the limited control offered over the correlation process in both commercial and open-source software limits its widespread adoption. This paper proposes a project which aims to create a Matlab based program capable of performing 2D DIC and DVC on a given set of images while allowing the user control over the correlation process. It is hypothesised that the different DIC algorithms use different methods to perform the same tasks in the correlation process. Thus by analysing these algorithms it is possible to combine these different methods into one program which will allow the user to select which methods to use for the various correlation tasks. If successful, this project will produce a freely available program, which offers many of the correlation methods of the different DIC algorithms, so that users will be able to perform in depth DIC and DVC analyses.

% which allows users full control over the correlation process 

% This project primarily consists of researching different DIC algorithms to determine the various methods of performing correlation and incorporating these methods into the program so that the user can choose which to use.



%  This project is motivated by the lack of freely available DIC software which allows the user to control the correlation process. This software is intended to combine most of the 2D DIC algorithms that are beneficial to material science applications into a single code. The successful completion of this project  will allow researchers of material science a comprehensive tool to determine specimen deformation from images taken of the specimen.

\tableofcontents
\listoffigures
\chapter{Introduction}
Digital Image Correlation (DIC) has become a popular tool in the field of material science to measure the deformations and displacements of test specimens as they are loaded. In order to gain a better understanding of the DIC process this project aims to create code in Matlab that is capable of performing the basic elements of DIC. This includes the calibration process, which is used to relate optical flow between images to metric displacements in the real world, and the correlation process which determines the optical flow between images. The purpose of this report is to present the mathematical basis used within the Matlab codes.

% code that is not a black box so that users build on code so that it suites their specific needs

% briefly explain how dic used in material science to measure material properties and how the method is rigid offering little control.

\section{Project background}
% Material science is a field of study that focuses mainly on quantifying the characteristics of materials so that these quantified characteristics can be used to predict material behaviour. The material science research group at Stellenbosch University focuses specifically on mechanical properties of materials which relate the deformation of a material to the forces applied to the material.


% Material science is a field of study that focuses mainly on quantifying the characteristics of materials. These quantified characteristics are referred to as material properties and they are used to predict the behaviour of a material when under certain boundary conditions. The material science research group at Stellenbosch University focuses specifically on mechanical properties of materials which relate the deformation of a material to the forces applied to it.


Material science is a field of study that focuses mainly on quantifying the characteristics of materials. These quantified characteristics are referred to as material properties and they are used to predict the behaviour of a material when under certain boundary conditions. The material science research group at Stellenbosch University focuses specifically on mechanical properties of materials. 

These mechanical properties are used in constitutive equations to approximate the response of a material, in terms of deformation, to external stimuli, such as a force, where the material property indicates the degree of response of the material. Thus the constitutive equations mathematically define the material properties such that they can be quantified.

This is important since these constitutive equations and material properties can be used to predict the behaviour of a component when subjected to a external force that the component is expected to withstand during its use. This predicted behaviour can then be used to determine whether the component is susceptible to failure during its intended use case. However the accuracy of this predicted behaviour is dependent on the accuracy of the material properties used.

As such ASTM standards have been developed to accurately and reliably determine material properties. These ASTM standards rely upon using conventional displacement measurement devices which typically determine displacement as the change in distance between two points on the specimen (give example of tension and CT specimen standards). Therefore the ASTM standards place strict requirements on the specimen geometry for two reasons. 

First to ensure that the deformation experienced by the specimen over the portion of the specimen that lies between the measurement points is directly dependent on the material property to be determined. Secondly to either mitigate the effects that other material properties play in this deformation or have them influence the deformation in a way that can be reliably accounted for. This is done so that a specialised constitutive equation which is calibrated to a specific specimen, in terms of how the displacement is measured, and only accounts for one material property can be used to quantify the material property.

 % so that the effects of other material properties are either minimized or can be easily accounted for.under consideration plays the largest role in the deformation of the specimen over the portion of the specimen that lies between the measurement points.


% Therefore the ASTM standards place strict requirements on the specimen geometry so that the material property under consideration plays the largest role in the deformation of the specimen over the portion of the specimen that lies between the measurement points. This is done so that a specialised constitutive equation which is calibrated to a specific specimen, in terms of how the displacement is measured, and only accounts for one material property can be used to quantify the material property.

Consequently a different specimen is required in order to accurately determine each material property. However the use of optical full-field measurement techniques to determine displacement across the full-field of the specimen have been successfully applied within the field of material science (give references examples). Digital Image Correlation (DIC) is one such technique which determines displacements experienced across the surface of the specimen from images taken of the specimen as it deforms.

Since this technique provides displacements across the whole surface of the specimen more complex constitutive equations, that include the effects of multiple material properties, can be used. This allows more than one material property to be determined during a single experiment (ref richard). This also relaxes the rigid requirements that were placed on the specimen geometry when using conventional measurement techniques. Now the geometry need only ensure that the material properties that are to be determined play a significant enough role in the deformation of the specimen.

Additionally since DIC is a non-contact optical measurement technique the specimen can be exposed harsh environments while measurements can still be taken as long as there is a clear view of the specimen. For example DIC can be used to measure the creep of a material that is exposed to high temperatures in an oven while a camera captures images of the specimen through a window in the oven.


% Material properties are determined by applying a force of known magnitude to a specimen of well-defined geometry and measuring the displacement of the specimen at a specific point on the specimen. This displacement and force are then used in a constitutive equation to solve for the material property which relates the two. Thus the accuracy of the material property is dependent on the accuracy of the force and displacement which are used to determine it. Forces are often easy to determine accurately since they are often the independent variable of the experiment.


% Material properties are determined by applying a force of known magnitude to a specimen of well-defined geometry and measuring the displacement of the specimen at a specific point on the specimen. This displacement and force are then used in a constitutive equation to solve for the material property which relates the two. Thus the accuracy of the material property is dependent on the accuracy of the displacement measured.


% Thus the accuracy of the material property is dependent on the accuracy of the force and displacement which are used to determine it. Forces are often easy to determine accurately since they are often the independent variable of the experiment.


% As such ASTM standards have been developed to accurately and reliably determine material properties. Since displacement measurement is 

% These astm standards place strict requirements on the specimen goemetry and the location of where displacement measurements should be made. This is done so that a constitutive equation 

% These ASTM standards rely upon using conventional displacement measurement devices which typically determine displacement as the change in distance between two points on the specimen. Therefore the ASTM standards place strict requirements on the specimen geometry so that the material property under consideration plays the largest role in the deformation of the specimen over the portion of the specimen where displacement is to be measured. This is done so that a more basic constitutive equation that accounts for only one material property can be used to quantify this material property.



% Therefore the ASTM standards place strict requirements on the specimen geometry and the location where displacement is to be measured so that the material property under consideration plays the largest role in the displacement that is to be measured.

% Because these astm standards rely upon using conventional displacement measurement devices 



% Therefore it is the accuracy in measuring the displacement that greatly affects the accuracy of the material property. Conventional displacement measurement devices measure the change in distance between two points on the specimen and not the displacement of a single point on the specimen. Thus strict requirements are placed on the geometry of the test specimen so that these two points are well defined and the change in displacement between these two points is predominantly dependant on the material property that is being measured.

% need to be more concise on how the astm standards place strict requirements due to limited const eq and conventional measurement techniques

% However other techniques that measure displacement across the full-field of the specimen can also be used. These include 

% Since these techniques provide displacements across the whole surface of the specimen more complex constitutive equations, that include the effects of multiple material properties, can be used. This allows more than one material property to be determined during a single experiment (ref richard). This also relaxes the rigid requirements that were placed on the specimen geometry when using conventional measurement techniques. Now the geometry need only ensure that the material properties that are to be determined play a significant enough role in the deformation of the specimen.

% Additionally since DIC is a non-contact optical measurement technique the specimen can be exposed harsh environments while measurements can still be taken as long as there is a clear view of the specimen. For example DIC can be used to measure the creep of a material that is exposed to high temperatures in an oven while a camera captures images of the specimen through a window in the oven.



% Thus strict requirements are placed on the geometry of the test specimen so that these two points are well defined and the change in displacement between these two points is reliably related to the constitutive equation.
% It is for this reason that such strict requirements are placed on the geometry of the test specimen so that the point on the specimen where displacement is to be measured is well-defined.
% However the accuracy of this predicted behaviour is dependent on the accuracy of the material properties used. As such strict standard need to be followed in order to determine material properties using conventional deformation measurement techniques since these measurement techniques 
% which relate the deformation of a material to the forces applied to it using constitutive equations.
% This is important since knowing a components mechanical properties, geometry and expected boundary conditions during use allows one to predict whether the component will fail or not. Additionally the method of failure can be predicted thereby allowing the geometry to be redesign or a better material to be selected 


% other techniques exist for displacement measurement
% give examples and explain why dic so popular
% say how this relaxes the requirements on specimen geometries and allows more than one thing to be measured at once

% what material properties are, why important, how determined

% how dic used measure deformation and get material properties, advantages, examples of where used successfully (ref), determine more than one property in test

% material properties
% full field
% more objective
% non contact
% link to eskom?

% why astm standards need to be strict on geometry


\section{Project motivation}
It is clear that the widespread adoption of this measurement tool in the field of material science is well supported however it still suffers from some shortcomings chief among which is the way in which this software is typically used. Since it is viewed as a measurement tool users treat it as a sort of black box in the same way they would have treated a conventional measurement device. For conventional measurement devices this is fine since these devices are designed to be used in one way and as such are well calibrated for their use case. 

However since DIC involves solving an ill-posed problem which involves complex operations, it can break down in certain situations. For example the method assumes a continuous displacement field over the surface of the specimen in order to simplify the ill-posed problem. Yet there are specimen geometries which result in discontinuous displacement fields such as specimens which contain a crack. In such situations treating the software as a black box can be detrimental.

Violating the assumptions of the method is not the only drawback of treating it as a black box. Over the past 30 years many correlation methods have been proposed and these differ predominantly in the way the correlation problem is set up and what optimization algorithm is used to solve it. The optimization part of the algorithms mostly affect the efficiency of the correlation process whereas how the correlation problem is set up directly affects the results. 

Therefore the correlation problem should be set up in such a way that it is relevant to the displacement field which the specimen is expected to produce. However most software, commercial and open source, allow the user minimal control over setting up the correlation problem. In this way the user is almost forced to treat DIC like a black box. Generally this is not detrimental since most software packages have their correlation problem set up in such a way that it can reliably capture the displacement fields of conventional specimen geometries. Issues however arise when attempting to solve for complex displacement fields caused by unusual specimen geometries.

This wouldn't be such an issue if DIC was simply used to replace conventional measurement devices within the field of material science. However DIC is attractive because it enables other methods of determining material properties to be used, which can determine multiple material properties from one experiment, and these methods can benefit form using unusual specimen geometries. Unusual specimen geometries can be beneficial by ensuring that all the desired material properties play a significant enough role in the deformation of the specimen such that they can be accurately determined.

Thus this project is aimed at developing a DIC algorithm which allows the user more control over the correlation process so that the correlation problem can be set up such that it is appropriate for the expected displacement field.

%  but this negates the advangtages of the method, namely that it is capable of determining multiple material properties from a single experiment.

% However DIC is attractive because it enables other methods of determining material properties to be used which can determine multiple material properties from one experiment and these methods can benefit form using unusual specimen geometries.

% This is not detrimental in most cases since this software has its correlation problem set up in such a way that it is relevant to most general displacement fields.

% Although this is not detrimental in most cases since this software has its correlation problem set up in such a way that it is relevant to most general displacement fields, it becomes important when complex specimens are being used.

% Issues arise when unusual specimen geometries are used which result in more complex displacement fields than can be captured using a general correlation problem setup. 


% Generally this is not detrimental since most software packages have their correlation problem set up in such a way that it can reliably capture the displacement fields of conventional specimen geometries. Issues however arise when attempting to solve for complex displacement fields caused by unusual specimen geometries.

% Generally this is not detrimental since displacement fields resulting from conventional specimen geometries are well suited to the how the correlation problem is set up in the software.

% talk about how setting up the correlation problem affects the results obtained.

% For example DIC typically struggles to deal with discontinuous displacement fields such as in the presence of a crack. This is because the method assumes continuous displacement across the whole surface of the specimen. In such situations treating the software as a black box can be detrimental.

% However since DIC measures full-field displacement it involves much more complex operations which can affect results in certain situations. For example DIC typically struggles to deal with discontinuous displacement fields such as in the presence of a crack. In such situations treating the software as a black box can be detrimental.

% Often this is not the fault of user since most DIC software seldom provides sufficient control over the correlation process and so the user is forced to treat DIC as a black box.

% Biased results is not the only negative of being forced to use DIC as a black box, this way of restricting users from understanding the correlation process limits the ability of users to develop smarter ways of applying DIC 
% It also prohibits users from setting up correlation that is more suited to the displacements that are expected within their experiments.
% Prevents users from being able to implement clever algorithms that solve some of the traditional shortcomings of dic

% This project is aimed at addressing the need for DIC software that provides more control over the correlation process so that more adept users can set up correlation analyses that are more suited to their situation. 


% Although this is fine for general users it can be detremintal wihiht specialised applications

% It is clear that the widespread adoption of this measurement tool in the field of material science is well supported however it still suffers from some shortcomings. Firstly although cameras and computers are becoming more affordable the commercial software required to perform DIC is still very expensive since it is still a niche technology. 

% Chief among which is the way in which this software is typically used. Since it is viewed as a measurement tool users treat it as a sort of black box in the same way they would have treated a conventional measurement device. For conventional measurement devices this is fine since these devices are designed to be used in one way and as such are well calibrated for their use case. 

% However since DIC measures full-field displacement it involves much more complex operations which can affect results in certain situations. For example DIC typically struggles to deal with discontinuous displacement fields such as in the presence of a crack. In such situations treating the software as a black box can be detrimental.

% Often this is not the fault of user since most DIC software seldom provides sufficient control over the correlation process and so the user is forced to treat DIC as a black box.

% Biased results is not the only negative of being forced to use DIC as a black box, this way of restricting users from understanding the correlation process limits the ability of users to develop smarter ways of applying DIC 
% It also prohibits users from setting up correlation that is more suited to the displacements that are expected within their experiments.
% Prevents users from being able to implement clever algorithms that solve some of the traditional shortcomings of dic

% This project is aimed at addressing the need for DIC software that provides more control over the correlation process so that more adept users can set up correlation analyses that are more suited to their situation. 

% However DIC can be used to measure all types of displacement and deformation

% Although the advantages of DIC for material science purposes are clear there are disadvantages

% Current software is reliable but does not allow sufficient control for specific use cases and is treated as a black box tool.

% high cost of software
% cameras and computers becoming progressively more affordable.
% conventional software breaks down at disp disc
% limitted warp func




% black box
% lack control


\section{Objectives}
The aim of this project is to create a DIC algorithm which provides the user with more control over setting up the correlation process. In order to do so the following key objectives need to be achieved.
\begin{itemize}
\item Complete a comprehensive literature review on gradient-descent, subset-based DIC.
\item Use literature review to develop a basic flow diagram of the steps involved in the correlation process.
\item Develop basic Newton-Raphson and Lucas-Kanade DIC algorithms that are capable of determining full-field displacements across the surface of a specimen from images taken of the specimen.
\item Modify the Lucas-Kanade algorithm so that is it more modular allowing the user to set up the correlation problem.
\item Validate this modular algorithm by testing how well it correlates synthetic images of known displacements. Additionally use this to show how setting up the correlation problem for specific displacement fields can be beneficial.
\item Compare the performance of the algorithm to commercial software by correlating experimental image sets and comparing the calculated displacements.
\end{itemize}

\section{Contents}
\begin{enumerate}
\item Introduction
  \begin{enumerate}
    \item Project background
    \item Project motivation
    \item Objectives
  \end{enumerate}
\item Literature review
  \begin{enumerate}
    \item Digital cameras
    \item Camera optics
    \item World to sensor coordinates (give other coordinate systems upon which this is based in the appendices)
    \item Distortion (give ones that are accounted for while others in appendices)
    \item Calibration
      \begin{enumerate}
        \item Inverse problem
        \item Calibration plate
        \item Homography
        \item Estimating homography using direct linear transformation
        \item Absolute conic
        \item Constraints on the intrinsic parameters
        \item Intrinsic parameters and the absolute conic
        \item Distortion in calibration
        \item Non-linear optimisation
      \end{enumerate}
    \item Correlation
    \begin{enumerate}
      \item Correspondence problem and speckle patterns
      \item Correlation process (give flow diagram of correlation process and elaborate on the elements involved)
        \begin{enumerate}
          \item Correlation criteria
          \item Warp function
          \item Interpolation
          \item Optimization
        \end{enumerate}
      \end{enumerate}
  \end{enumerate}

\item Correlation algorithms
  \begin{enumerate}
    \item Newton-raphson
      \begin{enumerate}
        \item Code explanation
      \end{enumerate}
    \item Lucas-Kanade
      \begin{enumerate}
        \item Code explanation
      \end{enumerate}
    \item Phase shift correlation
      \begin{enumerate}
        \item Code explanation
      \end{enumerate}
    \item Subset splitting
      \begin{enumerate}
        \item Code explanation
      \end{enumerate}
  \end{enumerate}
\item Synthetic validation
  \begin{enumerate}
    \item Generating synthetic images
    \begin{enumerate}
      \item Speckle function
      \item Deformed images
      \item Code explanation
    \end{enumerate}
    \item Displacement fields (used to create synthetic images)
    \begin{enumerate}
      \item Plate with hole
      \item Plate with crack
    \end{enumerate}
    \item Results
  \end{enumerate}
\item Experimental validation
  \begin{enumerate}
    \item Specimens
    \begin{enumerate}
      \item Tension specimen with hole
      \item CT specimen
      \item Arcan specimen
    \end{enumerate}
  \item Results
  \end{enumerate}
\item Conclusion
\end{enumerate}

Explain code in a section after the mathematics are presented so it easier to illustrate what is going on (link maths to code)

\chapter{Literature review}
This chapter outlines the theory that is relevant for Digital Image Correlation. First the digital camera is discussed in order to understand how it captures light and stores it as data. Thereafter the optical system, made of a system of lenses and apertures, which focuses the light for the digital camera is reviewed since it has a significant influence on how a three dimensional scene is converted into two dimensional data. Then the camera model that is used to mathematically relate three dimensional coordinates in the real world to two dimensional pixels in the image is presented. Lastly the distortions that are caused by the slight imperfections of lenses are explained. 

\section{Digital cameras}
Cameras at the basic level rely upon using an aperture and a lens to focus light rays that originate from objects onto a plane within the camera called the sensor plane. At the sensor plane exist a Charge-Coupled Device (CCD) that consists of a matrix of light sensors. Each light sensor converts the light incident upon its surface into an electrical charge through the photoelectric effect. The charge is proportional to the light intensity. Then each light sensor's voltage is read by an analogue-to-digital converter which measures the voltage and assigns a digital value to it. These digital values are then stored at the corresponding position in a matrix which forms the digital image.

% Thus for an image of an object to be in focus the light incident upon each light sensor should originate from one point on the object surface. However light is reflected by objects in many directions and so a means of focusing the light is required. This is accomplished through using lenses and an aperture. This is discussed in the next section.

% In order to take an image of an object, such that it is in focus, 

% A point on an object reflects light in all directions and so in order to capture an image of an object, such that it is in focus, the light rays leaving a point on an object must be refocused so that they all intersect at the same point on the sensor plane. 


% If a point on the sensor plane receives light rays from different parts of the object then this will result in a blurry image.
\section{Camera optics}
For an image of an object to be in focus the light incident upon each light sensor should originate from one point on the object's surface. However light is reflected by objects in many directions and so a means of focusing the light is required. This is accomplished through using lenses and an aperture. Throughout this project thin lenses are assumed. A thin lens is one in which its thickness is negligible in comparison with its focal length or radius of curvature \cite{sutton2009image}. Additionally the paraxial approximation is assumed which states that light rays passing through the lens do so with a small angle to the lens's optical axis and pass through the lens close to the optical axis. This leads to the small angle approximation.
\begin{equation}
  \sin(\theta) \approx \tan(\theta) \approx \theta
\end{equation}

Lenses are usually disk shaped pieces of glass with two convex surfaces. The convex surfaces are designed to bend light towards the optical axis with the degree of bending increasing with the distance from the light to optical axis at the lens mid plane. Thus diffuse light emanating from a point M $[x,y,z]^T$ on an object will pass through the lens and the light rays will converge at a point called the ideal image point M' $[x',y',z']^T$. Thereafter the light rays diverge again to points M'' $[x'', y'', z'']^T$ as shown in Figure \ref{fig:optics}. If the sensor plane is coincident with the ideal image points of the light rays the image that forms on the sensor is inverted due to the way the lens bends the light. As a result an inverted coordinate system is used for the sensor plane.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{Optics}
    \caption{Illustration of how light rays are manipulated by the lens}
    \label{fig:optics}
\end{figure}

% Due to the way the lens bends the light rays the image formed if the sensor plane is coincident with the resulting in an inverted image M'' (x'',y'',z'') if the sensor plane is behind the ideal image point. Since this is the most common camera configuration an inverted coordinate system is used for the sensor plane.

The thin lens equation can be stated as
\begin{equation}
\frac{1}{|CM|} + \frac{1}{CM'} = \frac{1}{\mean{f}}
\end{equation}
where $\mean{f}$ is the focal length; an inherent property of the lens. Additionally through similarity of triangles we have
\begin{align}
\frac{y}{CM} = \frac{-y'}{CM'}\\
\frac{x}{CM} = \frac{-x'}{CM'}\\
\frac{z}{CM} = \frac{-z'}{CM'}.
\end{align}
Combining these a series of equations for the ideal image point can be obtained.
\begin{align}
\label{eq:optics y'2y}
\begin{split}
x'&=\frac{-\mean{f} x}{z - \mean{f}} \\
y'&=\frac{-\mean{f} y}{z - \mean{f}} \\
z'&=\frac{-\mean{f} z}{z - \mean{f}}
\end{split}
\end{align}

\subsection{Circle of confusion}
It is impossible to align the sensor plane perfectly with the ideal image points and so the divergence of the light rays after the ideal image point causes the light rays to illuminate a circular area on the sensor plane. This blurred region is known as the circle of confusion. If this circular area is larger than the area spanned by an element of the sensor then this will result in blurring in the image as the light spills over multiple sensors. This blur could be eliminated by having the sensor plane the same distance from the lens as the the ideal image point but different points on the object will have different ideal image points.

The light rays that make up the outer perimeter of the circle of confusion are the rays that pass through the lens on the outer edges. Using these rays it is possible to determine the radius of the circle of confusion. Taking $r$ to be the radius of the lens these outer light rays pass through the midplane of the lens along the perimeter of a circle described by $[r \cos \beta, r \sin \beta, 0]^T$ where $0 \leq \beta \leq 2\pi$. Here $\beta$ is the angle subtended by the radius and the x axis that lies in plane with the lens. Using trigonometry the x and y components of these light rays can be related as
\begin{align}
  \frac{x' - r \cos \beta}{z'} &= \frac{x' - x''}{\gamma + z'} \\
  \frac{y' - r \sin \beta}{z'} &= \frac{y' - y''}{\gamma + z'} 
\end{align}
The distance between the midplane of the lens and the sensor plane is given by $\gamma$. Using these and Equation \ref{eq:optics y'2y} an expression for the perimeter of the circle of confusion can be found.
\begin{equation}
\label{eq:cof}
  \begin{bmatrix}
  x'' \\
  y'' \\
  z''
  \end{bmatrix} =
  \begin{bmatrix}
  r \cos \beta + r \cos \beta \left( 1 + \frac{\gamma \left( \mean{f} - z \right)}{\mean{f} z} \right) \\
  r \sin \beta + r \sin \beta \left( 1 + \frac{\gamma \left( \mean{f} - z \right)}{\mean{f} z} \right) \\
  0
  \end{bmatrix} +
  \begin{bmatrix}
  -\frac{\gamma x}{z} \\
  -\frac{\gamma y}{z} \\
  - \gamma
  \end{bmatrix}
\end{equation}
The radius of the circle of confusion can be determine by taking the difference in the y components of $y''$ for $\beta=90 ^{\circ} $ and $\beta=270 ^{\circ}$.

\begin{align}
  2 r_{cof} &= \left[r \sin 90 ^{\circ} \left( 2 + \frac{\gamma \left( \mean{f} - z \right)}{\mean{f} z} \right) \right] - \left[r \sin 270 ^{\circ} \left( 2 + \frac{\gamma \left( \mean{f} - z \right)}{\mean{f} z} \right) \right] \\
  r_{cof}&=r \left( 1 + \frac{\gamma \left( \mean{f} - z \right)}{\mean{f}z} \right)
\end{align}

Another way that the blur can be improved is by using an aperture. An aperture is at a basic level an opaque diaphragm with a hole in it which serves to reduce the amount of light that is incident upon the sensor. Thus by blocking off the light that passed through the outer edges of the lens the aperture reduces the size of the circle of confusion by effectively reducing the radius of the lens. The aperture can be place in front or behind the lens. 

\subsection{Depth of field}
Depth of field is defined as the distance ahead and behind the object that is in focus. For a particular camera system the ideal image point of an object will fall upon the sensor plane if the object is at an ideal focal length from the lens. If the object is any closer or further from the lens it will cause a circle of confusion on the sensor plane as opposed to a point. If the circle of confusion is small enough it will not spill significant light over multiple sensors which will result in a clear image.% be interpreted as a point by the light sensor in which case it is clear in the image.

However if the circle of confusion is large enough it will cause blurring in the image. The largest circle of confusion which still results in a sharp image is referred to as the acceptable circle of confusion \cite{sutton2009image}. Thus the depth of field can be considered as the distance along the z-axis ahead or behind the ideal focus length which results in an acceptable circle of confusion.

The largest degree of freedom for a specific camera system occurs when the object is at the hyperfocal length, $H$, from the camera. The hyperfocal length is defined as the closest an object can be to the camera such that the depth of field extends to infinity behind the object. In this situation the depth of field starts at a distance of $\frac{H}{2}$. The hyperfocal length can be approximated as
\begin{equation}
  H \simeq \frac{\mean{f}^2}{2 N r_{cof}}
  \label{eq:hyperfocal}
\end{equation}
Here $N$ is given by $\frac{\mean{f}}{D_p}$, where $D_p$ is the diameter of the entrance pupil for the camera system. Letting $s$ be the distance from the camera to the object such that the camera is ideally focused at a distance $s$. The distance from the camera to the near limit of the depth of field, $D_N$, and the distance from the camera to the far limit of the depth of field, $D_F$, can be approximated.
\begin{align}
  D_N \simeq \frac{Hs}{H+s} \\
  D_F \simeq \frac{Hs}{H-s}
\end{align}
These approximations assume that the object distance is large compared to the lens focal length. The depth of field can then be determined.
\begin{equation}
  DOF = D_F - D_N = \frac{2 H s^2}{H^2 - s^2}
  \label{eq:DOF}
\end{equation}
Combining Equation \ref{eq:hyperfocal} and \ref{eq:DOF} 
\begin{equation}
  DOF = \frac{4 N r_{cof} \mean{f}^2 s^2}{\mean{f}^4 - 4 N^2 r_{cof}^2 s^2}.
\end{equation}
Thus it is clear that the depth of field can be controlled by altering the focal length of the lens. This can be done by either changing the size of the aperture $D_p$ or by changing the distance between the camera and the object.

\subsection{Field of view}
\todo{viewing frustum}
The field of view is the extent of the world that the camera is capable of capturing in an image. It is quantified as the largest angle that a light ray, that is incident upon the sensor, makes with the optical axis. This angle is referred to as the angle-of-view.

Using the pinhole camera model with a distance of $L$ \todo{need to illustrate these dimensions in an image} between the sensor and the lens and taking the lens height to be $d$ a relation for the angle-of-view, $\alpha$, can be derived using trigonometry.
\begin{align}
  \tan \left( \frac{\alpha}{2} \right) &= \frac{d}{2 L} \\
  \alpha &= 2 \arctan \left( \frac{d}{2L} \right)
\end{align}

However for the best picture quality the distance between the sensor and the lens should be equal to the focal length $f$.
\begin{equation}
  \alpha = 2 \arctan \left( \frac{d}{2f} \right)
\end{equation}

\subsection{Transformation to image plane}
The relation between an object point and the projection of the object point onto the sensor can be derived from Equation \ref{eq:cof} by eliminating the circle of confusion component. Additionally when the camera system is set up properly $z'$ will be equal to $\gamma$ so that the sensor plane is approximately coincident with the ideal image points. 
\begin{equation}
\label{eq:w2i}
  \begin{bmatrix}
  x'' \\
  y'' \\
  z''
  \end{bmatrix} = 
  \begin{bmatrix}
  -\frac{\gamma}{z} & 0 & 0 \\
  0 & -\frac{\gamma}{z} & 0 \\
  0 & 0 & -\gamma
  \end{bmatrix}
  \begin{bmatrix}
  x \\
  y \\
  1
  \end{bmatrix}
\end{equation}
This equation presents two issues. Firstly the dependence of the sensor positions on $z$. Secondly all the terms in the equation have metric units whereas the image coordinate system has dimensions measured in pixels. These issues are fixed by using a homogeneous from of Equation \ref{eq:w2i}.
\begin{equation}
\label{eq:w2ih}
  \alpha \begin{bmatrix}
  -x'' \\
  -y'' \\
  1
  \end{bmatrix} =
  \begin{bmatrix}
  \gamma & 0 & 0 & 0 \\
  0 & \gamma & 0 & 0 \\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  x \\
  y \\
  z \\
  1
  \end{bmatrix}
\end{equation}
Here $\alpha$ is a scale factor which allows for the conversion from metric units for the world coordinate system to pixels used in the sensor coordinate system. This equation is referred to as perspective projection \cite{sutton2009image}.

\subsection{Front image plane model}
A new imaging model that is often preferred for computer vision applications can be obtained by translating the sensor plane a distance of $2\gamma$ along the optical axis. In this case the sensor plane is in front of the lens. Treating the sensor coordinates $M''$ of an object as the intersection of the light ray with the sensor plane; Equation \ref{eq:w2ih} remains valid for this configuration.

This imaging model is advantageous in that the sensor plane coordinates are no longer inverted. This means that the scene being imaged is also not inverted.

% \begin{itemize}
%   \item intro
%   \item circle of confusion
%   \item M to M''
%   \item aperture
%   \item DOF
%   \item angle of view
%   \item pinhole inversion
% \end{itemize}

% The surfaces are convex such that diffuse light emanating from a point on an object that passes through the lens will converge and intersect at a point called the ideal imaging point. After the ideal imaging point the light rays begin to diverge again. Once this divergence occurs the image becomes inverted. The lens is placed in front of the sensor plane in an attempt to focus the light from each part of the object onto a single light sensor.

% Lenses are usually disk shaped pieces of glass with two convex surfaces. The surfaces are convex such that diffuse light emanating from a point on an object that passes through the lens will converge and intersect at a point called the ideal imaging point. After the ideal imaging point the light rays begin to diverge again. Once this divergence occurs the image becomes inverted. The lens is placed in front of the sensor plane in an attempt to focus the light from each part of the object onto a single light sensor.

% This is illustrated in figure \todo{figure}.

%  which refract light that passes through them to bring the light closer to the usually shaped such that they have convex surfaces that are shaped in such a way that light passing through the lens is refracted towards the optical axis such that  
\section{Coordinate systems}
\label{sec:coord sys}
Images are only capable of storing two dimensional information whereas we live in a three dimensional world. Thus cameras convert three dimensional information from the world coordinate system into two dimensional information in the sensor coordinate system when a picture is taken as shown in Figure \ref{fig:coordsys}. The mathematical relationship between these two coordinate systems is discussed here.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{CoordinateSystems2}
    \caption{The conversion between coordinate systems that occurs when an image is taken \cite{sutton2009image}}
    \label{fig:coordsys}
\end{figure}

\todo{make sure the symbols are the same as used in the text and possibly rework this image (it is pretty original still)}
% Throughout this project thin lenses are assumed. A thin lens is one in which its thickness is negligible in comparison with its focal length or radius of curvature. Additionally the paraxial approximation is assumed which states that light rays passing through the lens do so with a small angle to the lens's optical axis and pass through the lens close to the optical axis. This leads to the small angle approximation.
% \begin{equation}
%   \sin(\theta) \approx \tan(\theta) \approx \theta
% \end{equation}
\todo[inline]{describe how homogeneous coordinates allow n dim vectors to be manipulated in n+1 dim space}
\subsection{Homogeneous coordinates}
It is common knowledge that any object's shape can be fully defined using distances and angles in 3D Euclidean space. However when an image is taken of this object these distances and angles become distorted. For example railway tracks consist of two beams that remain parallel to one another at a set distance apart in Euclidean space but in an image of the railway track (projective space) these beams appear to get closer and closer to one another as seen in figure \ref{fig:traintrack}. Therefore the parallelism between the beams in Euclidean space is distorted in projective space. This occurs as a result of reducing 3D information to a 2D image. \todo{add image}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.1]{TrainTrack}
    \caption{How projective space distorts parallelism present in Euclidean space}
    \label{fig:traintrack}
\end{figure}

Homogeneous coordinates make it possible to take a 3D coordinate in Euclidean space into a 4D coordinate in projective space. This is necessary since a transformation matrix for 3D coordinates that applies both rotation and translation has 4 columns and so can only be applied to a 4D coordinate. Conversion to homogeneous coordinates involves adding an additional coordinate $w$ to the coordinate vector and setting $w=1$. To convert back from homogeneous coordinates requires dividing the $x$, $y$ and $z$ coordinates by $w$ and then eliminating $w$.

\todo{do a 2D example}
% http://www.songho.ca/math/homogeneous/homogeneous.html
% http://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/
\subsection{World to camera coordinate system}
Converting from the world coordinate system to the camera coordinate system involves rigid transformations of translation $T$ and rotation $R$. The world coordinate system is simply the coordinate system that would be used to classify the position and orientation of objects in the real world. The world coordinate systems orientation and origin is somewhat arbitrary and it is usually classified according to the object to be imaged. 

The camera coordinate system is as the name suggests fixed according to the cameras position and orientation. The camera coordinate system's z axis is the optical axis of the camera. The conversion between the two coordinate systems can be represented as
\begin{equation}
  \bm{X}_c = 
  \begin{bmatrix}
  x_c \\
  y_c \\
  z_c \\
  1
  \end{bmatrix}
  =
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  z_w \\
  1
  \end{bmatrix} = 
  \begin{bmatrix}
  \bm{R} & \bm{T} \\
  \bm{0} & 1
  \end{bmatrix} \bm{X}_w.
\end{equation}
\todo{change T and R to lowercase}
Scaling of dimensions between the two coordinate systems also needs to be taken into account. The parameters in $\bm{R}$ and $\bm{T}$ are referred to as extrinsic parameters since they are not dependent on the camera system's hardware but rather the camera's position and orientation in the world coordinate system.

\subsection{Camera and imaging plane coordinates}
The 3D object defined in the camera coordinate system is converted to its projection on imaging plane. This conversion is from a 3D coordinate system to a 2D coordinate system as given below using homogeneous coordinates.
\begin{equation}
\label{eq:c2i}
  \bm{X}_p = \alpha
  \begin{bmatrix}
  x_p \\
  y_p \\
  1
  \end{bmatrix} 
  = 
  \begin{bmatrix}
  \gamma & 0 & 0 & 0 \\
  0 & \gamma & 0 & 0 \\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  x_c \\
  y_c \\
  z_c \\
  1
  \end{bmatrix}
\end{equation}
The variable $\alpha$ is an arbitrary scale factor.

\subsection{Image plane and sensor coordinates}
At this point the locations of where the light rays originating from the object will intersect the imaging plane are known. Now it is necessary to mathematically represent how the sensor would interpret these light rays incident upon the sensor into the form of pixels. In order to do this a relation between the position of a point within a coordinate system and the pixel within an image is necessary. Additionally the sensor array is not guaranteed to the orthogonal and so a skewed coordinate system must be taken into account.

The transformation from the image plane coordinate system to a temporary skewed coordinate system with an angle of $\phi$ between the two axes can be represented by
\begin{equation}
\label{eq:plane2skew}
  \begin{bmatrix}
  x_{temp} \\
  y_{temp} 
  \end{bmatrix} =
  \begin{bmatrix}
  1 & -\cot \phi \\
  0 & \frac{1}{\sin \phi}
  \end{bmatrix}
  \begin{bmatrix}
  x_p\\
  y_p
  \end{bmatrix}.
\end{equation}
It is assumed that the two principle directions in the sensor coordinate system have different scale factors, $S_x$ and $S_y$,which have units of pixels per unit length. Applying these to the coordinates calculated in equation \ref{eq:plane2skew} and accounting for the translations $\hat c_x$ and $\hat c_y$ to convert to the origin of the sensor coordinate system results in the sensor coordinates below.
\begin{equation}
  \begin{bmatrix}
  x_s \\
  y_s
  \end{bmatrix} = 
  \begin{bmatrix}
  S_x & 0\\
  0 & S_y
  \end{bmatrix}
  \begin{bmatrix}
  x_{temp}\\
  y_{temp}
  \end{bmatrix} -
  \begin{bmatrix}
  S_x \hat c_x - S_x \hat c_y \cot \phi \\
  \frac{S_y \hat c_y}{\sin \phi}
  \end{bmatrix}=
  \begin{bmatrix}
  S_x & -S_x \cot \phi \\
  0 & \frac{S_y}{\sin \phi}
  \end{bmatrix}
  \begin{bmatrix}
  x_p \\
  y_p
  \end{bmatrix} -
  \begin{bmatrix}
  S_x \hat c_x - S_x \hat c_y \cot \phi \\
  \frac{S_y \hat c_y}{\sin \phi}
  \end{bmatrix}
\end{equation}

This can be rewritten in homogeneous form so that it is consistent with equation \ref{eq:c2i}.
\begin{equation}
  \bm{X}_s = 
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} =
  \begin{bmatrix}
  S_x & -S_x \cot \phi & -S_x \left( \hat c_x - \hat c_y \cot \phi \right) \\
  0 & \frac{S_y}{\sin \phi} & -\frac{S_y \hat c_y}{\sin \phi} \\
  0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_p \\
  y_p \\
  1
  \end{bmatrix} =
  \bm{A} \bm{X}_i
\end{equation}
\todo{talk about these parameters being intrinsic and combine them better}

\subsection{World to sensor coordinates}
The conversions between coordinate systems described in the previous sections can be combined into one conversion from the world coordinate system to the sensor coordinate system.
\begin{equation}
  \bm{X}_s = \alpha
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} =
  \begin{bmatrix}
  S_x & -S_x \cot \phi & -S_x \left( \hat c_x - \hat c_y \cot \phi \right) \\
  0 & \frac{S_y}{\sin \phi} & -\frac{S_y \hat c_y}{\sin \phi} \\
  0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  \gamma & 0 & 0 & 0 \\
  0 & \gamma & 0 & 0 \\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  z_w \\
  1
  \end{bmatrix}
\end{equation}

This can be simplified by combining the first two matrices.
\begin{align}
  \bm{X}_s = \alpha
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} =
  \begin{bmatrix}
  \gamma S_x & -\gamma S_x \cot \phi & -S_x \left( \hat c_x - \hat c_y \cot \phi \right) & 0\\
  0 & \frac{\gamma S_y}{\sin \phi} & -\frac{S_y \hat c_y}{\sin \phi} & 0\\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  z_w \\
  1
  \end{bmatrix}
  \label{eq:world 2 sensor cumbersome}
\end{align}

Replacing the elements of the first matrix in equation \ref{eq:world 2 sensor cumbersome} with single variables for the purposes of simplicity the equation can be rewritten as

\begin{align}
  \alpha
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} &=
  \begin{bmatrix}
  f_x & f_s & c_x & 0\\
  0 & f_y & c_y & 0\\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  z_w \\
  1
  \end{bmatrix} \\
  &= \bm{K} \bm{V} \bm{X}_w.
  \label{eq:world 2 sensor}
\end{align}
Here the parameters relating the world coordinate system to the sensor coordinate system are separated into two matrices. The first matrix $\bm{K}$ contains the intrinsic parameters which are fixed for a specific camera. The second matrix $\bm{V}$ contains the extrinsic parameters. The extrinsic parameters change when the camera's position and orientation relative to the world coordinate system changes.
\todo{explain this better}

\section{Distortion}
Distortion refers to a collection of phenomena that cause the actual image to differ from that of the idealised image expected from the pinhole camera model. This happens because lenses cannot be manufactured and assembled perfectly and so some misalignment and defects exist in the imaging system. Therefore in order to calculate accurate displacement information the distortions must be accounted for and corrected prior to the correlation process.

Distortion can be separated into many types as done below. The overall distortion of the image can then be mathematically represented as the linear combination of the mathematical expressions for the individual types. A brief explanation of each distortion type and the equation that accounts for the distortion it creates is given below.
% https://www.lensrentals.com/blog/2010/10/the-seven-deadly-aberrations/

\subsection{Spherical distortion}
Spherical distortion is when light rays originating from a point on an object intersect at different points on the sensor plane. This is as a result of the lens not having the perfect curvature required for all the light rays to intersect at the same point on the sensor plane. It is assumed to be axis symmetric relative to the axis passing through the centre of the sensor and to be a function of the radial distance.
\begin{equation}
  \bm{D} = \kappa_1 \rho ^4 \bm{e}_r
\end{equation}
Here $\bm{e}_r$ is the radial unit vector and $\rho$ is the distance from the origin of the sensor coordinate system to the point under consideration.

\subsection{Coma distortion}
Coma distortion affects light rays that travel towards the lens at an angle to the optical axis. Light rays that go through the centre portion of lens refocus to a point on the sensor plane, whereas light rays that pass through the outer portion of the lens don't refocus fully and intersect the sensor plane further (positive coma) or closer (negative coma) to the optical axis. This results in light from a point on an object creating a comet-like shape on the sensor plane. It is corrected with the following equation.
\begin{equation}
  \bm{D} = \kappa_2 \rho^3 \cos \left( \mean{\theta} - \mean{\theta_c} \right) \bm{e}_r
\end{equation}
Here $\mean{\theta_c}$ is the orientation of the projected lens tilt angle in the sensor plane.

\subsection{Astigmatism}
Astigmatism is caused by a lens having curvature that varies when measured along perpendicular planes. For instance the vertical plane of the lens has a different curvature to the horizontal plane of the lens. The different curvatures have different focal lengths and so light passing through the vertical portion of the lens will focus at a different distance from the lens than light that passes through the horizontal portion of the lens. This results in line like blurs on the sensor plane for the curvature that is not in focus. The degree to which the astigmatism affects the image increases further away from the centre of the image.
\begin{equation}
  \bm{D} = \kappa_3 \rho^2 \cos \left( \mean{\theta} - \mean{\theta}_A \right) \bm{e}_r
\end{equation}
Where $\mean{\theta}_A$ is the orientation of the projected astigmatic plane within the sensor plane.

\subsection{Curvature of field}
Curvature of field is a distortion that results due to the curved nature of the optical elements such as the lens. As light rays pass through the lens with a small angle to the optical axis they refocus at the sensor plane. However light rays that pass through the lens at a larger angle to the optical axis refocus at a point that is closer to the lens. This results in the light rays refocusing on a curved plane much like a shallow dome. Since the sensor is planar (flat) this causes the centre of the image to be in focus while the edges of the image are not in focus.

Curvature of field is assumed to be symmetric with respect to the optical axis and to be a quadratic function of the radial position \cite{sutton2009image}.
\begin{equation}
  \bm{D}=\kappa_4 \rho^2 \bm{e}_r
\end{equation}
Here $\kappa_4$ is the amplitude of the curvature of distortion in the sensor plane measured in $\text{pixels}^{-1}$.
% https://photographylife.com/what-is-field-curvature

\subsection{Linear}
This non-symmetric distortion component is assumed to be a linear function of radial position and is dependent on the angular position \cite{sutton2009image}.
\begin{equation}
  \bm{D} = \kappa_5 \rho \cos \left( \mean{\theta} - \mean{\theta_L} \right) \bm{e}_r
\end{equation}
Here $\mean{\theta_L}$ is the angular orientation of the linear distortion axis.

\subsection{Radial}
Radial distortion is caused by the lens having different magnification levels based on the angle of the light rays to the optical axis. As a result the image can experience a decrease in magnification with increasing distance from the optical axis (barrel distortion) or the image can experience increasing magnification with increasing distance from the optical axis (pincushion distortion). This distortion is symmetric with respect to the optical axis.
\begin{equation}
  \bm{D} = \kappa_6 \rho ^3 \bm{e}_r + \kappa_7 \rho^5 \bm{e}_r + \kappa_8 \rho^7 \bm{e}_r
\end{equation}
% http://www.shariblog.com/2013/08/difference-between-barrel-pincushion-distortion/
% https://photographylife.com/what-is-distortion
\subsection{De-centering}
De-centering distortion is caused by the lens not being in perfect alignment with the rest of the camera system. Usually this type of distortion is less severe than radial or spherical distortions.
\begin{equation}
  \bm{D} = \kappa_9 \rho^2 \left[ 3 \sin \left( \mean{\theta} - \mean{\theta_d} \right) \bm{e}_r + \cos \left( \mean{\theta} - \mean{\theta_d} \right) \bm{e}_t \right]
\end{equation}
Here $\bm{e}_t$ is the tangential unit vector and $\mean{\theta_d}$ is the orientation of the axis for maximum tangential distortion.

% http://www.pcigeomatics.com/geomatica-help/concepts/orthoengine_c/Chapter_47.html

















\chapter{Calibration}
Calibration is a necessary process that must be completed in order to extract metric information from images. The calibration process solves for parameters that define the optical characteristics of the camera, parameters that define the orientation and position of the camera coordinate system to the world coordinate system and parameters that define the distortions that must be corrected for in the images. By determining all of these parameters the calibration process will have solved the camera model that predicts how an object in three dimensional space is related to the pixel, which containing the light reflected by the object, in the image captured of the object.

The parameters that define the optical characteristics and the distortions are referred to as intrinsic parameters since they are fixed for a specific camera. The parameters that describe the orientation and position of the camera within the world coordinate system are extrinsic parameters because they change if the camera is moved. \todo{explain intrinsic and extrinsic better}

Multiple methods of calibration exist however calibration using a calibration plate is used in this project since it is one of the most popular methods and manufacturing a calibration plate is relatively simple and inexpensive.

\section{Inverse problem}
Calibration is essentially a method of finding the parameters that allow 3D coordinates in the world coordinate system to be accurately related to 2D coordinates in the image coordinate system. Thus the inputs, 3D world coordinates, and outputs, 2D image coordinates, needed to be used to solve for the parameters which describe the relationship between the two which constitutes an inverse problem.

Inverse problems are typlically hard to solve and calibration is no exception. In order to solve for the calibration parameters more reliably the camera model that relates world points to sensor points is broken down to the simple pinhole camera model initially in order to solve for as few parameters as possible in the beginning. These parameters are solved for using a closed form solution which gives good estimates to the parameters. Thereafter once these parameters have been estimated the camera model is made more complex by introducing radial distortion in order to account for imperfections in the lens system of the camera. These radial parameters are first estimated and now with each parameter having a corresponding estimate all the parameters are optimized simultaneously in an iterative manner.

This inverse problem is sensitive to errors in the 3D world coordinates (inputs) and 2D image coordinates (outputs) used. Thus these need to be known to a high degree of accuracy in order to solve for the intrinsic and extrinsic camera parameters reliably. This is achieved by using a calibration plate.

\section{Calibration plate}
A calibration plate is an object with a flat surface containing a high-contrast, regular pattern. The pattern is such that it contains definitive, point-like features which can be located to a high degree of accuracy within images taken of it. For example a checker board pattern allows for accurate calculation of the points at the corners of the squares. Thus the coordinates of these point-like features on the calibration plate will be known (inputs) and the coordinates of these point-like features in the image can be determined to a high degree of accuracy (outputs). These definitive, point-like features are hereafter referred to as calibration targets.

Since images inherently contain some level of noise it is best to have an overdetermined system of equations. This is accomplished by taking multiple images of the calibration plate and changing the relative position and orientation between the calibration plate and the camera for each image. This effectively reduces the effects of noise; making the system more robust.

\section{Homography}
\label{sec: homography}
Homography is a transformation that can be applied to points on a plane to bring it into alignment with another plane. It is used to bring the calibration targets on the calibration plate in the world coordinate system into alignment with their location in the image in the sensor coordinate system. The transformation from world coordinates to sensor coordinates in equation \ref{eq:world 2 sensor} is a type of homography.

Treating the calibration plate such that it lies in the x-y plane of the world coordinate system; the homography, $\bm{H}$, for calibration is given by the following equation.
\begin{equation}
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} = 
  \alpha
  \begin{bmatrix}
  f_x & f_s & c_x & 0\\
  0 & f_y & c_y & 0\\
  0 & 0 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  0 \\
  1
  \end{bmatrix} \\
\end{equation}
Since the calibration targets are set to lie on the x-y plane within the world coordinate system this eqaution can be reduced to 
\begin{align}
  \begin{bmatrix}
  x_s \\
  y_s \\
  1
  \end{bmatrix} &=
  \alpha
  \begin{bmatrix}
  f_x & f_s & c_x \\
  0 & f_y & c_y \\
  0 & 0 & 1 
  \end{bmatrix}
  \begin{bmatrix}
  r_{11} & r_{12} & t_1 \\
  r_{21} & r_{22} & t_2 \\
  r_{31} & r_{32} & t_3 
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  1
  \end{bmatrix} \\
  &= \alpha \bm{K} 
  \begin{bmatrix}
  \bm{r}_1 & \bm{r}_2 & \bm{t}
  \end{bmatrix}
  \begin{bmatrix}
  x_w \\
  y_w \\
  1
  \end{bmatrix} \\
  &= \alpha
  \begin{bmatrix}
  h_{11} & h_{12} & h_{13} \\
  h_{21} & h_{22} & h_{23} \\
  h_{31} & h_{32} & h_{33}
  \end{bmatrix} 
  \begin{bmatrix}
  x_w \\
  y_w \\
  1
  \end{bmatrix}
  =\alpha \bm{H} 
  \begin{bmatrix}
  x_w \\
  y_w \\
  1
  \end{bmatrix} \label{eq: homography 1}
\end{align}
Here $\bm{r}_1$ and $\bm{r}_2$ are the first and second columns of the rotation matrix. It is clear that the homography matrix contains both intrinsic and extrinsic parameters and thus it is different for each image taken of the calibration plate. Additionally note that the homography matrix is defined up to a scale factor $\alpha$.

%http://www.learnopencv.com/homography-examples-using-opencv-python-c/

\section{Estimating homography with direct linear transformation}
The homographies that relate the world coordinates of the calibration plate targets to the targets in the image can be estimated using direct linear transformation (DLT) \cite{zhangtut}. Equation \ref{eq: homography 1} can be written out as
\begin{align}
  x_s &= \alpha \left( h_{11} x_w + h_{12} y_w + h_{13} \right) \label{eq: homo line 1} \\
  y_s &= \alpha \left( h_{21} x_w + h_{22} y_w + h_{23} \right) \label{eq: homo line 2} \\
  1 &= \alpha \left( h_{31} x_w + h_{32} y_w + h_{33} \right) \label{eq: homo line 3}.
\end{align}
The scale factor, $\alpha$, can be eliminated by dividing equations \ref{eq: homo line 1} and \ref{eq: homo line 2} by \ref{eq: homo line 3} to get
\begin{align}
  x_s \left( h_{31} x_w + h_{32} y_w + h_{33} \right) &= \left( h_{11} x_w + h_{12} y_w + h_{13} \right) \\
  y_s \left( h_{31} x_w + h_{32} y_w + h_{33} \right) &= \left( h_{21} x_w + h_{22} y_w + h_{23} \right)
\end{align}
These equations then reduce to
\begin{align}
  - h_y x_w x_s - h_{32} y_w x_s + h_{11} x_w + h_{12} y_w +h_{13} &= h_{33} x_s \label{eq: solve homo 1}\\
  - h_{31} x_w y_s - h_{32} y_w y_s + h_{21} x_w + h_{22} y_w + h_{23} &= h_{33} y_s. \label{eq: solve homo 2}
\end{align}
In order to avoid the trivial solution where every element in the homogrpahy matrix is equal to zero; constraints need to be placed on the elements of the homography matrix. In this case the element $h_{33}$ is set equal to 1 however other constraints are also possible such as $h_{31}^2 + h_{32}^2 + h_{33}^2 = 1$. Note that if the true value of $h_{33}$ is close to zero then this assumption will introduce a singluarity \cite{emerging}.

Each calibration target, having points $x_{w_i}$ and $y_{w_i}$, on the calibration plate that is captured within an image of the calibration plate, having points $x_{s_i}$ and $y_{s_i}$, provides two relations of the form of equations \ref{eq: solve homo 1} and \ref{eq: solve homo 2}. These equations are then stacked together into an equation of the form
\begin{align}
  \begin{bmatrix}
  x_{w_1} & y_{w_1} & 1 & 0 & 0 & 0 & -x_{w_1} x_{s_1} & -y_{w_1} x_{s_1} \\
  0 & 0 & 0 & x_{w_1} & y_{w_1} & 1 & -x_{w_1} y_{s_1} & -y_{w_1} y_{s_1} \\
  x_{w_2} & y_{w_2} & 1 & 0 & 0 & 0 & -x_{w_2} x_{s_2} & -y_{w_2} x_{s_2} \\
  0 & 0 & 0 & x_{w_2} & y_{w_2} & 1 & -x_{w_2} y_{s_2} & -y_{w_2} y_{s_2} \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots 
  \end{bmatrix}
  \begin{bmatrix}
  h_{11} \\
  h_{12} \\
  h_{13} \\
  h_{21} \\
  h_{22} \\
  h_{23} \\
  h_{31} \\
  h_{32}
  \end{bmatrix} =
  \begin{bmatrix}
  x_{s_1} \\
  y_{s_1} \\
  x_{s_2} \\
  y_{s_2} \\
  \vdots
  \end{bmatrix}
  \label{eq:DLT}
\end{align}
This overdetermined system of equations then can be used to solve for the homography matrices of each calibration image using least squares. This is the first step involved in calibration.

% http://www.kwon3d.com/theory/dlt/dlt.html

\subsection{Direct linear transformation normalisation}
In practice the use of the DLT method to find estimates to the homographies was found to be sensitive to noise in the images. It was discovered by R. Hartley \cite{DLTnorm} that the DLT method can be made much more robust to noise by applying translation and scaling to the image and world coordinate points prior to performing DLT. Specifically the sensor and world coordinate points are first translated such that the origin of each coordinate system is located at the centroid of the points defined in that coordinate system. Then the coordinates are scaled such that the average distance of the coordinates from the origin is equal to $\sqrt{2}$. This scaling and translation has the effect of reducing the condition number of the matrix in equation \ref{eq:DLT}.

The translation and scaling of the coordinates is done separately for the sensor and world coordinates as shown with $\bm{X_{w}'}$ and $\bm{X_{s}'}$ representing the transformed and scaled versions of the world and sensor coordinate points respectively.
\begin{align}
  \bm{X_{w}'} &= \bm{T_w} \bm{X_w} \\
  \bm{X_{s}'} &= \bm{T_s} \bm{X_s}
\end{align}

Since different coordinate points are used for DLT the homography computed, $\bm{H'}$, is not the homography of the original coordinate pairs, $\bm{H}$. However the homography calculated can be transformed back in order to obtain the desired homography matrix.
\begin{equation}
  \bm{H} = \bm{T_{s}^{-1}} \bm{H'} \bm{T_{w}}
\end{equation}

\section{Absolute conic}
\label{sec: abs conic}
As mentioned before two objects that are parallel in Euclidean space appear to intersect each other in projective space. The intersection of these two lines in projective space occurs at a point that lies on the plane at infinity. If a point lies upon the plane at infinity its $w$ is equal to zero in homogeneous coordinates.

The absolute conic lies on the plane at infinity and is defined by the set of points, $\tilde{\bm{x}}_{ac} = [x, y, z, w]^T$, that satisfies the following. 
\begin{align}
  w = 0 \\
  \bm{x}_{ac}^T \bm{x}_{ac}=x^2 + y^2 + z^2 = 0 
  \label{eq:conditions of absolute conic}
\end{align}
Here tilde is used to indicate homogeneous coordinates (projective space) whereas $\bm{x}_{ac} = [x, y, z]^T$ would be the point in Euclidean space.

Thus it is this conic of purely imaginary points that lies upon the plane at infinity. The importance of the absolute conic is that it is invariant under any Euclidean transformations. In other words the relative position of the absolute conic to a moving camera is unaffected by extrinsic parameters. Consider a point $\bm{x}_{ac}$ in Euclidean space in the world coordinate system that lies on the absolute conic. It has homogeneous coordinates $\tilde{\bm{x}}_{ac}=[\bm{x}_{ac}^T, 0]^T$ in projective space. Applying a different rotation and translation to this point to obtain $\tilde{\bm{x}}_{ac}'$, which is equivalent to moving the camera in the world coordinate system, a corresponding point is obtained in the camera coordinate system.
\begin{align}
  \tilde{\bm{x}}_{ac}' &=
  \begin{bmatrix}
  \bm{R} & \bm{T} \\
  \bm{0} & 1
  \end{bmatrix}
  \tilde{\bm{x}}_{ac} =
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1 \\
  r_{21} & r_{22} & r_{23} & t_2 \\
  r_{31} & r_{32} & r_{33} & t_3 \\
  0 & 0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  x_{ac} \\
  y_{ac} \\
  z_{ac} \\
  0
  \end{bmatrix} \\
  &= 
  \begin{bmatrix}
  \bm{R} \bm{x}_{ac} \\
  0
  \end{bmatrix}
\end{align}
It is clear that this point still lies on the plane at infinity since its $w$ is equal to zero. More importantly it can be proven that this point $\bm{x}_{ac}'$ is on the same absolute conic.
\begin{equation}
  \bm{x}_{ac}'^T \bm{x}_{ac}' = \left( \bm{R} \bm{x}_{ac} \right) ^T \left( \bm{R} \bm{x}_{ac} \right) = \bm{x}_{ac}^T \bm{R}^T \bm{R} \bm{x}_{ac} = \bm{x}_{ac}^T \bm{x}_{ac} = 0
\end{equation}
Thus the absolute conic is invariant to Euclidean transformations. Now consider again a point, $\bm{x}_{ac}$, lying on the absolute conic. The corresponding point, $\bm{m}_s$, in the sensor plane is given by
\begin{align}
  \tilde{\bm{m}}_s &= \frac{1}{\alpha} \bm{K} 
  \begin{bmatrix}
  \bm{R} & \bm{T} \\
  \bm{0} & 1
  \end{bmatrix}
  \begin{bmatrix}
  \bm{x}_{ac} \\
  0
  \end{bmatrix} = \frac{1}{\alpha} \bm{K}
  \begin{bmatrix}
  \bm{R} \bm{x}_{ac} \\
  0
  \end{bmatrix} \\
  \therefore \bm{m}_s &= \frac{1}{\alpha} \bm{K} \bm{R} \bm{x}_{ac}.
\end{align}
Checking whether this point satisfies equation \ref{eq:conditions of absolute conic} results in
\begin{equation}
  \bm{m}_s^T \bm{K}^{-T} \bm{K}^{-1} \bm{m}_s = \frac{1}{\alpha^2} \bm{x}_{ac}^T \bm{R}^T \bm{R} \bm{x}_{ac} = \frac{1}{\alpha^2} \bm{x}_{ac}^T \bm{x}_{ac} = 0
\end{equation}
Thus the image of the absolute conic is itself an absolute conic. \todo{absolute or imaginary conic?} The image of the absolute conic is defined by $\bm{K}^{-T} \bm{K}^{-1}$ \cite{luong1997self}. Thus since the image of the absolute conic is dependent only on intrinsic camera parameters it can be used to solve for the intrinsic camera parameters.

% http://www.cs.unc.edu/~marc/tutorial/node87.html
\section{Constraints on intrinsic parameters}
\label{sec: intrinsic constraints}
According to Zhang \cite{emerging} there are two constraints placed upon the intrinsic parameters of the camera. These are important later on in the solving for these intrinsic parameters. The plane of the calibration plate in the camera coordinate system is given by \cite{emerging} 
\begin{equation}
  \begin{bmatrix}
  \bm{r}_3 \\
  \bm{r}_3^T \bm{t}
  \end{bmatrix}^T
  \begin{bmatrix}
  x \\
  y \\
  z \\
  w
  \end{bmatrix} = 0.
\end{equation}
Here $w$ is zero for points on the plane at infinity and one for those that are not. This plane intersects the plane at infinity on a line and it happens that $[\bm{r}_1^T, 0]^T$ and $[\bm{r}_2^T, 0]^T$ are points on this line \cite{emerging}. Thus it is known that any point on this line, $x_c^{\infty}$, is a linear combination of these two points.
\begin{equation}
  x_c^{\infty} = a \begin{bmatrix}
  \bm{r}_1 \\
  0
  \end{bmatrix} + b
  \begin{bmatrix}
  \bm{r}_2 \\
  0
  \end{bmatrix} =
  \begin{bmatrix}
  a \bm{r}_1 + b \bm{r}_2 \\
  0
  \end{bmatrix}
\end{equation}
Now assume that this point, $x_c^{\infty}$, lies on the absolute conic. Then this point must satisfy equation \ref{eq:conditions of absolute conic}. This would require that $a^2+b^2=0$ which results in the solution $b=\pm a i$, where $i=\sqrt{-1}$. As a result it can be seen that the two points along this line intersect the absolute conic at
\begin{equation}
  x_c^{\infty} = a 
  \begin{bmatrix}
  \bm{r}_1 \pm i \bm{r_2} \\
  0
  \end{bmatrix}.
\end{equation}
Since these points lie on the absolute conic they are invariant under Euclidean transformations. Their projection, up to a scale factor, in the sensor coordinate system is given by
\begin{equation}
  x_s^{\infty} = \bm{K} \left( \bm{r}_1 \pm \bm{r}_2 \right) = \bm{h}_1 \pm i \bm{h}_2.
\end{equation}
Here $\bm{h}_i = [h_{1i} \, h_{2i} \, h_{3i}]^T$ is the i\textsuperscript{th} column of the homography matrix. Substituting these points into equation \ref{eq:conditions of absolute conic} results in
\begin{equation}
  \left( \bm{h}_1 \pm i \bm{h}_2 \right)^T \bm{K}^{-T} \bm{K}^{-1} \left( \bm{h}_1 \pm i \bm{h}_2 \right) = 0.
\end{equation}
Requiring both the imaginary and real parts of this equation to equal zero results in two constraints on the intrinsic camera parameters.
\begin{align}
  \label{eq:intrinsic constraints 1}
  \bm{h}_1^T \bm{K}^{-T} \bm{K}^{-1} \bm{h}_2 &= 0 \\
  \label{eq:intrinsic constraints 2}
  \bm{h}_1^T \bm{K}^{-T} \bm{K}^{-1} \bm{h}_1 &= \bm{h}_2^T \bm{K}^{-T} \bm{K}^{-1} \bm{h}_2
\end{align}
% Where $\bm{h}_i = [h_{i1} \, h_{i2} \, h_{i3}]^T$ is the i\textsuperscript{th} column of the homography matrix.

\section{Intrinsic parameters and the absolute conic}
\label{sec: parameters and the absolute conic}
An homography, $\bm{H}$, between the calibration plate and the image of the calibration plate can be estimated. This homography can then be used to solve for the image of the absolute conic. Once the image of the absolute conic is known it can be used to solve for the intrinsic camera parameters. The image of the absolute conic can be represented as
\begin{equation}
  \bm{B} = \bm{K}^{-T} \bm{K}^{-1} = 
  \begin{bmatrix}
  b_{11} & b_{12} & b_{13}\\
  b_{12} & b_{22} & b_{23} \\
  b_{13} & b_{23} & b_{33}
  \end{bmatrix}
\end{equation}
Since $\bm{B}$ is symmetric its contents can be represented by a 6D vector $\bm{b}=[b_{11} \, b_{12} \, b_{22} \, b_{13} \, b_{23} \, b_{33}]^T$. Then
\begin{equation}
  \bm{h}_I^T \bm{B} \bm{h}_j = \bm{v}_{ij} \bm{b}
\end{equation}
where 
\begin{equation}
  \bm{v}_{ij} = [ h_{i1} h_{j1}, h_{i1} h_{j2} + h_{i2} h_{j1}, h_{i2} h_{j2}, h_{i3} h_{j1} + h_{i1} h_{j3}, h_{i3} h_{j2} + h_{i2} h_{j3}, h_{i3} h_{j3}]^T.
\end{equation}
Then equations \ref{eq:intrinsic constraints 1} and \ref{eq:intrinsic constraints 2} can be rewritten as 
\begin{equation}
  \begin{bmatrix} 
  \bm{v}_{12}^T \\
  ( \bm{v}_{11} - \bm{v}_{22})^T
  \end{bmatrix} \bm{b} = \bm{0}
\end{equation}
A separate version of this equation exist for each image taken of the calibration plate and these equations can be stacked, in $\bm{V}$, to give
\begin{equation}
  \bm{V}\bm{b}=\bm{0}.
\end{equation}
The solution for $\bm{b}$, up to a scale factor $\lambda$, is known to be the eigenvector of $\bm{V}^T\bm{V}$ associated with the smallest eigenvalue \cite{emerging}. Once b has been determined it can be used to determine the intrinsic parameters of matrix $\bm{K}$. The relation between $\bm{B}$ and $\bm{K}$ is $\bm{B}=\lambda \bm{K}^{-T} \bm{K}^{-1}$. The intrinsic parameters are determined as follows.
\begin{align}
  c_y &= (b_{12} b_{13} - b_{11} b_{23})/(b_{11} b_{22} - b_{12}^2) \\
  \lambda &= b_{33} -(b_{13}^2 + c_y(b_{12} b_{13} - b_{11} b_{23}))/b_{11}\\
  f_x &= \sqrt{\frac{\lambda}{b_{11}}} \\
  f_y &= \sqrt{\frac{\lambda b_{11}}{b_{11} b_{22} - b_{12}^2}} \\
  f_s &= \frac{-b_{12} f_x^2 f_y}{\lambda} \\
  c_x &= \frac{f_s c_y}{f_y} - \frac{b_{13} f_x^2}{\lambda}
\end{align}
Thereafter the extrinsic parameters can be determined.
\begin{align}
  \bm{r}_1 &= \lambda \bm{K}^{-1} \bm{h}_1 \\
  \bm{r}_2 &= \lambda \bm{K}^{-1} \bm{h}_2 \\
  \bm{r}_3 &= \bm{r}_1 \times \bm{r}_2 \\
  \bm{T} &= \lambda \bm{K}^{-1} \bm{h}_3
\end{align}
At this point it is necessary to incorporate distortions and use them to find better approximations for the intrinsic and extrinsic parameters.

\todo{this section is too close to the paper} %a fleixible new technique and reading 1
% by which the optical characteristics of a camera and the relation between its coordinate system and the world coordinate system are determined. It is performed in order to be able to extract \hl{metric} information from images accurately.

% \section{Closed form solution}
% Sections \ref{sec: homography} through to \ref{sec: parameters and the absolute conic} illustrate that estimates to the intrinsic and extrinsic parameters can be obtained by first determining the homographies for each calibration image, then these can be used to determine the image of the abolute conic which in turn can be used to determine the intrinsic and extrinsic camera parameters. This is closed form solution is the first part of the calibration process

% put inverse problem in calibration plate section

% explain homography and absolute conic

% then explain closed form solution and optimization - give closed form and explain optimization


\section{Distortion in Calibration}
Sections \ref{sec: homography} through \ref{sec: parameters and the absolute conic} outline the framework for the closed form solution of the calibration problem when distortion is not considered. It is only once the closed form solution to the extrinsic and intrinsic parameters is optimised that distortion can be accounted for during the calibration process. As already presented, there are many types of distortion that occur when an image is taken. However it is seldom possible to take all of these distortion types into account since the more distortion parameters introduced into the camera model; the more likely the optimization process becomes unstable.

It has been found \todo{add reference for this} that good calibration results can be achieved by taking only radial distortion into account during calibration \cite{tsai1987versatile,wei1994implicit}. This is beneficial since it is possible to solve for initial guesses to the radial distortion parameters which helps the optimisation process to avoid local minima as a result of the distortion parameters used.

The distortion applied to the points on the image plane is of the form
\begin{align}
  \hat{\bm{X_{p_i}}} =
  \begin{bmatrix}
  \hat{x_{p_i}} \\
  \hat{y_{p_i}}
  \end{bmatrix} &= \left(
  1 + k_1 r^2 + k_2 r^4 \right)
  \begin{bmatrix}
  x_{p_i} \\
  y_{p_i}
  \end{bmatrix}
  = D 
  \begin{bmatrix}
  x_{p_i} \\
  y_{p_i}
  \end{bmatrix}\\
  \text{where} \quad \quad r_i &= \sqrt{x_{p_i}^2 + y_{p_i}^2} \quad \text{and} \quad D=\left(
  1 + k_1 r^2 + k_2 r^4 \right).
\end{align}
Here $\hat{\bm{X_{p_i}}} = \begin{bmatrix} \hat{x_{p_i}} & \hat{y_{p_i}} \end{bmatrix} ^T$ represent the distorted image coordinates.

At this point the intrinsic and extrinsic parameters have been determined using the pinhole camera model and the error between the predicted calibration plate targets and the actual location of these in the images is attributed to radial distortion \cite{zhangtut}. This error is represented as the observed distortion vector $\bm{d_{1_i}}$.
\begin{equation}
  \bm{d_{1_i}} = \bm{X_{s_i}}^{a} - \bm{X_{s_i}}^{c}
\end{equation}
Here $\bm{X_{s_i}}^{a}$ is the actual coordinates observed and $\bm{X_{s_i}}^{c}$ is the predicted coordinates that are calculated according to the camera model.

An undistorted point, $\bm{X_{p_i}}$ is distorted to point $\hat{\bm{X_{p_i}}}$ according to
\begin{align}
  \hat{\bm{X_{p_i}}} &= \bm{U_c} + (\bm{X_{p_i}}-\bm{U_c}) \times (1 + k_1 r_i^2 + k_2 r_i^4) \\
  &= \bm{U_c} + \bm{X_{p_i}} - \bm{U_c} + (\bm{X_{p_i}}-\bm{U_c}) \times (k_1 r_i^2 + k_2 r_i^4) \\
  &= \bm{X_{p_i}} + (\bm{X_{p_i}}-\bm{U_c}) \times (k_1 r_i^2 + k_2 r_i^4) \\
  &= \bm{X_{p_i}} + \bm{d_{2_i}}
\end{align}
Here $\bm{U_c} = \begin{bmatrix} u_c & v_c \end{bmatrix}^T$ is the projection centre or principal point (the intersection of the optical axis with the image plane) and $\bm{d_{2_i}}$ is referred to as the model distortion vector.

The distortion parameters can be estimated by minimizing the difference between the model and observer distortion vectors. Thus the least squares solution to the overdetermined system of equations of the form $\bm{d_{2_i}} = \bm{d_{1_i}}$ is what is needed. For each point $\bm{X_{p_i}}$ considered there are two equations of the form
\begin{align}
  (x_{p_i} - u_c) \times (k_1 r_i^2 + k_2 r_i^4) &= (x_{s_i}^{a} - x_{s_i}^{c}) \\
  (y_{p_i} - v_c) \times (k_1 r_i^2 + k_2 r_i^4) &= (y_{s_i}^{a} - y_{s_i}^{c}).
\end{align}
Two equations of this form are created for each target of the calibration plate and these equations are then stacked into a system of equations which is used to solved for the least squares solution to the distortion parameters.
\begin{equation}
  \begin{bmatrix}
    (x_{p_1} - u_c) \times r_1^2 & (x_{p_1} - u_c) \times r_1^4\\
    (y_{p_1} - v_c) \times r_1^2 & (y_{p_1} - v_c) \times r_1^4\\
    (x_{p_2} - u_c) \times r_2^2 & (x_{p_2} - u_c) \times r_2^4\\
    (y_{p_2} - v_c) \times r_2^2 & (y_{p_2} - v_c) \times r_2^4\\
    \vdots & \vdots
  \end{bmatrix}
  \begin{bmatrix}
    k_1 \\
    k_2
  \end{bmatrix} =
  \begin{bmatrix}
    x_{s_1}^{a} - x_{s_1}^{c}\\
    y_{s_1}^{a} - y_{s_1}^{c}\\
    x_{s_2}^{a} - x_{s_2}^{c}\\
    y_{s_2}^{a} - y_{s_2}^{c}\\
    \vdots
  \end{bmatrix}
\end{equation}

\section{Non-linear optimisation}
At this point good approximations have been calculated for all the calibration parameters using closed form solution methods. However, since the correlation process relies upon the calibration process to link optical flow in images to displacement in the 3D world, these calibration parameters need to be refined so that they don't negatively affect the correlation results. This is done by combining the camera model and distortion into one mathematical relation that related 3D coordinates in the physical world to 2D coordinates in the images. It is important to note that the distortion model is inserted into the camera model prior to converting the image plane coordinates to the sensor plane coordinates. Thus the camera model becomes
\begin{equation}
  \bm{X_s}=\bm{K} D \bm{V} \bm{X_w}.
\end{equation}
This camera model and the approximations of the calibration parameters are employed in a non-linear optimization scheme in order to improve all of the calibration parameters simultaneously. Matlab's built in Levenberg-Marquart non-linear least squares optimisation algorithm was used for this. The sum of the absolute of the difference between the calculated and observed calibration plate targets was used as a measure of the error for the algorithm.

% However these estimates need to be improved upon in order to obtain the true calibration parameter values. This is done by using a non-linear optimisation scheme in order to improve all of the parameters simultaneously. Thus the camera model is combined with the distortion model to obtain one mathematical model of the image capturing system. The Levenberg-Marquart non-linear least squares optimisation algorithm was used to iteratively improve the calibration parameters.

\chapter{Correlation}
The Lucas-Kanade algorithm was used in combination with the zero-mean normalised sum of squared differences correlation criteria. This correlation criteria was chosen since it takes into account offset and scaling in the light intensities of the images making the overall algorithm more robust. The Lucas-Kanade algorithm assumes that the deformation of the image is constant over a area of the image referred to as the subset. As such this algorithm tries to find the deformation field that best corrects a subset of the deformed image so that it appears to be the same as the reference image. 

\section{Correspondence problem and speckle patterns}
Although subsets of pixels are usually easier to track than individual pixels, using subsets leads to a new issue. The correspondence problem is to do with the inability to track displacements as a result of the grayscale patterns within the image. To understand this consider the aperture problem which is a special case of the correspondence problem.

The aperture problem involves the ambiguity in attempting to track the motion of a one-dimensional spatial structure (such as a line) when viewed through an aperture (such as when considering a subset of pixels) that cuts off the view of the ends of the one-dimensional spatial structure \cite{apertureProblem}. This can be illustrated by considering the difficulty in tracking the motion of a line in the direction along the line when the end points of the line are not visible. \todo{image?}

Similarly if the pattern in the image to be tracked is a repeating pattern with a constant grid spacing then the displacement cannot be uniquely determined since the image will contain the same pattern repeated. Thus the algorithm will determine the displacement to be within a multiple of the grid spacing from the actual displacement unless the subset contains a unique feature such as being at the edge of the specimen.

\todo[inline]{deformation and difficulty in tracking edges?}

Thus it is clear that the patterns to be tracked in the images can have a significant impact on the performance of the correlation process. To avoid the aperture problem is is clear that the pattern should be isotropic such that it can be tracked equally well in all directions. Additionally to solve the correspondence problem the pattern should also be highly random and non-periodic so that each subset of an image contains a unique pattern. 

This is achieved by applying a random speckle pattern to the surface of the specimen to be tested since these provide very unique patterns which are isotropic in nature. Additionally the high information density enables smaller subsets to be used than would otherwise have been possible.

\section{Correlation criteria}
A correlation criteria is a mathematical way of quantifying how well a subset in one image matches a subset in another image. It is used to determine whether the warp function parameters that has been solved for are sufficiently accurate to explain how the deformed image is related to the reference image. Furthermore the correlation criteria is usually used as the basis for the iterative solve that performs the correlation process. As such it is important that the correlation criteria reliably indicates the fit between the two subsets.

This reliability refers to dealing with the changes in lighting between images. As images are taken, as the specimen is deformed, the intensity of the images can change as a result of changes in lighting, changes in reflectivity as the material strains and so on. Although this does not cause the patterns in the subsets to change it does cause the grayscale values of individual pixels, that make up the patterns, to change which can negatively affect the correlation process. The changes in lighting that takes place between images has been classified into two types; offset and scaling. 

% The first is offset in light intensities which is dealt with by using zero-mean normalisation. Zero-mean normalisation determines the mean intensity value of a subset and then subtracts that mean intensity from every pixel in that subset.
% \begin{align}
%   G_{i}^{ZN} &= G_i - \mean{G}\\
%   \text{where} \quad \mean{G} &= \frac{\sum_{i}^{n} G_{i}}{n}
% \end{align}

% The second is scaling of the light intensity values between images. This corrected for by dividing the correlation criteria determined by the 

The two most commonly used correlation criteria are the Cross-Correlation and Sum of Squared difference criteria. Each of these has four versions for dealing with various combinations of light intensity changes.

\subsection{Cross-Correlation}
The cross-correlation criteria obtains a coefficient by multiplying each element of the reference subset with its corresponding element in the investigated subset and then summing all of these values together as shown below. Thus a higher value means better correlation.
\begin{equation}
  C_{CC} = \sum_{i}^n F_i G_i
\end{equation}

This correlation criteria can be modified to be capable of dealing with offsets in light intensity by using zero-mean normalisation. Zero-mean normalisation determines the mean intensity value of a subset and then subtracts that mean intensity from every pixel in that subset. This is done for both subsets to obtain the Zero-Mean Cross-Correlation as shown below.
\begin{align}
  C_{ZCC} &= \sum_i^n \left( F_i - \mean{F} \right) \left( G_i - \mean{G} \right) \\
  \text{where} \quad \mean{F} &= \frac{\sum_{i}^{n} F_{i}}{n} \\
   \text{and} \quad \mean{G} &=\frac{\sum_{i}^{n} G_{i}}{n}
\end{align}

The correlation criteria can also be modified to deal with scaling of light intensities by dividing it by the root-sum-square of the subset's intensities. This is referred to as Normalised Cross-Correlation. 
\begin{align}
  C_{NCC} = \frac{\sum_i^n F_i G_i}{\sqrt{\sum_i^n F_i^2 \sum_i^n G_i^2}}
\end{align}

Both of these methods can be combined together to obtain Zero-Mean Normalised Cross-Correlation which is insensitive to both scaling and offset.
\begin{equation}
  C_{ZNCC} = \frac{\sum_i^n (F_i - \mean{F}) (G_i - \mean{G} )}{\sqrt{\sum_i^n (F_i - \mean{F})^2 \sum_i^n (G_i - \mean{G})^2}}
\end{equation}

\subsection{Sum of Squared Difference}
As the name suggests this correlation criteria is based on summing the square of the differences between the reference subset and the investigated subset.
\begin{equation}
  C_{SSD} = \sum_i^n \left( F_i - G_i \right)^2
\end{equation}
Since this correlation criteria measures differences; a smaller value means better correlation between the subsets. This correlation criteria can be modified in the same way to become insensitive to both offset (Zero-Mean Sum of Squared Difference) and scaling (Normalised Sum of Squared Difference).
\begin{align}
  C_{ZSSD} &= \sum_i^n \left[ \left( F_i - \mean{F} \right) - \left( G_i - \mean{G} \right) \right]^2 \\
  C_{NSSD} &= \sum_i^n \left[ \frac{F_i}{\sqrt{\sum_i^n F_i^2}} - \frac{G_i}{\sqrt{\sum_i^n G_i^2}} \right]^2
\end{align}

These can then be combined to obtain the Zero-Mean Normalised Sum of Squared Difference correlation criteria which is insensitive to both scaling and offset in light intensities.
\begin{equation}
  C_{ZNSSD} = \sum_i^n \left[ \frac{F_i - \mean{F}}{\sqrt{\sum_i^n (F_i - \mean{F})^2}} - \frac{G_i - \mean{G}}{\sqrt{\sum_i^n (G_i - \mean{G})^2}} \right]^2
\end{equation}

\subsection{Relation between ZNCC and ZNSSD}
It can be shown that the Zero-Mean Normalised Cross-Correlation and Zero-Mean Normalised Sum of Squared Difference correlation criteria are related.
\begin{align}
  C_{ZNSSD} &= \sum_i^n \left[ \frac{F_i - \mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} - \frac{G_i - \mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right]^2 \\
  &= \sum_i^n \left( \frac{(F_i-\mean{F})^2}{\sum_i^n (F_i-\mean{F})^2} - 2 \frac{(F_i-\mean{F})(G_i-\mean{G})}{\sqrt{\sum_i^n (F_i-\mean{F})^2} \sqrt{\sum_i^n (G_i-\mean{G})^2}} + \frac{(G_i-\mean{G})^2}{\sum_i^n (G_i-\mean{G})^2} \right) \\
  &= \frac{\sum_i^n(F_i-\mean{F})^2}{\sum_i^n (F_i-\mean{F})^2} + \frac{\sum_i^n (G_i-\mean{G})^2}{\sum_i^n (G_i-\mean{G})^2} - 2 \frac{\sum_i^n(F_i-\mean{F})(G_i-\mean{G})}{\sqrt{\sum_i^n (F_i-\mean{F})^2} \sqrt{\sum_i^n (G_i-\mean{G})^2}}\\
  &= 2 \left( 1- C_{ZNCC} \right)
\end{align}
This is significant since the ZNSSD criteria is easier to calculate while the ZNCC criteria value is easier to interpret. Thus it is common practice to calculate the ZNSSD criteria coefficient and convert it to the ZNCC value for easier interpretation.
\todo{slightly different from Czncc, make sure equal}

\section{Warp function}
When applying DIC to to material science the deformation behaviour of materials under load must be taken into account. This is because the deformation of the material causes the speckle pattern on its surface to deform in the same way. This is an issue since the pattern contained within a subset of the reference image will be deformed in the second image which can cause difficulties in matching the two subsets.

This is solved by allowing for the subsets to deform in a similar way to that of the material by defining the allowable deformations in a warp function. The purpose of these warp functions is to transform the pixel coordinates of the reference subset so that the resulting pattern is closer to the pattern in the deformed image. For this analysis the common warp function which accounts for affine transformations is used since it is consistent with the strains that the material is expected to experience. Thus by determining the warp function parameters for a specific subset; the strains for that subset are also determined.
\begin{equation}
  W (\bm{\zeta},\bm{p}) = 
  \begin{bmatrix}
  \Delta x_{warp} \\
  \Delta y_{warp}
  \end{bmatrix} 
  = \begin{bmatrix}
  1+\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} & u\\
  \frac{\partial v}{\partial x} & 1+\frac{\partial v}{\partial y} & v \\
  0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  \Delta x \\
  \Delta y \\
  1
  \end{bmatrix}.
  \label{eq:warp}
\end{equation}
Here $u$ and $v$ represent the displacements in the x and y directions respectively, $\frac{\partial u}{\partial x}$ and $\frac{\partial v}{\partial y}$ represent the elongation in the x and y directions respectively whereas $\frac{\partial v}{\partial x}$ and $\frac{\partial u}{\partial y}$ represent the shear deformation of the subset. All of these variables that explain how the subset is warped and translated are stored within the vector $\bm{p} = \begin{bmatrix} u & \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} & v & \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y} \end{bmatrix}$ which are referred to as the warp function parameters. Additionally $\Delta x$ and $\Delta y$ are the distances from the centre of the undeformed subset to the pixel under consideration and are contained within the variable $\bm{\zeta} = \begin{bmatrix} \Delta x & \Delta y & 1 \end{bmatrix}^T$. The outputs $\Delta x_{warp}$ and $\Delta y_{warp}$ are the modified distances from the centre of the subset to the pixel under consideration. Thus it is the pixel locations within the subset that are modified in order to induce warp and not the subset itself which is warped by this function. In other words the $\Delta x$ and $\Delta y$ are warped by the function which has the effect of warping the subset itself since it changes the locations within the subset that are to be evaluated. Thus the warp function applied to a subset is mathematically expressed as
\begin{equation}
  F_{warped}=F(\bm{x_0}+W(\bm{\zeta},\bm{p}))
  \label{eq:subset warp}
\end{equation}
where $\bm{x_0} = \begin{bmatrix} x_0 & y_0 \end{bmatrix}^T$ are the x and y coordinates, in units of pixels, of the centre of the subset.

With most subset based DIC algorithms the more complex the warp function becomes the more computationally expensive each iteration is. This is because the correlation criteria must be optimized in terms of more variables leading to more work. \todo{keep this?}

\section{Interpolation}
It is clear that warping a subset according to equation \ref{eq:subset warp} requires that the light intensity values between pixel locations can be determined. This is an issue because images store this information in a discrete manner. This is solved by using interpolation to determine the light intensity values between pixels based on the light intensity values of neighbouring pixels. For the purposes of this project Matlab's built in interp2 fucntion was used to perform linear interpolation to determine the light intensity values for non-standard pixel locations where needed.

\todo{need to talk about bicubic}
% For DIC to be useful in the field of material science it must be able to determine displacements to sub-pixel accuracy. This requires knowing the light intensity values, within an image, between pixel locations. This is an issue because images store this information in a discrete manner. Interpolation is used to determine the light intensity values between pixels based on the light intensity values of neighbouring pixels. For the purposes of this project Matlab's built in interp2 fucntion was used to perform linear interpolation to determine the light intensity values for non-standard pixel locations where needed.

\section{Subset matching}
The purpose of DIC is to find the warp function parameters which optimise the correlation criteria between a specific subset, $F$, in the reference image and the investigated subset, $G$, in the deformed image.\todo{check}. Multiple methods of doing this have been proposed; each with their own advantages and disadvantages. 

\subsection{Lucas-Kanade}
Typically the method used to solve the subset matching problem has been to make use of a gradient based optimization algorithm that is applied to the correlation criteria equation in order to iteratively improve upon an initial guess to the warp function parameters. The Lucas-Kanade method is used to illustrate this. It makes use of intensity gradients to guide the search direction to optimise the correlation criteria. A limitation of the method is that it can only achieve good correlation if the initial guess provided for the warp function parameters is reasonably close to the true value otherwise it will converge to a local minimum for the correlation criteria. \todo{change to displacements need to be small?}



% The Lucas-Kanade method is used in this project and it is explained here for the case of correlating a reference subset $F$ to the subset under investigation $G$.

% The Lucas-Kanade method is one of the most widely used subset based DIC techniques. It makes use of intensity gradients to guide the search direction to optimise the correlation criteria. A limitation of the method is that it requires the displacement between the images to be small which is common in material science applications.

The Lucas-Kanade method can be implemented in two distinct ways that differ in how the correlation criteria function is implemented in the Guass-Newton solver. The first, the original way, is referred to as the forward additive method which applies the current estimate of the warp function parameters, $\bm{p}$, and the iterative improvement of the warp function parameters, $\Delta \bm{p}$, to the investigated subset. This deformed subset is then compared with the unchanged reference subset and the difference between the two is used to incrementally improve the warp function parameter estimates. In this method the Hessian matrix is dependent on the investigated subset's light intensity values, $G$, and as such it needs to be computed for each iteration of the solver.

The second method, the inverse compositional method, is a modification to the forward additive method in an effort to decrease the computational cost per iteration. For this method the current estimate for the warp function parameters, $\bm{p}$, is applied to the investigated subset while the iterative improvement to the warp function parameters, $\Delta \bm{p}$, is applied to the reference subset. This is done so that the Hessian that needs to be determined depends only on the reference subset and is independent of the warp function parameters which allows it to be pre-computed outside of the iterations. As a result this method is more efficient which means it takes less time to converge to the solution.

The algorithm explained here is based on the one proposed by B. Pan \cite{panfast}. It is the inverse compositional Lucas-Kanade method using the Guass-Newton approximation to the Hessian matrix with the Zero-Mean Normalised Sum of Squared Difference used as the correlation criterion. The current guess of the warp function parameters is applied to the investigated subset while the iterative improvement of the warp function parameters is applied to the reference subset as shown below.
\begin{equation}
  C_{ZNSSD} = \sum_i^n  \left[ \frac{F(\bm{x_0}+W(\zeta,\Delta \bm{p}))-\mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} -\frac{G(\bm{x_0}+W(\zeta,\bm{p}))-\mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}}\right]^2
  \label{eq:correlation criteria solver}
\end{equation}
Taking the first-order Taylor expansion of equation \ref{eq:correlation criteria solver} in terms of $\Delta \bm{p}$ gives
\begin{equation}
  C_{ZNSSD} = \sum_{\zeta}  \left[ \frac{F(\bm{x_0}+\zeta)+\triangledown F \frac{\partial W}{\partial \bm{p}} \Delta\bm{p}-\mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} -\frac{G(\bm{x_0}+W(\zeta,\bm{p}))-\mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}}\right]^2
  \label{eq:correlation criteria solver taylor expansion}
\end{equation}
where $\triangledown F = \begin{bmatrix} \frac{\partial F(\bm{x_0} + \bm{\zeta})}{\partial x} & \frac{\partial F(\bm{x_0} + \bm{\zeta})}{\partial y} \end{bmatrix}$ is the gradients of the reference subset in the x and y directions and $\frac{\partial W}{\partial \bm{p}} = \begin{bmatrix} 1 & \Delta x & \Delta y & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & \Delta x & \Delta y \end{bmatrix}$ is the Jacobian of the warp function.

Taking the derivative of equation \ref{eq:correlation criteria solver taylor expansion} with respect to $\Delta \bm{p}$ and setting it to zero gives the least squares solution for finding $\Delta \bm{p}$.
% \begin{align}
%   \frac{\partial \chi_{ZNSSD}^2}{\partial \Delta \bm{p}} = 0 = 2 \sum_{\zeta}  \left[ \frac{F(\bm{x_0}+\zeta)+\triangledown F \frac{\partial W}{\partial \bm{p}} \Delta\bm{p}-\mean{F}}{\Delta F} -\frac{G(\bm{x_0}+W(\zeta,\bm{p}))-\mean{G}}{\Delta G}\right] \frac{\triangledown F \frac{\partial W}{\partial \bm{p}}}{\Delta F}\\
%   \Delta \bm{p} = - \sum_{\zeta} \left[ (\triangledown F \frac{\partial W}{\partial \bm{p}})^T (\triangledown F \frac{\partial W}{\partial \bm{p}}) \right]^{-1} \times \sum_{\zeta} \left[ (\triangledown F \frac{\partial W}{\partial \bm{p}})^T \times \left[ (F(\bm{x_0} + \bm{\zeta}) - \mean{F}) - \frac{\Delta F}{\Delta G}(G(\bm{x_0}+W(\bm{\zeta}, \bm{p}))-\mean{G}) \right] \right]\\
%   \Delta \bm{p} = - H^{-1} \times \sum_{\zeta} \left[ (\triangledown F \frac{\partial W}{\partial \bm{p}})^T \times \left[ (F(\bm{x_0} + \bm{\zeta}) - \mean{F}) - \frac{\Delta F}{\Delta G}(G(\bm{x_0}+W(\bm{\zeta}, \bm{p}))-\mean{G}) \right] \right]
% \end{align}

\begin{align}
  \frac{\partial C_{ZNSSD}}{\partial \Delta \bm{p}} &= 0 = 2 \sum_{\zeta}  \left[ \frac{F(\bm{x_0}+\zeta)+\triangledown F \frac{\partial W}{\partial \bm{p}} \Delta\bm{p}-\mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} -\frac{G(\bm{x_0}+W(\zeta,\bm{p}))-\mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}}\right] \frac{\triangledown F \frac{\partial W}{\partial \bm{p}}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}}\\
  \Delta \bm{p} &= - \bm{H}^{-1} \times \sum_{\zeta} \left[ (\triangledown F \frac{\partial W}{\partial \bm{p}})^T \times \left[ (F(\bm{x_0} + \bm{\zeta}) - \mean{F}) - \frac{\sqrt{\sum_i^n (F_i-\mean{F})^2}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}}(G(\bm{x_0}+W(\bm{\zeta}, \bm{p}))-\mean{G}) \right] \right]
  \label{eq:delta p} \\
  \text{where} \qquad \bm{H} &= \sum_{\zeta} \left[ (\triangledown F \frac{\partial W}{\partial \bm{p}})^T (\triangledown F \frac{\partial W}{\partial \bm{p}}) \right] \label{eq: LK Hessian}
\end{align}

Here $\bm{H}$ is the Guass-Newton approximation to the Hessian matrix which remains constant since it depends only on the reference subset. Thus the Hessian matrix can be computed before the algorithm starts iterating in order to reduce computational costs. However in order to achieve this reduced computational cost the iterative improvement to the warp function parameters was applied to the reference subset. This means that the iterative improvement cannot simply be added to the old warp function parameters to obtain the updated warp function parameters as in the forward additive method. Instead the updated warp function parameters must be determined using the following equation.
\begin{align}
  W(\bm{\zeta},\bm{p_{new}})&=W(\bm{\zeta},\bm{p_{current}})\times W(\bm{\zeta},\Delta \bm{p})^{-1} \label{eq: LK update}\\
  &=\begin{bmatrix}
  1+\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} & u\\
  \frac{\partial v}{\partial x} & 1+\frac{\partial v}{\partial y} & v \\
  0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
  1+\Delta \frac{\partial u}{\partial x} & \Delta \frac{\partial u}{\partial y} & \Delta u\\
  \Delta \frac{\partial v}{\partial x} & 1+\Delta \frac{\partial v}{\partial y} & \Delta v \\
  0 & 0 & 1
  \end{bmatrix}^{-1}
  \label{eq:get new p}
\end{align}
The new approximation to the warp function parameters can then be extracted from this warp function matrix, $W(\bm{\zeta},\bm{p_{new}})$, and used in the next iteration of the solver.

\subsection{Newton-Raphson}
The Newton-Raphson method is one of the most popular methods for finding the roots of a function. Its uses the first two terms of the Taylor series approximation of a function to iteratively improve upon an initial estimate to the roots of a function. The Taylor series of function is given as
\begin{equation}
  f(x_{i+1})=f(x_i)+f'(x_i) \times (x_{i+1}-x_i) + \frac{f''(x_i)}{2!} \times (x_{i+1}-x_i)^2 + ...
\end{equation}
The Newton-Raphson method is derived by considering only the first two terms of the Taylor series and setting the value of $f(x_{i+1})=0$.
\begin{equation}
  \label{eq: newton raphson}
  x_{i+1}=x_i-\frac{f(x_i)}{f'(x_i)}
\end{equation}
Thus this method uses the tangent line of the function at the current estimate to find an improved estimate to the root of the function. The improved estimate is the intersection point of the tangent line with the x-axis. However for the purposes of subset matching this method needs to be modified to find the minimum of a function and not the root since it is unlikely that a root exists. A root to the correlation criteria does not exist since that would indicate perfect correlation which is not possible due to errors introduced in the correlation process as a result of noise in the image and interpolation used to sample light intensity values between pixel locations. 

Since it is known that a minimum or maximum of a function exists where the derivative of the function is equal to zero the Newton-Raphson method is modified by using the derivative of the correlation coefficient as $f(x_i)$ in equation \ref{eq: newton raphson}. In doing so the algorithm changes from a local root finding algorithm to a local minimum-maximum finding algorithm. As such, in order to ensure that the algorithm finds a minimum to the correlation coefficient, the initial guess supplied to the algorithm needs to be in the vicinity of a local minimum. Thus the Newton-Raphson minimization algorithm is given as
\begin{equation}
  \label{eq: newton raphson 2}
  x_{i+1}=x_i-\frac{f'(x_i)}{f''(x_i)}
\end{equation}

However this form of the algorithm is for an equation with one independent variable. Subset matching usually requires solving an equation with six or more independent variables. As a result the derivative $f'(x_i)$ is replaced by the Jacobian of the correlation coefficient, $\bm{J}$, and the second derivative $f''(x_i)$ is replaced by the Hessian of the correlation coefficient,$\bm{H}$, in terms of the warp function parameters $\bm{p}$. For the general case of six warp function parameters the Jacobian is
\begin{equation}
  \label{eq: jacobian}
  \bm{J}=\frac{\partial C}{\partial \bm{p}}=\begin{bmatrix}
  \frac{\partial C}{\partial p_1} &
  \frac{\partial C}{\partial p_2} &
  \frac{\partial C}{\partial p_3} &
  \frac{\partial C}{\partial p_4} &
  \frac{\partial C}{\partial p_5} &
  \frac{\partial C}{\partial p_6} 
  \end{bmatrix}.
\end{equation}
Similarly the Hessian is of the form
\begin{equation}
  \label{eq: hessian}
  \bm{H}=\frac{\partial^2 C}{\partial \bm{p} \partial \bm{p}}=\begin{bmatrix}
  \frac{\partial^2 C}{\partial p_1^2} &
  \frac{\partial^2 C}{\partial p_1 \partial p_2} &
  \frac{\partial^2 C}{\partial p_1 \partial p_3} &
  \frac{\partial^2 C}{\partial p_1 \partial p_4} &
  \frac{\partial^2 C}{\partial p_1 \partial p_5} &
  \frac{\partial^2 C}{\partial p_1 \partial p_6} \\
  \frac{\partial^2 C}{\partial p_2 \partial p_1} &
  \frac{\partial^2 C}{\partial p_2^2} &
  \frac{\partial^2 C}{\partial p_2 \partial p_3} &
  \frac{\partial^2 C}{\partial p_2 \partial p_4} &
  \frac{\partial^2 C}{\partial p_2 \partial p_5} &
  \frac{\partial^2 C}{\partial p_2 \partial p_6} \\
  \frac{\partial^2 C}{\partial p_3 \partial p_1} &
  \frac{\partial^2 C}{\partial p_3 \partial p_2} &
  \frac{\partial^2 C}{\partial p_3^2} &
  \frac{\partial^2 C}{\partial p_3 \partial p_4} &
  \frac{\partial^2 C}{\partial p_3 \partial p_5} &
  \frac{\partial^2 C}{\partial p_3 \partial p_6} \\
  \frac{\partial^2 C}{\partial p_4 \partial p_1} &
  \frac{\partial^2 C}{\partial p_4 \partial p_2} &
  \frac{\partial^2 C}{\partial p_4 \partial p_3} &
  \frac{\partial^2 C}{\partial p_4^2} &
  \frac{\partial^2 C}{\partial p_4 \partial p_5} &
  \frac{\partial^2 C}{\partial p_4 \partial p_6} \\
  \frac{\partial^2 C}{\partial p_5 \partial p_1} &
  \frac{\partial^2 C}{\partial p_5 \partial p_2} &
  \frac{\partial^2 C}{\partial p_5 \partial p_3} &
  \frac{\partial^2 C}{\partial p_5 \partial p_4} &
  \frac{\partial^2 C}{\partial p_5^2} &
  \frac{\partial^2 C}{\partial p_5 \partial p_6} \\
  \frac{\partial^2 C}{\partial p_6 \partial p_1} &
  \frac{\partial^2 C}{\partial p_6 \partial p_2} &
  \frac{\partial^2 C}{\partial p_6 \partial p_3} &
  \frac{\partial^2 C}{\partial p_6 \partial p_4} &
  \frac{\partial^2 C}{\partial p_6 \partial p_5} &
  \frac{\partial^2 C}{\partial p_6^2} 
  \end{bmatrix}.
\end{equation}

Although the Jacobian and Hessian can be determined numerically it was decided to determine them analytically in order to avoid the errors associated with the numerical approach as these errors would only make the algorithm less reliable \todo{reliable or robust}. The Jacobian and Hessian are determined in two steps. First the chosen correlation coefficient equation is differentiated according to equations \ref{eq: jacobian} and \ref{eq: hessian} up until the point that it is the light intensity values of the deformed subset that are being differentiated.This is done by hand and is then coded as a function. To illustrate this consider the zero-mean normalised sum of squared difference correlation criterion. The Jacobian of this correlation criterion is given as

\begin{equation}
\label{eq: jacobian hand}
  \bm{J}=\frac{\partial C_{ZNSSD}}{\partial \bm{p}} = 2 \sum_i^n \left[ \frac{F_i-\mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} - \frac{G(\bm{x_0} + W(\zeta, \bm{p}))-\mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right] \times \left( -\frac{\frac{\partial G(\bm{x_0} + W(\zeta, \bm{p}))}{\partial \bm{p}}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right)
\end{equation}

The accompanying Hessian is then
\begin{multline}
\label{eq: hessian hand}
  \bm{H}=\frac{\partial^2 C_{ZNSSD}}{\partial \bm{p} \partial \bm{p}} = 2 \sum_i^n \left[ \frac{F_i-\mean{F}}{\sqrt{\sum_i^n (F_i-\mean{F})^2}} - \frac{G(\bm{x_0} + W(\zeta, \bm{p}))-\mean{G}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right] \times \left( -\frac{\frac{\partial^2 G(\bm{x_0} + W(\zeta, \bm{p}))}{\partial \bm{p} \partial \bm{p}}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right) \\ + 2 \sum_i^n \left[ \left( - \frac{\frac{\partial G(\bm{x_0} + W(\zeta, \bm{p}))}{\partial \bm{p}}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right) \times \left( - \frac{\frac{\partial G(\bm{x_0} + W(\zeta, \bm{p}))}{\partial \bm{p}}}{\sqrt{\sum_i^n (G_i-\mean{G})^2}} \right) \right]
\end{multline}

Note that these equations \ref{eq: jacobian hand} and \ref{eq: hessian hand} are dependent on the subset size since they are summed over the whole subset. However this is handled by using for loops within the coded function so that they can adjust to any subset size.

To complete the Jacobian and Hessian the derivative of the light intensity values of the deformed subset needs to be determined. This is done by symbolically creating the equation used determine the light intensity values of the deformed subset in a Matlab function. These equations are then symbolic differentiated using Matlab's symbolic toolbox in order to determine the appropriate analytical derivatives of the light intensity values in terms of the warp function parameters. These symbolic derivatives are then written as Matlab functions using Matlab's built in functionality; namely the function called "matlabFunction". Thus calculating these derivatives is handled automatically.

The reason for separating the differentiation process into two steps is that the derivatives of the light intensity values of the deformed subset are dependent on the warp function used. However equations \ref{eq: jacobian hand} and\ref{eq: hessian hand} are independent of the warp function and only require the derivatives of the light intensity values of the deformed subset to be substituted in. Thus the derivatives of the correlation coefficient equations, such as in equations \ref{eq: jacobian hand} and\ref{eq: hessian hand}, only need to be determined once since they remain unchanged when the warp function is changed. Thus when the user changes the warp function the derivatives of the light intensity values of the deformed subset are determined automatically making the process of analytically determining the Jacobian and Hessian for different warp functions automatic without the user needing to work out derivatives.

Although it is possible to symbolically defining the overall correlation coefficient equation and use Matlab's symbolic toolbox to differentiate it to determine the Jacobian and Hessian, the resulting Jacobian and Hessian would be specific to a certain subset size and warp function. Thus if either of these is changed the Jacobian and Hessian would need to be recalculated symbolically within Matlab. The problem with this is that since they contain summations over the whole subset this process would take a very long time. Thus by separating it into an unchanging function, which can adjust to the size of the subset, and a function for the derivative of the light intensity values of the deformed subset, that is determined automatically, the process is more efficient.

However the Newton-Raphson method is only locally convergent, thus it has poor global convergence properties, and it becomes less reliable as the number of independent dimensions, the warp function parameters, increases. In order to account for this the Newton-Raphson method is combined with the Golden section line search method to improve its robustness by determining an optimal step size along the search direction determined by the Newton-Raphson method. As such equation \ref{eq: newton raphson 2} changes to 
\begin{equation}
\label{eq: Newton update}
  \bm{p}_{i+1}=\bm{p}_i-\alpha \frac{C'(\bm{p}_i)}{C''(\bm{p}_i)}
\end{equation}
where $\alpha$ is the step size determined by the golden section method, $\bm{p}_i$ is the previous estimate of the warp function parameters and $C$ is the chosen correlation coefficient equation.

The golden section search method is a bracketing search method. For the one dimensional case this means that it searches for a minimum of a function within a range of the independent variable. However the correlation problem needs to be solved in terms of multiple dimensions; a dimension for each type of deformation that is present within the warp function. This method is extended to multiple dimensions by determining the optimal step size, within an allowable step size range, that optimises the correlation coefficient in a certain search direction, $\bm{dP}$. The search direction is determined by the Newton-Raphson method and is given as
\begin{equation}
  \bm{dP}=-\frac{C'(\bm{p}_i)}{C''(\bm{p}_i)} = - \frac{\bm{J}}{\bm{H}}.
\end{equation}

Thus once the Jacobian and Hessian of the correlation coefficient have been determined, at the current estimate of the deformation parameters $\bm{p}$, the search direction is known. Thereafter a range for the step size is decided upon with the lower and upper limit being $\alpha_0$ and $\alpha_1$ respectively. Within the first iteration the function the correlation coefficient equation is evaluated at two step sizes that fall within the step size range; namely $\alpha_2$ and $\alpha_3$. The magnitude of these step sizes are determined by the golden ration, $\phi$, and the limits place on the step size.
\begin{align}
  \alpha_2 &=\alpha_0+(1-\phi) \times (\alpha_1-\alpha_0) \\
  \alpha_3 &= \alpha_0+\phi \times (\alpha_1-\alpha_0) \\
  \text{where} \quad \phi &= \frac{\sqrt(5)-1}{2}
\end{align}
The function is evaluated as 
\begin{align}
    C_2 &=C(\bm{p}_i - \alpha_2 \times \frac{\bm{J}}{\bm{H}}) \\
    C_3 &=C(\bm{p}_i - \alpha_3 \times \frac{\bm{J}}{\bm{H}})
\end{align}
where $\bm{p}_i$ is the previous estimate of the warp function parameters.

Once the function is evaluated at these two step sizes the function values are compared. If $C_2$ is less than $C_3$ then it is known that the minimum of the function lies between step sizes $\alpha_0$ and $\alpha_3$. Otherwise, if $C_2$ is greater than $C_3$, the minimum lies between step sizes $\alpha_2$ and $\alpha_1$. The latter is illustrated in figure \ref{fig: golden section 1}. Thus on the next iteration of the golden section method the step-size search interval changes based on the conditions above thereby shrinking the search interval. Thereafter the process is repeated to find two new step sizes to investigate within the new step-size search interval.
\begin{figure}[H]
\begin{tikzpicture}
  \draw [->] (0,0) -- (9,0);
  \draw [->] (0,0) -- (0,5);
  % \draw [thick] (0,3) to [out=-60,in=-135] (8,4);
  \draw[red, ultra thick, domain=0:8] plot (\x, {((\x-4.5)/3)^2+0.5});
  \node [below] at (0,0) {\tiny $\alpha_0$};
  \node [below] at (8,0) {\tiny $\alpha_1$};
  \node [below] at (3.05,0) {\tiny $\alpha_2$};
  \node [below] at (4.94,0) {\tiny $\alpha_3$};
  \draw [-] (3.05,-0.1) -- (3.05,0.731769038890291);
  \draw [-] (4.94,-0.1) -- (4.94,0.521930836668256);
  \draw [-] (0,0) -- (0,-0.1);
  \draw [-] (8,-0.1) -- (8,1.861111111111111);
  \node [left,rotate=90] at (-0.3,4) {\small Correlation Coefficient};
  \node [below] at (4.5,-0.7) {\small Step size in search direction};
  \node [above] at (3.05,0.8) {\tiny $C_2$};
  \node [above] at (4.94,0.6) {\tiny $C_3$};
\end{tikzpicture}
\caption{First iteration of golden section search method}
\label{fig: golden section 1}
\end{figure}

The reason for using the golden ratio to define the two step sizes that the function is to be evaluated at is that on subsequent iterations one of the step sizes to be investigated will be identical to one that was evaluated in the previous iteration. This is shown in figure \ref{fig: golden section 2} where the correlation coefficient at step size $\alpha_3$ is to be evaluated again. Thus, after the first iteration, each subsequent iteration requires only one function evaluation. This is beneficial since evaluating the correlation coefficient is computationally expensive.
\begin{figure}[H]
\begin{tikzpicture}
  \draw [->] (0,0) -- (9,0);
  \draw [->] (0,0) -- (0,5);
  % \draw [thick] (0,3) to [out=-60,in=-135] (8,4);
  \draw[red, ultra thick, domain=3.05:8] plot (\x, {((\x-4.5)/3)^2+0.5});
  \node [below,white] at (0,0) {\tiny $P_{0}+\alpha_0 \times dP$};
  \node [below] at (8,0) {\tiny $\alpha_1$};
  \node [below] at (3.05,0) {\tiny $\alpha_2$};
  \node [below] at (4.94,0) {\tiny $\alpha_3$};
  \node [below] at (6.1,0) {\tiny $\alpha_4$};
  \draw [-] (3.05,-0.1) -- (3.05,0.731769038890291);
  \draw [-] (4.94,-0.1) -- (4.94,0.521930836668256);
  \draw [-] (6.1,-0.1) -- (6.1,0.784444444444444);
  \draw [-] (0,0) -- (0,-0.1);
  \draw [-] (8,-0.1) -- (8,1.861111111111111);
  \node [left,rotate=90] at (-0.3,4) {\small Correlation Coefficient};
  \node [below] at (4.5,-0.7) {\small Step size in search direction};
  \node [above] at (3.05,0.8) {\tiny $C_2$};
  \node [above] at (4.94,0.6) {\tiny $C_3$};
  \node [above] at (6.1,0.9) {\tiny $C_4$};
\end{tikzpicture}
\caption{Second iteration of golden section search method}
\label{fig: golden section 2}
\end{figure}

The golden section method continue until the iteration where the distance between the two step sizes, where the correlation coefficient is to be evaluated, is smaller than a predefined tolerance. At this point the optimal step size is taken to be the average of these last two step sizes.

However the golden section method works based on two assumptions. First it assumes that the minimum of the function lies between the upper and lower limit of the range of the step size. Secondly it requires that the function be convex between the limits of the step size. \todo{still need to figure out a function to ensure this}

\todo{talk about memory requirements and interpolation requirements, concerns of accuracy of initial guess}

\subsection{Phase shift correlation}
Since the Lucas-Kanade and Newton-Raphson correlation algorithms are iterative in nature they require an initial guess of the warp function parameters to improve. Although many methods can be used to provide an initial guess it was decided that the FFT cross correlation DIC algorithm would be used to determine the integer pixel displacement that occurs between images.

The phase shift correlation algorithm is an interesting correlation method because it performs correlation within the frequency domain in order to determine the rigid body shift that occurs between two images. First it is important to understand that an image is a two-dimensional, discrete function in the spatial domain since it defines the light intensity values that occur at specific points in the x and y directions. Secondly since the light intensity values change as a function of position in space the image itself contains spatial frequencies. To understand this consider one row of an image matrix. The light intensity values change as a function of x as illustrated on the left in Figure \ref{fig:fourier}. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{fourier_expansion.jpg}
    \caption{Example to illustrate the Fourier expansion of a row of an image matrix}
    \label{fig:fourier}
\end{figure}

Using Fourier series expansion this function can be broken down into a series of sine and cosine functions with each sine-cosine function pair having a unique frequency referred to as a spatial frequency. The sine-cosine functions of different spatial frequencies can be seen on the right in Figure \ref{fig:fourier}. Therefore an image can be broken down into sine-cosine functions of different frequencies in the same way as a temporal function with the difference being that the image contains spatial frequencies as opposed to temporal frequencies \cite{bourne2010spatial}. Therefore, since images can be broken down into a series of sine-cosine functions, images can be taken from the spatial domain to the frequency domain by using the Fast Fourier Transform algorithm (FFT).

In order to perform correlation in the frequency domain the correlation equation must be related to an equivalent function in the frequency domain. This is done using the cross correlation equation. Cross correlation is a method used to determine the similarity between two signals as a function of the displacement or lag of the one relative to the other. In the continuous case the cross correlation between continuous functions $f$ and $g$ is 
\begin{equation}
\label{eq: cross correlation cont}
 (f\otimes g)(t) \triangleq \int_{-\infty}^{\infty} f^*(\tau) g(\tau+t) \mathrm{d} \tau
\end{equation}
where $t$ is known as the lag between the two signals and $f^*$ denotes the complex conjugate of $f$. It is important to note that the resulting function gives the cross-correlation value between the signals as a function of the lag $t$. Similarly an operation known as convolution is defined as
\begin{equation}
  (f * g)(t) \triangleq \int_{-\infty}^{\infty} f(\tau) g(t-\tau) \mathrm{d} \tau.
\end{equation}

It can be proved that cross correlation and convolution are related by the following equation.
\begin{equation}
  (f(\tau) \otimes g(\tau))(t) = (f^*(-\tau) * g(\tau))(t)
\end{equation}
The reason for calculating cross correlation by using convolution is so that the convolution theorem can be used. The convolution theorem states that convolution in the time or spatial domain is equivalent to multiplication in the frequency domain. More specifically the Fourier Transform of a convolution is equivalent to the product of the Fourier Transforms of the functions which are being convoluted. This is expressed mathematically as
\begin{equation}
\label{eq: convolution theorem}
  \mathcal{F} \left[ (f(\tau) * g(\tau))(t) \right] = \mathcal{F} \left[ f(\tau) \right] \times \mathcal{F} \left[ g(\tau) \right]
\end{equation}
where $\mathcal{F}$ denotes the Fourier Transform operation. Note that the multiplication in this equation is element wise multiplication. This greatly improves the efficiency of convolution \todo{explain why}. Using equations \ref{eq: cross correlation cont} thorough \ref{eq: convolution theorem} correlation is performed in the frequency domain according to
\begin{equation}
  (f \otimes g)(t) = (f^*(-\tau) * g(\tau))(t) = \mathcal{F}^{-1} \left[ \mathcal{F} \left[ f^*(\tau) \right] \times \mathcal{F} \left[ g(\tau) \right] \right]
\end{equation}
where $\mathcal{F}^{-1}$ is the inverse Fourier Transform. Note that $f^*(-\tau)$ changed to $f^*(\tau)$ within the Fourier Transform in accordance with the complex conjugation property of the Fourier Transform \cite{arfken2011mathematical}. The mathematics presented here is for a one-dimensional, continuous function however the relations hold for the case of two-dimensional, discrete functions such as images.

The discrete Fourier Transform of an image matrix is determined according to
\begin{equation}
  \mathcal{F} \left[ F(x,y) \right] = \mathbf{F}(u,v) = \sum_{x=0}^{x=M-1} \sum_{y=0}^{y=N-1} F(x,y) e^{-i2\pi(\frac{xu}{M}+\frac{yv}{N})}
\end{equation}
where $i$ is the imaginary number and $M$ and $N$ are the number of pixels within the image in the x and y directions respectively. Here $u=0,1,2,...M-1$ and $v=0,1,2,...N-1$ in order to fully populate the discrete Fourier Transform matrix ,$\mathbf{F}(u,v)$, of the image.

Phase shift correlation relies upon using the assumption that the deformed image, $G$, is equivalent to the reference image, $F$, shifted in the x and y direction by $\Delta x$ and $\Delta y$ respectively \cite{barros2017dft}.
\begin{equation}
  G(x,y)=F(x + \Delta x, y + \Delta y)
\end{equation}
Therefore the Discrete Fourier Transform of the deformed image can be expressed as
\begin{align}
   \mathcal{F} \left[ G(x,y) \right] &= \mathcal{F} \left[ F(x + \Delta x,y + \Delta y) \right] \\ &=  \sum_{x=0}^{x=M-1} \sum_{y=0}^{y=N-1} F(x + \Delta x,y + \Delta y) e^{-i2\pi(\frac{xu}{M}+\frac{yv}{N})} \\
   \text{letting} \quad &x'=x+\Delta x \quad \text{and} \quad y'=y+\Delta y \\
   &=  \sum_{x'=0}^{x'=M-1} \sum_{y'=0}^{y'=N-1} F(x',y') e^{-i2\pi(\frac{(x'-\Delta x)u}{M}+\frac{(y'-\Delta y)v}{N})} \\
   &=  e^{-i2\pi(\frac{-\Delta x u}{M}+\frac{-\Delta y v}{N})} \sum_{x'=0}^{x'=M-1} \sum_{y'=0}^{y'=N-1} F(x',y') e^{-i2\pi(\frac{x'u}{M}+\frac{y'v}{N})} \\
   &=  \mathcal{F} \left[ F(x,y) \right] e^{i2\pi(\frac{\Delta x u}{M}+\frac{\Delta y v}{N})} .
\end{align}

Thus the correlation between image matrices $F(x,y)$ and $G(x,y)$ is calculated as
\begin{align}
  (F(x,y) \otimes G(x,y))(l,k) &= \mathcal{F}^{-1} \left[ \mathcal{F} [F^*(x,y)] \times \mathcal{F} [G(x,y)] \right] \\
  &= \mathcal{F}^{-1} \left[ \mathcal{F} [F^*(x,y)] \times \mathcal{F} \left[ F(x,y) \right] \times e^{i2\pi(\frac{\Delta x u}{M}+\frac{\Delta y v}{N})} \right] \\
  &= \mathcal{F}^{-1} \left[ \mathbf{F}^*(u,v) \times \mathbf{F}(u,v) \times e^{i2\pi(\frac{\Delta x u}{M}+\frac{\Delta y v}{N})} \right] \\
  &= \mathcal{F}^{-1} \left[ \left| \mathbf{F}(u,v) \right|^2 \times e^{i2\pi(\frac{\Delta x u}{M}+\frac{\Delta y v}{N})} \right] \label{eq: FFT impulse}
\end{align}
which returns a matrix where the each element of the matrix is a cross-correlation value for the two image matrices where image $G(x,y)$ has been shifted in the x and y direction by a specific amount. The shift of image $G(x,y)$ is related to the index of the matrix. For images of size $M$ by $N$ the indexes $l$ and $k$ correspond to shifting image $G(x,y)$ by $x_{shift}$ and $y_{shift}$, to get $G(x+x_{shift},y+y_{shift})$, according to
% this equation is for shifting of F not G
% \begin{align}
%   x_{shift}&= 
%     \begin{cases}
%         l-1 & \text{if $l \leq \frac{M}{2}+1$ and M is even} \\
%         l-2-M & \text{if $l > \frac{M}{2}+1$ and M is even}   \\
%         l-1 & \text{if $l \leq \frac{M+1}{2}$ and M is odd} \\
%         l-2-M & \text{if $l > \frac{M+1}{2}$ and M is odd}
%     \end{cases} \\
%   y_{shift}&= 
%     \begin{cases}
%         k-1 & \text{if $k \leq \frac{N}{2}+1$ and N is even} \\
%         k-2-N & \text{if $k > \frac{N}{2}+1$ and N is even}   \\
%         k-1 & \text{if $k \leq \frac{N+1}{2}$ and N is odd} \\
%         k-2-N & \text{if $k > \frac{N+1}{2}$ and N is odd}
%     \end{cases}
% \end{align}

\begin{align}
  x_{shift}&= 
    \begin{cases}
        1-l & \text{if $l \leq \frac{M}{2}+1$ and M is even} \\
        M+2-l & \text{if $l > \frac{M}{2}+1$ and M is even}   \\
        1-l & \text{if $l \leq \frac{M+1}{2}$ and M is odd} \\
        M+2-l & \text{if $l > \frac{M+1}{2}$ and M is odd}
    \end{cases} \label{eq: FFT xshift} \\
  y_{shift}&= 
    \begin{cases}
        1-k & \text{if $k \leq \frac{N}{2}+1$ and N is even} \\
        N+2-k & \text{if $k > \frac{N}{2}+1$ and N is even}   \\
        1-k & \text{if $k \leq \frac{N+1}{2}$ and N is odd} \\
        N+2-k & \text{if $k > \frac{N+1}{2}$ and N is odd}
    \end{cases} \label{eq: FFT yshift}
\end{align}

Referring back to equation \ref{eq: FFT impulse} the term $e^{i2\pi(\frac{\Delta x u}{M}+\frac{\Delta y v}{N})}$ is a Fourier Transform of a finite response impulse function which has its peak located at $(\Delta x, \Delta y)$ in the spatial domain. Therefore when computing the cross-correlation between two images using the FFT method described above the resulting matrix contains correlation values for various shifts applied to image $G(x,y)$ and the highest correlation value occurs at point $(\Delta x, \Delta y)$ corresponding to the optimal shifting of the deformed image to bring it into alignment with the reference image. These index values $\Delta x$ and $\Delta y$ are substituted into Equations \ref{eq: FFT xshift} and \ref{eq: FFT yshift} respectively to determine the optimal shift that should be applied to the deformed image.

Although this method of correlation is substantially faster than either Newton-Raphson or Lucas-Kanade, especially as the subset size increases, it has limitations. First of all only shifts of integer values can be investigated since this method does not employ interpolation. This limitation can be overcome by modifying the method to perform interpolation to increase the resolution of the matrices in the frequency domain \cite{malah1976dft}. However, in this project, the FFT based correlation is intended to be used to determine an initial guess for the Newton-Raphson and Lucas-Kanade algorithms so sub-pixel correlation is not important.

The second limitation is that this method can only determine the optimal shift values if they are less than half the size of the subset. This is of little concern since subset size can simply be adjusted to compensate for this if the need arises.

% Cross power spectral density
% This (say that it is based on the phase shift theorem and reference paper)
% Cross correlation is performed by using convolution so that the 
% explain FFT and why faster
% relate cross cor to convolution and convolution to FFT

\section{Subset splitting}
One of the major downfalls of DIC is that it assumes that the deformation field over every subset is continuous. Although this is true most of the time it breaks down when there is a displacement discontinuity in the specimen resulting from the presence of cracks or shear bands. In order to deal with this issue the method of subset splitting proposed by Poissant and Barthelat \cite{poissant2010novel} was implemented. In this method an equation defining a line, that passes through the subset, is used to split the pixels within the subset into two groups and a different set of warp function parameters is used for each group. Thus if the line is placed such that it lies on the crack the subset will be split into two groups allowing the deformations on either side of the crack to be determined separately. 

The first step is to determine the equation for the line such that it lies on the crack. A straight line is used to approximate the crack since a curve would make the process more complex and is in general not necessary since the subsets only capture a small portion of the crack. The method proposed by Poissant and Barthelat \cite{poissant2010novel} for determining the this line was found to give inconsistent results. Instead a more robust method, although more computationally expensive, was implemented in the form of a particle swarm. The equation for the line is of the form 
\begin{align}
    y&=m \times (x-d) + c \\
    \text{where} \quad m&=\tan{\theta}
\end{align}
where $d$ is the shift in the x direction, $c$ is the shift in the y direction and $m$ is the gradient which corresponds with angle $\theta$. The reason for including a shift in both directions is so that a definitive bounds can be placed on the range of values evaluated by the particle swarm algorithm. In other words if the shift in the x direction $d$ was taken out of the equation then the shift in the y direction $c$ would have to range from negative infinity to infinity in order to compensate. The x and y direction shifts are evaluated from zero to the subset size and the angle $\theta$ is evaluated from $0^\circ$ to $180^\circ$ measured anti-clockwise from the x-axis. 

Particle swarm optimisation is performed using the correlation coefficient equation as the objective function. This correlation coefficient equation takes in two sets of warp function parameters and the line equation for the crack. It then splits the pixels in the subset into the two groups according to the line equation for the crack and warps each group of pixels according to their set of warp function parameters. Then interpolation is performed on all the pixels in the subset and the chosen correlation equation is used to determine the degree of fit. Note that the particle swarm algorithm only investigates the three parameters $d$, $c$ and $\theta$ but the algorithm requires the two sets of warp function parameters to be provided.

These two sets of warp function parameters are taken as the warp function parameters of two subsets, with good correlation coefficient values, nearest to the subset under consideration. A temporary line is drawn through the centre of the subset under consideration such that this line has a gradient that is perpendicular to the line for the crack. Then the subset whose warp function parameters are to be used as an input for the particle swarm algorithm is the one which is nearest to the subset under consideration, has a good correlation value and is intersected by this temporary line. Such a subset is found on both sides of the crack line so that there is an appropriate guess of the warp function parameters for both groups of pixels.

The second step is to perform correlation on the subset according to the splitting line. To do this the Lucas-Kanade and Newton-Raphson algorithms discussed earlier need to be modified slightly. The subset is split up into two groups with each group having its own set of warp function parameters to optimise. Therefore within each algorithm the line equation is used to split the pixels within the subsets into these two groups. Then for the Newton-Raphson algorithm a Jacobian and Hessian is computed for each group of pixels. This is done by evaluating Equations \ref{eq: jacobian hand} and \ref{eq: hessian hand} using only the pixels within the group under consideration. With the Jacobian and Hessian known for each group the warp function parameters of both groups can be updated by computing the update for each group separately using Equation \ref{eq: Newton update}.

Similarly for the Lucas-Kanade algorithm the RHS term of Equation \ref{eq:delta p} is evaluated separately for both groups of pixels and the warp function parameters for each group are updated separately using Equation \ref{eq: LK update}. Although the warp function parameters are updated separately in both algorithms, the correlation coefficient is determined for the whole subset using both groups of pixels. This is because the correlation coefficient needs to indicate the degree of fit for a specific line equation and two sets of warp function parameters.

Once the subset has been successfully correlated using the subset-splitting method there are two sets of warp function parameters but only one set can be recorded per subset. The set of warp function parameters that applies to the largest number of pixels in the subset is recorded for the subset since this set of warp function parameters plays the larger role with respect to the correlation coefficient. In other words there is more material on this side of the crack line, within the subset, associated with this set of warp function parameters.

how works for diff algorithms
finding crack orientation
initial guess


\section{Stopping criteria}
The subset matching algorithm will continue to improve the warp function parameters until stopped. Thus a means of determining when the iterations should stop is needed. Ideally the iterations should stop once the correlation between the subsets is sufficiently accurate. Thus correlation could be halted once the correlation criteria coefficient has reached the desired value. However determining the correlation coefficient within each iteration is computationally expensive. Therefore the stopping criteria suggested by B. Pan \cite{panfast} is used since it is much more efficient to compute. This criteria halts the iterations once the norm of the warp function parameter update is less than $0.001$.
\begin{align}
  ||\Delta \bm{p}|| &= \sqrt{(\Delta u)^2 + (\Delta \tilde{x}\times\Delta \frac{\partial u}{\partial x}) + (\Delta \tilde{y}\times \Delta \frac{\partial u}{\partial y}) + (\Delta v)^2 + (\Delta \tilde{x}\times\Delta \frac{\partial v}{\partial x}) + (\Delta \tilde{y}\times \Delta \frac{\partial v}{\partial y})} \nonumber \\
  &\text{where} \quad \Delta \tilde{x} = max(\Delta x) \quad \text{and} \quad \Delta \tilde{y} = max(\Delta y)
\end{align}

\section{Algorithm outline}
The steps performed within the algorithm in order to achieve correlation are listed here in order to illustrate how the theory presented here is related to the code. It should be noted that the initial guesses for the warp function parameters were determined by the user selecting seed points in the reference and investigated images.

\subsection{Lucas-Kanade}
The following is the steps executed within the Lucas-Kanade subset matching method.
\begin{enumerate}
  \item First $\mean{F}$ is determined for the reference subset.
  \item Then $\triangledown F$ is calculated using Matlab's built in image gradient function (imgradientxy) and the Jacobian of the warp function is calculated. These are then used to determine the Hessian matrix.
  \item Thereafter $\Delta F$ is determined since it too remains constant.
  \item The initial guess for the warp function parameters is then used to calculate the $\Delta x_{warp}$ and $\Delta y_{warp}$ for the investigated subset $G$ using equation \ref{eq:warp}.
  \item $\Delta x_{warp}$ and $\Delta y_{warp}$ are used in equation \ref{eq:subset warp} to obtain the warped version of the investigated subset.
  \item $\mean{G}$ and $\Delta G$ are then determined.
  \item With all the necessary values determined; equation \ref{eq:delta p} can be used to calculate $\Delta \bm{p}$.
  \item Then equation \ref{eq:get new p} is used to determine the updated approximation to the warp function parameters.
  \item Step 4 until step 8 are repeated until the stopping criteria is satisfied.
\end{enumerate}

\subsection{Newton-Raphson}
The following illustrates the steps executed during the Newton-Raphson subset matching method.
\begin{enumerate}
  \item First the bicubic interpolation coefficients are determined for the deformed subset.
  \item Then the Jacobian and Hessian are determined for the current estimate of the warp function parameters.
  \item Use the Jacobian and Hessian to calculate the search direction.
  \item Thereafter golden section search method is used to determine the optimal step size for the search direction.
  \item Determine the norm of the change in the warp function parameters. If it is smaller than a predefined amount then correlation has been achieved otherwise repeat steps 2 until 5.
\end{enumerate}

\chapter{Code explanation}
\section{interpolation}
It is clear that warping a subset according to equation \ref{eq:subset warp} requires that the light intensity values between pixel locations can be determined. This is an issue because images store this information in a discrete manner. This is solved by using interpolation to determine the light intensity values between pixels based on the light intensity values of neighbouring pixels. For the purposes of this project Matlab's built in interp2 fucntion was used to perform linear interpolation to determine the light intensity values for non-standard pixel locations where needed.




Image interpolation is typically the most time consuming part of the correlation process for iterative sub-pixel DIC algorithms \cite{pan2016performance}. Therefore the interpolation schemes have been implemented in the most efficient way for the respective correlation algorithms.

The two different correlation algorithms used require different information from the interpolation scheme. The Lucas-Kanade algorithm only needs the interpolated light intensity values at the query points. Newton-Raphson on the other hand also needs the interpolation coefficients since the Hessian matrix is dependent on these interpolation coefficients. As such two different interpolation schemes have been used in these correlation algorithms.

Although these interpolation schemes are different they are both based on bicubic interpolation which attempts to fit a 4th order, two-dimensional polynomial surface to the known light intensity values at integer pixel locations. Thus a query point will lie within a square which has integer pixel locations at the corners of the square. The x and y coordinates of the query point are substituted into the equation for the surface to interpolate the approximate light intensity value. Each square section between integer pixel locations has its own equation making this interpolation piecewise.

A 4th order, two-dimensional polynomial surface has 16 coefficients which define the surface. As such 16 integer pixel points are required to determine these 16 coefficients. Therefore a $4 \times 4$ submatrix of light intensity values is required to determine the polynomial surface that approximates the light intensity values between the 4 integer pixel locations at the centre of the submatrix.


\subsection{Lucas-Kanade}
The Lucas-Kanade algorithm uses a matlab interpolation function called "griddedInterpolant" with the bicubic convolution based interpolation option selected. This function was chosen since it performs interpolation once, before starting the iterations, to create an interpolant object and this interpolant object is then called, within the iterations, to perform fast interpolation at the query points.

\subsection{Newton-Raphson}
Although "griddedInterpolant" works well for the Lucas-Kanade algorithm it doesn't not return the coefficients of interpolation. Therefore a function was created to solve for the coefficients of interpolation by fitting the 4th order, two-dimensional polynomial surface to a $4 \times 4$ submatrix of the image. This method of bicubic interpolation is rather basic but it still works well.

First consider the equation of the surface that is to be fitted
\begin{equation}
  F(x,y)=\sum^4_{i=1} \sum^4_{j=1} a_{ij} \times x^{i-1} y^{j-1}
\end{equation}
where $a_{ij}$ are the coefficients of the interpolation equation. Here x and y are the fractional parts of the pixel positions; thus they fall between zero and one.

To solve for the coefficients we need to calculate the gradients of the polynomial function at the four centre integer pixel locations. This is done using numerical differentiation techniques; specifically the two-point central difference formula. For the three partial derivatives these formulas are \cite{gilat2004matlab}
\begin{align}
  \frac{\partial F(x_i,y_j)}{\partial x} &= \frac{F(x_{i+1},y_j)-F(x_{i-1},y_j)}{2} \\
  \frac{\partial F(x_i,y_j)}{\partial y} &= \frac{F(x_{i},y_{j+1})-F(x_{i},y_{j-1})}{2} \\
  \frac{\partial^2 F(x_i,y_j)}{\partial x \partial y} &= \frac{\left[ F(x_{i+1},y_{j+1})-F(x_{i-1},y_{j+1}) \right] - \left[ F(x_{i+1},y_{j-1})-F(x_{i-1},y_{j-1}) \right]}{4}.
\end{align}
Note that since the x and y coordinates are integers the function values at these points correspond to the light intensity values at integer pixel locations which are known. Therefore these gradients can be solved immediately. Once this is done there are 12 gradients that are known and 4 integer pixel light intensity values. Thus these 16 known values can be used to solve for the 16 coefficients of the polynomial equation. The equations to do this are obtained by substituting the four centre integer pixel location x and y values ((0,0), (0,1), (1,0) and (1,1)) into the polynomial equation, the x partial derivative of the polynomial equation, the y partial derivative of the polynomial equation and the x-y partial derivative of the polynomial equation. This results in 16 equation with the known pixel light intensity values and the known gradients as a function of the coefficient of the polynomial equation. However by substituting the symbolic expression for the gradients and the integer pixel light intensity values the equations can be solved simultaneously to give the coefficients as a function of the known integer pixel light intensity values. These equations are then
\begin{align}
  A_{11} &= F(x_2,y_2) \\
  A_{12} &= -0.5 \times F(x_2,y_1) + 0.5 \times F(x_2,y_3) \\
  A_{13} &= F(x_2,y_1) - 2.5 \times F(x_2,y_2) + 2 \times F(x_2,y_3) - 0.5 \times F(x_2,y_4) \\
  A_{14} &= -0.5 \times F(x_2,y_1) + 1.5 \times F(x_2,y_2) - 1.5 \times F(x_2,y_3) + 0.5 \times F(x_2,y_4) \\
  A_{21} &= -0.5 \times F(x_1,y_2) + 0.5 \times F(x_3,y_2) \\
  A_{22} &= 0.25 \times F(x_1,y_1) - 0.25 \times F(x_1,y_3) - 0.25 \times F(x_3,y_1) + 025 \times F(x_3,y_3) \\
  % \begin{split}
  A_{23} &= -0.5 \times F(x_1,y_1) + 1.25 \times F(x_1,y_2) - F(x_1,y_3) + 0.25 \times F(x_1,y_4) \nonumber \\ &+ 0.5 \times F(x_3,y_1) - 1.25 \times F(x_3,y_2) + F(x_3,y_3) - 0.25 \times F(x_3,y_4) \\
  % \end{split} \\
  A_{24} &= 0.25 \times F(x_1,y_1) - 0.75 \times F(x_1,y_2) + 0.75 \times F(x_1,y_3) - 0.25 \times F(x_1,y_4) \nonumber \\ &- 0.25 \times F(x_3,y_1) + 0.75 \times F(x_3,y_2) - 0.75 \times F(x_3,y_3) + 0.25 \times F(x_3,y_4) \\
  A_{31} &= F(x_1,y_2) - 2.5 \times F(x_2,y_2) + 2 \times F(x_3,y_2) - 0.5 \times F(x_4,y_2) \\
  A_{32} &= -0.5 \times F(x_1,y_1) + 0.5 \times F(x_1,y_3) + 1.25 \times F(x_2,y_1) - 1.25 \times F(x_2,y_3) \nonumber \\ &- F(x_3,y_1) + F(x_3,y_3) + 0.25 \times F(x_4,y_1) - 0.25 \times F(x_4,y_3) \\
  A_{33} &= F(x_1,y_1) - 2.5 \times F(x_1,y_2) + 2 \times F(x_1,y_3) - 0.5 \times F(x_1,y_4) \nonumber \\ &- 2.5 \times F(x_2,y_1) + 6.25 \times F(x_2,y_2) - 5 \times F(x_2,y_3) + 1.25 \times F(x_2,y_4) \nonumber \\ &+ 2 \times F(x_3,y_1) - 5 \times F(x_3,y_2) + 4 \times F(x_3,y_3) - F(x_3,y_4) - 0.5 \times F(x_4,y_1) \nonumber \\ &+ 1.25 \times F(x_4,y_2) - F(x_4,y_3) + 0.25 \times F(x_4,y_4) \\
  A_{34} &= -0.5 \times F(x_1,y_1) + 1.5 \times F(x_1,y_2) - 1.5 \times F(x_1,y_3) + 0.5 \times F(x_1,y_4) \nonumber \\ &+ 1.25 \times F(x_2,y_1) - 3.75 \times F(x_2,y_2) + 3.75 \times F(x_2,y_3) - 1.25 \times F(x_2,y_4) \nonumber \\ &- F(x_3,y_1) + 3 \times F(x_3,y_2) - 3 \times F(x_3,y_3) + F(x_3,y_4) + 0.25 \times F(x_4,y_1) \nonumber \\ &- 0.75 \times F(x_4,y_2) + 0.75 \times F(x_4,y_3) - 0.25 \times F(x_4,y_4) \\
  A_{41} &= -0.5 \times F(x_1,y_2) + 1.5 \times F(x_2,y_2) - 1.5 \times F(x_3,y_2) + 0.5 \times F(x_4,y_2) \\
  A_{42} &= 0.25 \times F(x_1,y_1) - 0.25 \times F(x_1,y_3) - 0.75 \times F(x_2,y_1) + 0.75 \times F(x_2,y_3) \nonumber \\ &+ 0.75 \times F(x_3,y_1) - 0.75 \times F(x_3,y_3) - 0.25 \times F(x_4,y_1) + 0.25 \times F(x_4,y_3) \\
  A_{43} &= -0.5 \times F(x_1,y_1) + 1.25 \times F(x_1,y_2) - F(x_1,y_3) + 0.25 \times F(x_1,y_4) \nonumber \\ &+ 1.5 \times F(x_2,y_1) - 3.75 \times F(x_2,y_2) + 3 \times F(x_2,y_3) - 0.75 \times F(x_2,y_4) \nonumber \\ &- 1.5 \times F(x_3,y_1) + 3.75 \times F(x_3,y_2) - 3 \times F(x_3,y_3) + 0.75 \times F(x_3,y_4) \nonumber \\ &+ 0.5 \times F(x_4,y_1) - 1.25 \times F(x_4,y_2) + F(x_4,y_3) - 0.25 \times F(x_4,y_4) \\
  A_{44} &= 0.25 \times F(x_1,y_1) - 0.75 \times F(x_1,y_2) + 0.75 \times F(x_1,y_3) - 0.25 \times F(x_1,y_4) \nonumber \\ &- 0.75 \times F(x_2,y_1) + 2.25 \times F(x_2,y_2) - 2.25 \times F(x_2,y_3) + 0.75 \times F(x_2,y_4) \nonumber \\ &+ 0.75 \times F(x_3,y_1) - 2.25 \times F(x_3,y_2) + 2.25 \times F(x_3,y_3) - 0.75 \times F(x_3,y_4) \nonumber \\ & - 0.25 \times F(x_4,y_1) + 0.75 \times F(x_4,y_2) - 0.75 \times F(x_4,y_3) + 0.25 \times F(x_4,y_4)
\end{align}
Thus the coefficients of interpolation are determined by substituting in the known integer pixel light intensity values into the above equations.

Interpolation is carried out over the whole image prior to the correlation process. Once the coefficients have been determined they are stored in ".mat" file. Then when interpolation needs to be performed within the correlation process the required coefficients are loaded from the saved file use Matlabs "matfile" function. This function is used since it enables the algorithm to only load the required coefficients thereby reducing the memory used. This is important since the Newton-Raphson algorithm needs a substantial amount of memory when correlation is done using parallel computing. This method of saving the coefficient for later use is common for interpolation schemes of this type.

% \subsection{Speckle pattern generator}
% It was discovered that because creating synthetic images involves a large number of simple operations it would run much faster on a graphics processing unit (GPU). In order to do this the equation \ref{eq: speckle} had to be vectorized \todo{not sure this word is 100\% accurate} so that the light intensities at many points in the image could be evaluated simultaneously using matrix multiplication. 

% First the function creates a grid of points (X and Y) that correspond to the centre location of the pixels which are to be generated. However a jittered grid of points is required for each pixel for the purposes of jittered super sampling. These points are obtained by displacing the pixel centre location, many times, to obtain a jittered grid of points that fall within the fillfactor area of the pixel. The same displacements are applied to each pixel centre location to create its corresponding jittered grid. Since multiple points are created for each pixel, the pixel locations are reshaped into a column vector (line 13 and 14) and the displacements are added as row vectors resulting in a matrix where each row holds the super sampling locations for each pixel. 

% The row vector of displacement values are created in 3 steps. First the grid-spacing between the points of the unjittered grid are determined in both directions (xdist and ydist) based on the step size between pixels (stepsize), the fill factor (fillfactor) and the number of points in each direction of the grid (numpoints). The second step uses these grid-spacings to determine the x and y displacements of the unjittered grid (xgrid1 and ygrid1). These are then used to create 2 matrices of unjittered displacement values for the x and y direction (xgrid2 and ygrid2). 

% The final step involves jittering the displacement values. This is done by creating a 2 matrices of random values between -0.5 and 0.5, multiplying them by their corresponding grid-spacing and adding them to the matrices of unjittered displacement values to get the two final matrices of jittered displacement values (xgrid3 and ygrid3 at line 32). These matrices are then reshaped to be row vectors (xxgrid and yygrid) which are passed to the GPU to be stored in VRAM (Video Random Access Memory).

% Thereafter the function defines the speckle pattern by assigning values to the properties of each speckle. First the vector of speckle sizes is converted from units of pixels to the units used in the displacement function hereafter referred to as displacement units (line 40). It was originally in units of pixels because for a good image the speckle sizes must be proportional to the subset size which is in units of pixels. The position of the speckles are then defined ,in displacement units, at random locations that fall within the range of the image. The light intensity at the centre of each speckle is then randomly defined to fall in the range between 0.5 and 1 where 1 is white and 0 is black.

% The choice to alter the speckle radius in a sinusoidal manner lead to some issues when the frequency of the sinusoid increased. At higher frequencies the resulting speckle would represent a star shape which is an unrealistic speckle shape. In order to adjust for this the amplitude of the sinusoid was adjusted based on its frequency using the function defined on line 48. This function exponentially decreases the amplitude of the sinusoid as the frequency increases.

% The for loop starting at line 54 saves the details of every speckle. First a random phase shift (a) and frequency (b) are defined. The amplitude (c) is then determined based on the frequency using the function on line 48. The base radius (d), x and y positions (Xk and Yk) and the light intensity (I0) are then all saved.

% GPUs have a limited amount of VRAM and so the number of points that can be evaluated at once is limited. To cope with this the points to be evaluated are divided into groups so that the VRAM required to evaluate each group is less than what is available. The number of groups is determined by dividing the overall memory requirements of the total image by the memory that is available. Using a simplified approach the total memory requirements of the image is the number of pixels in the image times the number of points that need to be evaluated for each pixel times the number of speckles. This is divided by the available memory which is 44567000 for the computer used during this project. If a different computer is used this value should be adjusted. Note that these memory values are relative values and have no units.

% Super sampling relies upon using the average of the sampled locations for each pixel to determine that pixels light intensity value. A Gaussian weighted average (H at line 72) is used to do this in order to allow the sampled locations closer to the pixel centre to have a greater influence on the light intensity value. The weighted average matrix is reshaped into a row vector and then this row vector is repeated to form a matrix (HH). The number of rows in this matrix is equivalent to the number of pixels that are evaluated on each pass to the GPU.

% With all the necessary variables defined the calculation of light intensity values at specific locations in the image can commence. This is done between lines 79 and 154. A counter is used to keep track of which group of elements is to be processed. The variables "begin" and "ending" correspond to the indexes of the first and last elements in X and Y that are to be to be processed (which pixels are to be processed) while "ending2" gives the number of pixels to be computed in each pass to the GPU ("ending2" is equal to "numOfPixels" except on the last pass to the GPU where a smaller number of pixels is processed)

% For the reference image (imageCount=1) the points to be evaluated (Xin and Yin) are determined by adding the row vectors of jittered displacements (xxgrid and yygrid) to the column vectors of x and y positions (X and Y). Note that since "X" and "Y" are column vectors with "numOfPixels" amount of rows and "xxgrid" and "yygrid" are row vectors with "numpoints" squared amount of columns the resulting matrices (Xin and Yin) are of size "numOfPixels" by "numpoints" squared. Each row in "Xin" and "Yin" is a collection of the jittered super sampling positions for a single pixel. Therefore each row corresponds to a pixel.

% For the case of the deformed image the points to be evaluated are determined by subtracting the displacement at that point (fux and fuy functions) from the group of x and y positions (X and Y) and then adding the jittered displacement matrices (xxgrid and yygrid). In doing so the pixel point is displaced in the opposite direction compared to what is described by the displacement function. This is done because displacement of the speckle pattern according the the displacement function while holding the pixel locations constant is equivalent to displacing the pixel locations in the opposite direction to the displacement function and holding the speckle pattern constant. 

% At this point the locations at which to evaluated the speckle pattern are available. To evaluate the light intensity at a location in the image involves four steps. The steps will be explained in terms of determining the light intensity at a single location although the algorithm determines this at multiple locations simultaneously. In the first step the distances between all the speckle centres and the location needs to be determined. To do this "Xin" and "Yin" are then reshaped to be column vectors (Xgrid and Ygrid) and the row vectors containing the location of the speckle centres (Xk and Yk) are subtracted from these to give "X1" and "Y1". Each row of "X1" and "Y1" contains the distance between each speckle centre and the specific location in the image.

% The second step involves determining the angles between the imaginary lines, that pass through the speckle centres and the location under consideration, and the x-axis. This is done by applying Matlab's atan2 function to the x and y distances "X1" and "Y1". The third step involves determining the radius of the each speckle along the imaginary line. This is done by applying the phase shifts and frequency changes to the angles, taking the sin of the angles, multiplying them by their respective amplitudes and adding the base radii. This results in the matrix "sins1". Each row of "sins1" gives the radii of the all the speckles in the direction of the location under consideration.

% The fourth step requires determining how much each speckle contributes to the location under consideration. This is done by first determining the squared distance between the speckle centre and the location under consideration which is done using Pythagoras to get "M". The contribution of each speckle is determined by taking the exponent of the negative of this distance (M) divided by the speckle radius squared. This results in the matrix "speckleContribution". 

% Each row in "speckleContribution" gives the contribution of each speckle to the point under consideration. This matrix is then multiplied by the column vector of the speckle light intensities to give a column vector of the overall light intensity at each location under consideration. This row vector is then reshaped to a matrix such that each row contains the light intensities at the jittered super sampling locations for a pixel. Then it is multiplied by the matrix of weighted averages and each row is summed to give a column vector of pixel light intensities. Throughout this process variables are cleared once they are no longer needed in order to free up space on the VRAM.

% Finally these values are collected from the GPU and stored in a column vector on the CPU. Once this vector is fully populated it is reshaped to the size of the desired image and the light intensity values are quantized using Matlab's "imquantize" function. This is 12 bit quantization which is consistent with the quantization that takes place within the camera used for experiments.

% \lstinputlisting[language=Matlab]{analyticalspeckleContinuous7_simplified_for_explanation.m}

% It was discovered that because creating synthetic images involves a large number of simple operations it would run much faster on a graphics processing unit (GPU). In order to to this the equation \ref{eq: speckle} had to be vectorized \todo{not sure this word is 100\% accurate} so that the light intensities at many points in the image could be evaluated simultaneously using matrix multiplication. 

% First the function creates a grid of points (X and Y) that correspond to the centre of the pixels which are to be generated. This grid of points (in matrix format) is then reshaped into a column vector of points. This is done so that these points can be evaluated simultaneously using a matrix multiplication. Thereafter the vector of speckle sizes is converted from units of pixels to the units used in the displacement function hereafter referred to as displacement units. It was originally in units of pixels because for a good image the speckle size must be proportional to the subset size which is in units of pixels. The position of the speckles are then defined at random locations that fall within the range of the image. Note that these locations are in displacement units. The light intensity at the centre of each speckle is then randomly defined to fall in the range between 0.5 and 1 where 1 is white and 0 is black.

% The choice to alter the speckle radius in a sinusoidal manner lead to some issues when the frequency of the sinusoid increased. At higher frequencies the resulting speckle would represent a star shape which is an unrealistic speckle shape. In order to adjust for this the amplitude of the sinusoid was adjusted based on its frequency using the function defined on line 25. This function exponentially decreases the amplitude of the sinusoid as the frequency increases.

% The for loop starting at line 31 saves the details of every speckle. First a random phase shift (a) and frequency (b) are defined. The amplitude (c) is then determined based on the frequency using the function on line 25. The base radius (d), x and y positions (Xk and Yk) and the light intensity (I0) are then all saved.

% GPUs have a limited amount of VRAM (Video Random Access Memory) and so the number of points that can be evaluated at once is limited. To cope with this the points to be evaluated are divided into groups so that the VRAM required to evaluate each group is less than what is available. The number of groups is determined by dividing the overall memory requirements of the total image by the memory that is available. Using a simplified approach the total memory requirements of the image is the number of pixels in the image times the number of points that need to be evaluated for each pixel (for the purpose of super sampling) times the number of speckles. This is divided by the available memory which is 44567000 for the computer used during this project. If a different computer is used this value should be adjusted. Note that these memory values are relative values and have no units.

% Presently the points to be investigated are defined to be at the centre of the pixel locations. However for the purpose of jittered super sampling many points within a pixel must be investigated to give the overall light intensity of the pixel. This is done by applying matrices of displacement values (xgrid3 and ygrid3), which when added to the pixel centre location, creates a jittered grid of points to be investigated which falls within the fillfactor area of the pixel. In this way the super sampling locations for any pixel can be determined by adding these matrices to the location of the pixel centre. These matrices of displacement values are created in 3 steps.

% First the grid-spacing between the points of the unjittered grid in both directions (xdist and ydist) are determined based on the step size between pixels (stepsize - in displacement units), the fill factor (fillfactor) and the number of points in each direction of the grid (numpoints). The second step uses these grid-spacings to determine the x and y displacements of the unjittered grid (xgrid1 and ygrid1). These are then used to create 2 matrices of unjittered displacement values for the x and y direction (xgrid2 and ygrid2). 

% The final step involves jittering the displacement values. This is done by creating a 2 matrices of random values between -0.5 and 0.5, multiplying them by their corresponding grid-spacing and adding them to the matrices of unjittered displacement values to get the two final matrices of jittered displacement values (xgrid3 and ygrid3).These matrices are then reshaped to be row vectors (xxgrid and yygrid) which are passed to the GPU (stored in VRAM).

% Super sampling relies upon using the average of the sampled locations for each pixel to determine that pixels light intensity value. A Gaussian weighted average (H) is used to do this in order to allow the sampled locations closer to the pixel centre to have a greater influence on the light intensity value. The weighted average matrix is reshaped into a row vector and then this row vector is repeated to form a matrix. The number of rows in this matrix is equivalent to the number of pixels that are evaluated on each pass to the GPU.

% With all the necessary variables defined the calculation of light intensity values at specific locations in the image can commence. This is done between line 82 and 152. A counter is used to keep track of which group of elements is to be processed. The variables "begin" and "ending" correspond to the indexes of the first and last elements in X and Y that are to be to be processed while "ending2" gives the number of elements to be computed in each pass to the GPU. "ending2" is equivalent to "$num_of_elements$" except on the last pass to the GPU where a smaller number of pixels is processed.

% For the reference image ($image_count=1$) the points to be evaluated (Xin and Yin) are determined by adding the jittered displacement matrices (xxgrid and yygrid) to the group of x and y positions (X and Y). Note that since "X" and "Y" are column vectors with "$num_of_elements$" amount of rows and "xxgrid" and "yygrid" are row vectors with "numpoints" squared amount of columns the resulting matrices (Xin and Yin) are of size "$num_of_elements$" by "numpoints" squared. Each row in "Xin" and "Yin" is a collection of the jittered super sampling positions for a single pixel. Therefore each row corresponds to a pixel.

% For the case of the deformed image the points to be evaluated are determined by subtracting the displacement at that point (fux and fuy functions) from the group of x and y positions (X and Y) and then adding the jittered displacement matrices (xxgrid and yygrid). In doing so the pixel point is displaced in the opposite direction compared to what is described by the displacement function. This is done because displacement of the speckle pattern according the the displacement function while holding the pixel locations constant is equivalent to displacing the pixel locations in the opposite direction to the displacement function and holding the speckle pattern constant. 
% %Thus by altering the pixel locations according to the inverse of the displacement function the resulting image exhibits speckle pattern which has moved according to the displacement function.

% At this point the locations at which to evaluated the speckle pattern are available. To evaluate the light intensity at a location in the image involves four steps. The steps will be explained in terms of determining the light intensity at a single location although the algorithm determines this at multiple locations simultaneously. In the first step the distances between all the speckle centres and the location needs to be determined. To do this "Xin" and "Yin" are then reshaped to be column vectors (Xgrid and Ygrid) and the row vectors containing the location of the speckle centres (Xk and Yk) are subtracted from these to give "X1" and "Y1". Each row of "X1" and "Y1" contains the distance between each speckle centre and that specific location in the image.

% The second step involves determining the angles between the imaginary lines, that pass through the speckle centres and the location under consideration, and the x-axis. This is done by applying Matlab's atan2 function to the x and y distances "X1" and "Y1". The third step involves determining the radius of the each speckle along the imaginary line. This is done by applying the phase shifts and frequency changes to the angles, taking the sin of the angles, multiplying them by their respective amplitudes and adding the base radii. This results in the matrix "sins1". Each row of "sins1" gives the values for the radii of the all the speckles in the direction of the location under consideration.

% The fourth step requires determining how much each speckle contributes to the location under consideration. This is done by first determining the squared distance between the speckle centre and the location under consideration which is done using Pythagoras to get "M". The contribution of each speckle is determined by taking the exponent of the negative of this distance (M) divided by the speckle radius. This results in the matrix "temp". Each row in temp gives the contribution of each speckle to the point under consideration. This matrix is then multiplied by the column vector of the speckle light intensities to give a column vector of the overall light intensity at each location under consideration. This row vector is then reshaped to a matrix such that each row contains the light intensities at the jittered super sampling locations for a pixel. Then it is multiplied by the matrix of wieghted averages and each row is summed to give a column vector of pixel light intensities. 

% Finally these values are collected from the GPU and stored in a column vector on the CPU. Once this vector is fully populated it is reshaped to the size of the desired image.




\chapter{Validation}
The algorithms presented here need to be validated in order to prove that they can perform correlation to an acceptable level of accuracy and precision. Two methods of validation have been proposed; \todo{not sure if semi-colon is correct} synthetic and experimental.

These methods differ in how the image sets, which are to be correlated, are obtained. For the synthetic method the images are generated artificially by applying a displacement field to a speckle pattern whereas the experimental method captures images of a specimen as it deforms. The reason for using both methods is that they validate the algorithm for different situations. The synthetic method can create image sets which are favourable or unfavourable for DIC allowing the algorithm to be tested for best and worst case scenarios. On the other hand the experimental method tests the algorithms performance in real world situations for which it is intended to be used.

Additionally these methods differ in how the correlation performance of the algorithm is determined. For synthetic tests the displacement field is known and so the calculated displacements can be compared directly to those applied to the speckle pattern. This allows the performance to be measured in a quantitative manner. In contrast the displacement fields in the experimental images are unknown and so the displacements calculated by the algorithm are compared to displacements calculated by a commercial DIC algorithm. Thus the performance is measured in a qualitative manner.

Thus by including both of these methods of validation the algorithms - 

The displacement fields chosen are those that exhibit features which DIC generally struggles with. This includes high displacement gradients and displacement discontinuities.

Talk about types of displacement fields and why those
similarity between synthetic and experimental in terms of displacement fields
How each specimen/disp fields performance will be measured - what calcs/ std error MTF correlation value for subset over crack/at crack tip

\section{Synthetic tests}
The synthetic tests are those in which correlation is performed on sets of images that have been created synthetically. The reason for creating images synthetically is because this allows a large degree of control over the inputs to the ill-posed correlation problem. This degree of control extends to the displacement field that is present between images and the quality of the information contained in the images. This quality of information refers to the degree of noise present in the image and the bit-depth used to digitize the image.

Having such a large degree of control over the images to be correlated is beneficial because sets of images can be created which range from favourable to unfavourable allowing the capabilities of the algorithm to be tested. More specifically image sets of continuous, low-complexity deformation fields with no noise and high resolution can be used to determine the best performance the algorithm can achieve. Conversely images of discontinuous, complex deformation fields with substantial noise will give an indication of the limitations of the algorithm.

Additionally since the displacement field present between images is known exactly the accuracy and precision of the calculated displacement field, from correlation, can be determined quantitatively without bias \todo{not sure if this is the right word}.

\subsection{Generating synthetic images}
The generated image must contain a speckle pattern since it is this highly random distribution of positive and negative space that DIC relies upon to identify optical flow between images \todo{check this is correct}. First a continuous, two-dimensional function, $f_{sp}(x,y)$, is created which defines a speckle pattern that spans the range of the desired image. Thus this function gives the light intensity at any location in the image due to the speckle pattern it defines. Therefore by sampling this function at locations corresponding to the centre of pixels the image matrix can be generated. The speckle function is designed to be continuous so that no interpolation is required to sample it since this interpolation would introduce undesired bias in the generated images.

Sampling the speckle function at pixel centre locations is however \todo{however used twice too close to one another} not fully representative of how digital images are actually captured. The light intensity for each pixel in a captured image is determined by the voltage generated by the pixel's corresponding CCD array element. However an element of the CCD array outputs a voltage that corresponds to all the light that falls upon its active surface and not just the light that falls at its centre.

To accurately simulate this a process called stochastic sampling \todo{ref here too?} is used which is a form of super sampling. Super sampling, in this context, refers to sampling the speckle function many times, at different positions that fall within the area of the same pixel (CCD array), and averaging these function values to obtain the light intensity value for that pixel. The easiest method of doing this is to create a grid of points that fall within a pixel and evaluate the speckle function at these points. Then use a weighted average to determine the light intensity for the pixel. However this can lead to aliasing in the image where the digitized image does not accurately represent the continuous image.

To avoid this the grid can be jittered whereby each grid point is translated by a random amount, less than the grid spacing, in a random direction. The speckle function is then sampled at these jittered positions. This is referred to as stochastic sampling. By doing this the aliasing effect is reduced at the expense of slightly increasing the noise contained in the image \cite{beets2000super}.

Another effect that needs to be considered is the fill factor of the CCD array. To understand this first consider how light falls upon an element of the CCD array. Each element of the array has a portion of its surface, generally the perimeter, which is not sensitive to light. Thus light that falls upon this portion is not picked up by the CCD array and does not influence the image recorded. The ratio of photo-sensitive surface area to the total surface area of an element of the CCD array is termed the fill-factor. 

This effect is simulated by not sampling the speckle function over areas that would correspond to light insensitive portions of the CCD array. More specifically if a pixel of width $w_{pixel}$ is located at point $(x_c,y_c)$ then the position of the grid points in the x direction would range from $x_c-\frac{w_{pixel}}{2}$ to $x_c+\frac{w_{pixel}}{2}$ if the fill-factor was not taken into account. But if the fill-factor is to be accounted for then the position of the grid points in the x direction would range from $x_c-\frac{w_{pixel}}{2}\times \left( 1-\frac{fillfactor}{2} \right)$ to $x_c+\frac{w_{pixel}}{2}\times \left( 1-\frac{fillfactor}{2} \right)$. Note that this is done prior to jittering the grid points.

Once the image matrix has been generated the light intensities need to be quantised according to 12 bit quantization to simulate the camera's quantization process.

\subsubsection{Speckle function}
The function used to create a speckle pattern is based on that used by B. Pan \cite{bing2006performance} of the form
\begin{equation}
  % F=\sum_{k=1}^{s} I \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{R^2} \right] }
  f_{sp}(x,y) = \sum_{k=1}^{s} I_k \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{R_k^2} \right] } \label{eq: single speckle}
\end{equation}

This function creates circular speckles that are centred at the locations $(x_k,y_k)$, with light-intensity amplitudes of $I_k$ and radii of $R_k$. The light intensity of each speckle is constant within the radius of the speckle and decays exponentially as the distance from the speckle centre exceeds the radius.  However speckles created in real life, by spraying paint, are seldom perfectly circular. %By summing this equation for many speckles at random locations and of varying size a speckle pattern function is created.

In order to account for this the radius is modified to be a function of the angle between the radius line and the x-axis, referred to as the radius angle ($\theta_R$ in Figure \ref{fig: specklerad}). In other words the radius is a function of the arctan of the vector suspended between the speckle centre and the investigated point in the image. Thus the radius function is a function of the coordinates of the investigated point in the image given as
\begin{align}
  % R=\frac{I}{3} \frac{(10^(1/c) -1)}{9} \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a_{shift} \right) \times b_{period} \right) +\frac{2}{3}I
  R(x,y) &=\frac{R_{in}}{3} \times f_{amp}(b) \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a \right) \times b \right) +\frac{2R_{in}}{3} \label{eq: speckle radius} \\
  \text{where} \quad f_{amp}(b) &= 0.1+2.44 \times e^{-b} %\frac{10^{\frac{1}{b}}-1}{9}
\end{align}
Here $R_{in}$ is the user specified speckle radius which is predefined along with the variables $a$, $b$, $x_k$ and $y_k$. The combination of these variables defines \todo{define vs defines?} the speckle's shape.

This function varies the base radius of a circle, the last term in the equation, in a sinusoidal manner to create more irregular speckle. The sinusoid has its phase shifted by $a$ and its frequency altered by $b$. The phase shift essentially rotates the created speckle while altering the frequency changes the shape of the speckle. Thus these two variables determine the overall shape of the resulting speckle as illustrated in Figure \ref{fig: specklerad}. Therefore a large variety of speckle shapes can be created by using different combinations of phase shifts and frequencies.

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[width=\textwidth]{speckleradius02.jpg}
%         \caption{$a=0$, $b=2$}
%         % \label{fig: specklerad 1}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%       %(or a blank line to force the subfigure onto a new line)
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[width=\textwidth]{speckleradius12.jpg}
%         \caption{$a=1$, $b=2$}
%         % \label{fig: specklerad 2}
%     \end{subfigure}
%     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%       %(or a blank line to force the subfigure onto a new line)
%     \begin{subfigure}[b]{0.3\textwidth}
%         \includegraphics[width=\textwidth]{speckleradius03.jpg}
%         \caption{$a=0$, $b=3$}
%         % \label{fig: specklerad 3}
%     \end{subfigure}
%     % ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%     %   %(or a blank line to force the subfigure onto a new line)
%     % \begin{subfigure}[b]{0.45\textwidth}
%     %     \includegraphics[width=\textwidth]{speckleradius053.jpg}
%     %     \caption{$a=0.5$, $b=3$}
%     %     \label{fig: specklerad 2}
%     % \end{subfigure}
%     \caption{The different speckle shapes created by changing variables $a$ and $b$}
%     \label{fig: specklerad}
% \end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
        \input{specklerad02.tex}
        }
        \caption{$a=0$, $b=2$, $R_{in}=15$}
        % \label{fig: specklerad 1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
        \input{specklerad053.tex}
        }
        \caption{$a=0.5$, $b=3$, $R_{in}=15$}
        % \label{fig: specklerad 2}
    \end{subfigure}
    % \fcolorbox{black}{red}{\rule{0pt}{6pt}\rule{6pt}{0pt}}\quad Modified radius according to equation \ref{eq: speckle radius}.
    \caption{The different speckle shapes created by changing variables $a$ and $b$. Red indicates the modified radius according to equation \ref{eq: speckle radius} and blue indicates the base radius $\frac{2R_{in}}{3}$.}
    \label{fig: specklerad}
\end{figure}


% \begin{figure}[H]
% \begin{tikzpicture}
%   \draw [->] (0,0) -- (9,0);
%   \draw [->] (0,0) -- (0,5);
%   % \draw [thick] (0,3) to [out=-60,in=-135] (8,4);
%   \draw[red, ultra thick, domain=0:8] plot (\x, {((\x-4.5)/3)^2+0.5});
%   \node [below] at (0,0) {\tiny $\alpha_0$};
%   \node [below] at (8,0) {\tiny $\alpha_1$};
%   \node [below] at (3.05,0) {\tiny $\alpha_2$};
%   \node [below] at (4.94,0) {\tiny $\alpha_3$};
%   \draw [-] (3.05,-0.1) -- (3.05,0.731769038890291);
%   \draw [-] (4.94,-0.1) -- (4.94,0.521930836668256);
%   \draw [-] (0,0) -- (0,-0.1);
%   \draw [-] (8,-0.1) -- (8,1.861111111111111);
%   \node [left,rotate=90] at (-0.3,4) {\small Correlation Coefficient};
%   \node [below] at (4.5,-0.7) {\small Step size in search direction};
%   \node [above] at (3.05,0.8) {\tiny $C_2$};
%   \node [above] at (4.94,0.6) {\tiny $C_3$};
% \end{tikzpicture}
% \caption{First iteration of golden section search method}
% \label{fig: golden section 1dfs}
% \end{figure}


% \input{myfile.tex}

The amplitude of the sinusoid is set to a third of the desired speckle size and it is altered according to the function $f_{amp}$ which is a function of the frequency. It was found that when the frequency of the sinusoid increased the resulting speckle would exhibit a star shape, as in Figure \ref{fig: star speckle 1a}, which is undesirable. This function, $f_{amp}$, corrects for this by decreasing the amplitude of the sinusoid as the frequency increases.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{star_speckle.jpg}
        \caption{Undesired star shaped speckle}
        \label{fig: star speckle 1a}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{speckle_5.jpg}
        \caption{Star shaped speckle corrected}
        % \label{fig: speckle 2a}
    \end{subfigure}
    \caption{The effect of the function $f_{amp}$ for higher frequencies}
    \label{fig: star speckle}
\end{figure}

Therefore the final speckle function is obtained by substituting Equation \ref{eq: speckle radius} into Equation \ref{eq: single speckle} to obtain
\begin{equation}
  f_{sp}(x,y) = \sum_{k=1}^{s} I_k \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{\left( \frac{R_{in}}{3} \times f_{amp}(b) \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a_k \right) \times b_k \right) +\frac{2R_{in}}{3} \right)^2} \right] }
\end{equation}

\subsubsection{Deformed images}
At this point the method for generating the reference image is fully outlined however deformed versions of this reference image are required to perform correlation on. A displacement field is required for this purpose. A displacement field consists of two 2D functions which describe the displacement in the x and y directions of the speckle pattern throughout the image, these are $f_{ux}(x,y)$ and $f_{uy}(x,y)$ respectively. 

Although this displacement field describes how the speckle pattern should deform it does not have to be applied to the speckle pattern directly to achieve this. Instead the inverse of the displacement field (displacement in the direction opposite to what the displacement field describes) can be applied to the grid of pixel centre locations. In doing so the locations where the speckle pattern is sampled change thereby simulating the deformation between the images that is described by the displacement field.

It is for this reason that the speckle function is created to be continuous so that no interpolation is needed to determine the light intensity at these new pixel locations.

Therefore if the reference speckle image is created by sampling the speckle function $f_{sp}$ at pixel locations $(\bm{x},\bm{y})$ like so \todo{not formal enough} 
\begin{equation}
  \bm{F_{speckle}} = f_{sp}(\bm{x},\bm{y}) 
\end{equation}
then the deformed image is obtain as
\begin{equation}
  \bm{G_{speckle}} = f_{sp}(\bm{x}-f_{ux}(\bm{x},\bm{y}),\bm{y}-f_{uy}(\bm{x},\bm{y})) 
\end{equation}

The grid of pixel centre locations are displaced as above prior to performing stochastic sampling.

\subsubsection{Code explanation}
First the function creates two matrices of points (X and Y) that correspond to the centre location of the pixels which are to be generated. Thereafter between lines 17 and 33 matrices of jittered displacements are created for the purpose of stochastic sampling. By adding these matrices to a pixel centre location a grid of jittered positions that fall within the fillfactor area of the pixel are obtained. Thus the same jittered grid is used for each pixel in order to improve efficiency of the algorithm.

Thereafter the function defines the speckle pattern by assigning values to the properties of each speckle. First the vector of speckle sizes is converted from units of pixels to the units used in the displacement function hereafter referred to as displacement units (line 40). It was originally in units of pixels because for a good image the speckle sizes must be proportional to the subset size which is in units of pixels. The position of the speckles are then defined, in displacement units, at random locations that fall within the range of the image (lines 42-43). The light intensity at the centre of each speckle is then randomly defined to fall in the range between 0.5 and 1 where 1 is white and 0 is black (line 44).

The for loop starting at line 54 saves the details of every speckle. First a random phase shift (a) and frequency (b) are defined. The phase shift falls in the range between $0$ and $2 \pi$ so that the speckle has equal chance of being rotated to any angle. The frequency is defined such that between 1 and 5 complete cycles of the sinusoid occur within one full revolution of the radius angle. Note that the frequency is only permitted to be whole numbers so that the radius of the speckle is continuous. The five possible speckle shapes resulting from the five different frequencies are illustrated in Figure \ref{fig: speckle shapes}. The amplitude (c) is then determined based on the frequency using the function on line 48. The base radius (d), x and y positions (Xk and Yk) and the light intensity (I0) are then all saved.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{speckle_1.jpg}
        \caption{b=1}
        % \label{fig: speckle 1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{speckle_2.jpg}
        \caption{b=2}
        % \label{fig: speckle 2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{speckle_3.jpg}
        \caption{b=3}
        % \label{fig: speckle 3}
    \end{subfigure}
     ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{speckle_4.jpg}
        \caption{b=4}
        % \label{fig: speckle 4}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{speckle_5.jpg}
        \caption{b=5}
        % \label{fig: speckle 5}
    \end{subfigure}
    \caption{Speckle shapes as a result of the frequency (b) of the sinusoid}\label{fig: speckle shapes}
\end{figure}

Stochastic sampling relies upon using the average of the sampled locations for each pixel to determine that pixels light intensity value. A Gaussian weighted average (H at line 72) is used to do this in order to allow the sampled locations closer to the pixel centre to have a greater influence on the light intensity value. 

With all the necessary variables defined the calculation of light intensity values at specific locations in the image can commence. This is done between lines 79 and 154. First the locations where the speckle function is to be sampled are defined between lines 94-100. For the reference image (imageCount=1) the points to be evaluated (Xin and Yin) are determined by adding the row vectors of jittered displacements (xxgrid and yygrid) to the column vectors of x and y pixel centre locations (X and Y).

For the case of the deformed image the points to be evaluated are determined by subtracting the displacement at that point (fux and fuy functions) from the group of x and y positions (X and Y) and then adding the jittered displacement row vectors (xxgrid and yygrid). 

% In doing so the pixel point is displaced in the opposite direction compared to what is described by the displacement function. This is done because displacement of the speckle pattern according the the displacement function while holding the pixel locations constant is equivalent to displacing the pixel locations in the opposite direction to the displacement function and holding the speckle pattern constant. 

Evaluating the light intensity at a location in the image involves five steps. First for each sampling location the distance between it and all the speckle centres needs to be determined (line 108-109).  

% The second step involves determining the angles between the imaginary lines, that pass through the speckle centres and the location under consideration, and the x-axis.
The second step involves determining the radius angle for each speckle such that the sampling location lies along the radius line at this angle. This is done by applying Matlab's atan2 function to the x and y distances "X1" and "Y1". The third step involves determining the radius of each speckle at these radius angles. This is done by applying the phase shifts and frequency changes to the angles, taking the sin of the angles, multiplying them by their respective amplitudes and adding the base radii. This results in the matrix "sins1" (line 114-117). 

The fourth step requires determining how much each speckle contributes to the sampling location. This is done by first determining the squared distance between the speckle centre and the location under consideration which is done using Pythagoras to get "M" (line 123-127). The contribution of each speckle is determined by taking the exponent of the negative of this distance (M) divided by the speckle radius squared (line 132). This results in the matrix "speckleContribution".

In the final step the overall light intensity at the sampling location can be determined by multiplying the contribution of each speckle by its light intensity to get "Image\_temp".

The light intensities are then rearranged to that the light intensities that contribute to the same pixel are grouped together (line 137-140). These groups of light intensities are then multiplied by their corresponding weighted average and then summed to obtain the light intensities for pixels of the image (line 143). 

Once both of the image matrices have been fully populated their light intensities are quantized according to 12 bit quantisation to be consistent with the cameras used during experimental tests (line 159-160). This is done using Matlab's imquantize function (line 169).

Since sampling the speckle function (line 83-153) involves a large number of simple operations it was discovered that it runs faster on a graphics processing unit (GPU). It is for this reason that Matlab's reshape function is often used to vectorise matrices so that the sampling process can be performed using matrix operations. In doing so multiple locations in the image are sampled simultaneously resulting in faster computation of the synthetic images. 

However GPUs have a limited amount of memory and so the number of points that can be evaluated simultaneously is limited. To cope with this the points to be evaluated are divided into groups so that the memory required to evaluate each group is less than what is available. This is the purpose of the for loop starting at line 83. 

The number of groups is determined by dividing the overall memory requirements of the total image by the memory that is available. Using a simplified approach the total memory requirements of the image is the number of pixels in the image times the number of points that need to be evaluated for each pixel times the number of speckles. This is divided by the available memory which is 44567000 for the computer used during this project (line 67). If a different computer is used this value should be adjusted. %Note that these memory values are relative values and have no units.






%\subsubsection{old code explanation for synthetic images}
% It was discovered that because creating synthetic images involves a large number of simple operations it would run much faster on a graphics processing unit (GPU). In order to do this the equation \ref{eq: speckle} had to be vectorized \todo{not sure this word is 100\% accurate} so that the light intensities at many points in the image could be evaluated simultaneously using matrix multiplication. 

% First the function creates a grid of points (X and Y) that correspond to the centre location of the pixels which are to be generated. However a jittered grid of points is required for each pixel for the purposes of jittered super sampling. These points are obtained by displacing the pixel centre location, many times, to obtain a jittered grid of points that fall within the fillfactor area of the pixel. The same displacements are applied to each pixel centre location to create its corresponding jittered grid. Since multiple points are created for each pixel, the pixel locations are reshaped into a column vector (line 13 and 14) and the displacements are added as row vectors resulting in a matrix where each row holds the super sampling locations for each pixel. 

% The row vector of displacement values are created in 3 steps. First the grid-spacing between the points of the unjittered grid are determined in both directions (xdist and ydist) based on the step size between pixels (stepsize), the fill factor (fillfactor) and the number of points in each direction of the grid (numpoints). The second step uses these grid-spacings to determine the x and y displacements of the unjittered grid (xgrid1 and ygrid1). These are then used to create 2 matrices of unjittered displacement values for the x and y direction (xgrid2 and ygrid2). 

% The final step involves jittering the displacement values. This is done by creating a 2 matrices of random values between -0.5 and 0.5, multiplying them by their corresponding grid-spacing and adding them to the matrices of unjittered displacement values to get the two final matrices of jittered displacement values (xgrid3 and ygrid3 at line 32). These matrices are then reshaped to be row vectors (xxgrid and yygrid) which are passed to the GPU to be stored in VRAM (Video Random Access Memory).

% Thereafter the function defines the speckle pattern by assigning values to the properties of each speckle. First the vector of speckle sizes is converted from units of pixels to the units used in the displacement function hereafter referred to as displacement units (line 40). It was originally in units of pixels because for a good image the speckle sizes must be proportional to the subset size which is in units of pixels. The position of the speckles are then defined ,in displacement units, at random locations that fall within the range of the image. The light intensity at the centre of each speckle is then randomly defined to fall in the range between 0.5 and 1 where 1 is white and 0 is black.

% The choice to alter the speckle radius in a sinusoidal manner lead to some issues when the frequency of the sinusoid increased. At higher frequencies the resulting speckle would represent a star shape which is an unrealistic speckle shape. In order to adjust for this the amplitude of the sinusoid was adjusted based on its frequency using the function defined on line 48. This function exponentially decreases the amplitude of the sinusoid as the frequency increases.

% The for loop starting at line 54 saves the details of every speckle. First a random phase shift (a) and frequency (b) are defined. The amplitude (c) is then determined based on the frequency using the function on line 48. The base radius (d), x and y positions (Xk and Yk) and the light intensity (I0) are then all saved.

% GPUs have a limited amount of VRAM and so the number of points that can be evaluated at once is limited. To cope with this the points to be evaluated are divided into groups so that the VRAM required to evaluate each group is less than what is available. The number of groups is determined by dividing the overall memory requirements of the total image by the memory that is available. Using a simplified approach the total memory requirements of the image is the number of pixels in the image times the number of points that need to be evaluated for each pixel times the number of speckles. This is divided by the available memory which is 44567000 for the computer used during this project. If a different computer is used this value should be adjusted. Note that these memory values are relative values and have no units.

% Super sampling relies upon using the average of the sampled locations for each pixel to determine that pixels light intensity value. A Gaussian weighted average (H at line 72) is used to do this in order to allow the sampled locations closer to the pixel centre to have a greater influence on the light intensity value. The weighted average matrix is reshaped into a row vector and then this row vector is repeated to form a matrix (HH). The number of rows in this matrix is equivalent to the number of pixels that are evaluated on each pass to the GPU.

% With all the necessary variables defined the calculation of light intensity values at specific locations in the image can commence. This is done between lines 79 and 154. A counter is used to keep track of which group of elements is to be processed. The variables "begin" and "ending" correspond to the indexes of the first and last elements in X and Y that are to be to be processed (which pixels are to be processed) while "ending2" gives the number of pixels to be computed in each pass to the GPU ("ending2" is equal to "numOfPixels" except on the last pass to the GPU where a smaller number of pixels is processed)

% For the reference image (imageCount=1) the points to be evaluated (Xin and Yin) are determined by adding the row vectors of jittered displacements (xxgrid and yygrid) to the column vectors of x and y positions (X and Y). Note that since "X" and "Y" are column vectors with "numOfPixels" amount of rows and "xxgrid" and "yygrid" are row vectors with "numpoints" squared amount of columns the resulting matrices (Xin and Yin) are of size "numOfPixels" by "numpoints" squared. Each row in "Xin" and "Yin" is a collection of the jittered super sampling positions for a single pixel. Therefore each row corresponds to a pixel.

% For the case of the deformed image the points to be evaluated are determined by subtracting the displacement at that point (fux and fuy functions) from the group of x and y positions (X and Y) and then adding the jittered displacement matrices (xxgrid and yygrid). In doing so the pixel point is displaced in the opposite direction compared to what is described by the displacement function. This is done because displacement of the speckle pattern according the the displacement function while holding the pixel locations constant is equivalent to displacing the pixel locations in the opposite direction to the displacement function and holding the speckle pattern constant. 

% At this point the locations at which to evaluated the speckle pattern are available. To evaluate the light intensity at a location in the image involves four steps. The steps will be explained in terms of determining the light intensity at a single location although the algorithm determines this at multiple locations simultaneously. In the first step the distances between all the speckle centres and the location needs to be determined. To do this "Xin" and "Yin" are then reshaped to be column vectors (Xgrid and Ygrid) and the row vectors containing the location of the speckle centres (Xk and Yk) are subtracted from these to give "X1" and "Y1". Each row of "X1" and "Y1" contains the distance between each speckle centre and the specific location in the image.

% The second step involves determining the angles between the imaginary lines, that pass through the speckle centres and the location under consideration, and the x-axis. This is done by applying Matlab's atan2 function to the x and y distances "X1" and "Y1". The third step involves determining the radius of the each speckle along the imaginary line. This is done by applying the phase shifts and frequency changes to the angles, taking the sin of the angles, multiplying them by their respective amplitudes and adding the base radii. This results in the matrix "sins1". Each row of "sins1" gives the radii of the all the speckles in the direction of the location under consideration.

% The fourth step requires determining how much each speckle contributes to the location under consideration. This is done by first determining the squared distance between the speckle centre and the location under consideration which is done using Pythagoras to get "M". The contribution of each speckle is determined by taking the exponent of the negative of this distance (M) divided by the speckle radius squared. This results in the matrix "speckleContribution". 

% Each row in "speckleContribution" gives the contribution of each speckle to the point under consideration. This matrix is then multiplied by the column vector of the speckle light intensities to give a column vector of the overall light intensity at each location under consideration. This row vector is then reshaped to a matrix such that each row contains the light intensities at the jittered super sampling locations for a pixel. Then it is multiplied by the matrix of weighted averages and each row is summed to give a column vector of pixel light intensities. Throughout this process variables are cleared once they are no longer needed in order to free up space on the VRAM.

% Finally these values are collected from the GPU and stored in a column vector on the CPU. Once this vector is fully populated it is reshaped to the size of the desired image and the light intensity values are quantized using Matlab's "imquantize" function. This is 12 bit quantization which is consistent with the quantization that takes place within the camera used for experiments.

% \subsection{old version}
% The generated image must contain a speckle pattern since it is this highly random distribution of positive and negative space that DIC relies upon to identify optical flow \todo{check this is correct}. Although many methods exist for creating synthetic speckle pattern images the method chosen in this project is to first create a continuous function for the speckle pattern and then sample it at pixel locations. In this way there is no need to perform interpolation during image creation which would otherwise introduce errors.

% The function used to create speckles is based on that used by B. Pan \cite{bing2006performance} of the form
% \begin{equation}
%   % F=\sum_{k=1}^{s} I \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{R^2} \right] }
%   F_{single\_speckle}(x,y) = I \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{R^2} \right] }
% \end{equation}

% This function creates a circular speckle that is centred at $(x_k,y_k)$, with a light-intensity amplitude of $I$ and a radius of $R$. The light intensity of the speckle is constant within the radius of the speckle and decays exponentially as the distance from the speckle centre exceeds the radius. By summing this equation for many speckles at random locations and of varying size a speckle pattern function is created. However speckles created in real life, by spraying paint, are seldom perfectly circular. 

% In order to account for this the radius is modified to be a function of the angle between the radius line and the x-axis. In other words the radius is a function of the arctan of the vector suspended between the speckle centre and the investigated point in the image. The radius function is given as
% \begin{align}
%   % R=\frac{I}{3} \frac{(10^(1/c) -1)}{9} \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a_{shift} \right) \times b_{period} \right) +\frac{2}{3}I
%   R(x,y) &=\frac{R_{in}}{3} \times f_{amp}(b) \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a \right) \times b \right) +\frac{2}{3}R_{in}\\
%   \text{where} & \quad f_{amp}(b) = \frac{10^{\frac{1}{b}}-1}{9}
% \end{align}

% In order to account for this the radius is modified to be a function of the angle of the line suspended between the speckle centre and the investigated point. In other words the radius is a function of the arctan of the vector suspended between the investigated point and the speckle centre. The radius function is given as
% \begin{align}
%   % R=\frac{I}{3} \frac{(10^(1/c) -1)}{9} \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a_{shift} \right) \times b_{period} \right) +\frac{2}{3}I
%   R(x,y) &=\frac{R_{in}}{3} \times f_{amp}(b) \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a \right) \times b \right) +\frac{2}{3}R_{in}\\
%   \text{where} & \quad f_{amp}(b) = \frac{10^{\frac{1}{b}}-1}{9}
% \end{align}

% where $R_{in}$ is the user specified speckle size and $a$ and $b$ are random numbers that fall in the range
% \begin{align}
%   a \in \mathbb{R} &: a \in \left[0, 2 \pi  \right] \\
%   b \in \mathbb{Z} &: b \in \left[ 1, 5 \right]
% \end{align}

% Thus the radius varies in a sinusoidal manner where the sinusoid is shifted in phase by $a$ and has a period of $\frac{2 \pi}{b}$. The amplitude of the sinusoid is set to be a third of the desired radius but it is adjusted according to the period of the sinusoid using the function $f_{amp}$. This function reduces the amplitude as the period of the sinusoid decreases. Without this function the speckles produced for higher frequency sinusoids (lower period) tend to exhibit a star shape which is not a realistic speckle shape.

% The amplitude of the sinusoid depends on the two variables $R_{in}$ and $b$. 

% Note that the the amplitude of the sinusoid is a third of the speckle size, $\frac{R_{in}}{3}$, in order to ensure that the speckle remains predominantly circular. The function $f_{amp}$ is used to adjust the 

% Show how the parameters of the speckle change how it looks
% define the angle in a more sensical way

% Therefore the function used to generate a continuous speckle pattern function is that of 
% \begin{equation}
%   F_{speckle}(x,y) = \sum_{k=1}^{s} I_k \exp{ \left[ -\frac{ \left( x-x_k \right) ^2 + \left( y-y_k \right) ^2}{\left( \frac{R_{base}}{3} \times \sin \left( \left( \arctan{ \left( \frac{y-y_k}{x-x_k} \right) } +a_k \right) \times b_k \right) +\frac{2}{3}R_{base} \right)^2} \right] }
% \end{equation}

% This is the speckle pattern for the reference image. To create the speckle pattern for the deformed image two functions describing the displacement in the x and y directions, as a function of the position, are required. If these are $f_{ux}(x,y)$ and $f_{uy}(x,y)$ then the speckle pattern for the deformed image is
% \begin{align}
%   G_{speckle}(x,y)&=F_{speckle}(x-f_{ux}(x,y),y-f_{uy}(x,y)) \\
%   \label{eq: speckle}
%   &= \sum_{k=1}^{s} I_k \exp{ \left[ -\frac{ \left( x-f_{ux}(x,y)-x_k \right) ^2 + \left( y-f_{uy}(x,y)-y_k \right) ^2}{\left( \frac{R_{base}}{3} \times \sin \left( \left( \arctan{ \left( \frac{y-f_{uy}(x,y)-y_k}{x-f_{ux}(x,y)-x_k} \right) } +a_k \right) \times b_k \right) +\frac{2}{3}R_{base} \right)^2} \right] } 
% \end{align}

% The next step is to simulate the image acquisition process. The speckle pattern functions described above simulates the light that falls upon the CCD array of the camera. This continuous speckle function needs to be converted to a digital image. This could be done by sampling the speckle function at positions corresponding to the centroids of the pixels in the image and taking this as the light intensity for the pixel however this is not representative of how CCD arrays actually function. An element of the CCD array outputs a voltage that corresponds to all the light that falls upon it and not just the light that falls at its centre.

% To accurately simulate the CCD array a process called jittered super sampling is used. Super sampling, in this context, refers to sampling the speckle function many times, at different positions that fall within the same pixel area (CCD array), and averaging the function values to obtain the light intensity value for that pixel. The easiest method of doing this is to create a grid of points that fall within a pixel and evaluate the speckle function at these points. Then use a weighted average to determine the light intensity for the pixel. However this can lead to aliasing in the image where the digitized image does not accurately represent the continuous image.

% To avoid this the grid can be jittered whereby each grid point is translated by a random amount, less than the grid spacing, in a random direction. The speckle function is then sampled at these jittered positions. By doing this the aliasing effect is reduced at the expense of slightly increasing the noise contained in the image \cite{beets2000super}.

% Another effect that is considered is the fill factor of the CCD array. To understand this first consider how light falls upon an element of the CCD array. Each element of the array has a portion of its surface, generally the perimeter, which is not sensitive to light. Thus light that falls upon this portion is not picked up by the CCD array and does not influence the image recorded. The ratio of photo-sensitive surface area to the total surface area of an element of the CCD array is termed the fill-factor. 

% This effect is simulated by not sampling the speckle function over areas that would correspond to light insensitive portions of the CCD array. More specifically if a pixel of width $w_{pixel}$ is located at point $(x_c,y_c)$ then the position of the grid points in the x direction would range from $x_c-\frac{w_{pixel}}{2}$ to $x_c+\frac{w_{pixel}}{2}$ if the fill-factor was not taken into account. But if the fill-factor is to be accounted for then the position of the grid points in the x direction would range from $x_c-\frac{w_{pixel}}{2}\times \left( 1-\frac{fillfactor}{2} \right)$ to $x_c+\frac{w_{pixel}}{2}\times \left( 1-\frac{fillfactor}{2} \right)$. Thus the grid is squeezed toward the pixel centre. Note that this is done prior to jittering the grid points.

% The final step is to quantize the light intensities of the image matrix. This is done using Matlab's imquantize function.

\subsection{Displacement fields}
A variety of displacement fields have been used to create deformed speckle pattern images.

\subsubsection{Plate with hole}
Interesting displacement fields often occur in the vicinity of a stress concentration. A hole in a plate is a specimen geometry that causes a such a stress concentration. This specimen geometry was chosen because equations can be derived that describe the displacement the specimen experiences when subjected to a force. These equations are derived using the Airy stress function as done by Barber in \cite{barber2002solid}. The Airy stress function is a method of using equilibrium equations and boundary conditions of the specimen to solve for the two-dimensional stress and displacement fields experienced by the specimen.

For the case of a large plate with a circular hole in it experiencing a uniform normal stress, $T$, in the x direction the stress function is
\begin{equation}
  \phi = \frac{Tr^2}{4} - \frac{Tr^2 \cos(2 \theta)}{4} -\frac{Ta^2}{2} + \frac{Ta^2}{2} \cos(2 \theta) - \frac{Ta^4}{4r^2} \cos(2 \theta)
\end{equation}
where $a$ is the radius of the hole.

The displacement equations are derived from this by substituting in the appropriate displacement components from the Michell solution \cite{michell1899direct}. These components are tabulated in \cite{barber2002solid} \todo{keep this in or not?}. After substitution the displacement equations in polar coordinates become
\begin{align}
  2 \mu u_r &= \frac{T}{4} \left[ (k-1)r \right]-\frac{T}{4} \left[ -2r\cos(2\theta) \right] -\frac{Ta^2}{2} \left[ -\frac{1}{r} \right] +\frac{Ta^2}{2}\left[ \frac{k+1}{r} \cos(2 \theta)\right] - \frac{Ta^4}{4}\left[ \frac{2}{r^3} \cos(2 \theta) \right] \\
  2 \mu u_{\theta} &= -\frac{T}{4} \left[ 2r \sin(2 \theta) \right] + \frac{Ta^2}{2}\left[ -\frac{k-1}{r} \sin(2 \theta) \right]- \frac{Ta^4}{4} \left[ \frac{2}{r^3} \sin(2 \theta) \right]
\end{align}
where the substitutions are in square brackets. Further substitution of $\frac{E}{2(1+v)}$ for $\mu$ and $\frac{3-v}{1+v}$ for $k$ (plane stress condition) gives
\begin{align}
  u_r(r,\theta) &= \frac{Tr \cos(2 \theta)}{2E} \left[ 1+v+4\frac{a^2}{r^2} - (1+v)\frac{a^4}{r^4} \right] + \frac{Tr}{2E} \left[ 1-v+(1+v)\frac{a^2}{r^2} \right] \\
  u_{\theta}(r,\theta) &= - \frac{Tr \sin(2 \theta)}{2E}\left[ 1+v+2(1-v)\frac{a^2}{r^2} + (1+v)\frac{a^4}{r^4} \right]
\end{align}

These equations can be converted to describe the displacement in terms of cartesian coordinates using the following equations
\begin{align}
  u_x(r,\theta) = u_r(r,\theta) \cos(\theta) - u_{\theta}(r,\theta) \sin(\theta) \\
  u_y(r,\theta) = u_r(r,\theta) \sin(\theta) + u_{\theta}(r,\theta) \cos(\theta).
\end{align}
As a result he displacement equations in cartesian coordinates becomes 
\begin{align}
  u_x(r,\theta) &= \frac{T\,a}{8\,\mu }\left(\frac{2\,a}{r} \left(\cos\left(3\,t\right)+\cos\left(t\right)\,\left(k+1\right)\right)-\frac{2\,a^3}{r^3} \cos\left(3\,t\right)+\frac{r}{a} \cos\left(t\right)\,\left(k+1\right) \right)\\
  u_y(r,\theta) &= \frac{T\,a}{8\,\mu }\left(\frac{2\,a}{r} \left(\sin\left(3\,t\right)-\sin\left(t\right)\,\left(k-1\right)\right) -\frac{2\,a^3}{r^3} \sin\left(3\,t\right) +\frac{r}{a} \sin\left(t\right)\,\left(k-3\right) \right)
\end{align}
Making use of the knowledge that $r=\sqrt{x^2+y^2}$ and $\theta = \text{atan2} (y,x)$ the displacement equations can be rewritten in terms of only cartesian coordinates

\begin{align}
  u_x(x,y) &= \frac{T\,a}{8\,\mu } \bigg[\frac{2\,a}{\sqrt{x^2+y^2}} \left(\cos\left(3\,atan2(y,x) \right)+\cos\left( atan2(y,x) \right)\,\left(k+1\right)\right) \nonumber \\ &-\frac{2\,a^3}{\left( \sqrt{x^2+y^2} \right) ^3} \cos\left(3\, atan2(y,x) \right)+\frac{\sqrt{x^2+y^2}}{a} \cos\left( atan2(y,x) \right)\,\left(k+1\right)  \bigg] \\
  u_y(x,y) &= \frac{T\,a}{8\,\mu } \bigg[\frac{2\,a}{\sqrt{x^2+y^2}} \left(\sin\left(3\, atan2(y,x) \right)-\sin\left( atan2(y,x) \right)\,\left(k-1\right)\right) \nonumber \\ &-\frac{2\,a^3}{ \left( \sqrt{x^2+y^2} \right)^3} \sin\left(3\, atan2(y,x) \right) +\frac{\sqrt{x^2+y^2}}{a} \sin\left( atan2(y,x) \right)\,\left(k-3\right)  \bigg]
\end{align}
\todo{take this out since it is unnecessary?}
Note that atan2 is a Matlab function which computes the arctan of a vector and returns the angle corresponding to the correct quadrant within which that vector lies. These displacement fields are only valid for x and y positions that lie outside of the radius of the hole.

\subsubsection{Plate with crack}
Another specimen geometry that creates a stress concentration leading to interesting displacement fields is that of a large plate with a crack in it. Equations that describe the displacement of the specimen around the crack can be derived using the Williams approach \cite{williams1997stress}. The resulting equations for displacement in terms of cartesian coordinates, for mode I and II crack propagation, are then of the form \cite{yates2009quantifying}
\begin{align}
  \text{Mode I:}\\
  u_x(r,\theta) &= \sum_{n=1}^{\infty} \frac{r^\frac{n}{2}}{2 \mu} a_n \left[ \left( k+\frac{n}{2} + (-1)^n \right) \cos{\frac{n \theta}{2}} - \frac{n}{2} \cos{\frac{(n-4)\theta}{2}} \right] \\
  u_y(r,\theta) &= \sum_{n=1}^{\infty} \frac{r^\frac{n}{2}}{2 \mu} a_n \left[ \left( k-\frac{n}{2} - (-1)^n \right) \sin{\frac{n \theta}{2}} + \frac{n}{2} \sin{\frac{(n-4)\theta}{2}} \right]\\
  \text{Mode II:} \\
  u_x(r,\theta) &= -\sum_{n=1}^{\infty} \frac{r^\frac{n}{2}}{2 \mu} b_n \left[ \left( k+\frac{n}{2} - (-1)^n \right) \sin{\frac{n \theta}{2}} - \frac{n}{2} \cos{\frac{(n-4)\theta}{2}} \right] \\
  u_y(r,\theta) &= \sum_{n=1}^{\infty} \frac{r^\frac{n}{2}}{2 \mu} b_n \left[ \left( k-\frac{n}{2} + (-1)^n \right) \cos{\frac{n \theta}{2}} + \frac{n}{2} \cos{\frac{(n-4)\theta}{2}} \right]
\end{align}
Although the sum in the above equations is stated as infinite, a finite number of terms will approximate the displacement field sufficiently. 
% The above equations can be transformed in such that they depend on cartesian based positions as follows
% \begin{align}
%   u_x(x,y) &= \sum_{n=1}^{\infty} \frac{(\sqrt{x^2+y^2})^\frac{n}{2}}{2 \mu} a_n \left[ \left( k+\frac{n}{2} + (-1)^n \right) \cos{\frac{n \times atan2(y,x) }{2}} - \frac{n}{2} \cos{\frac{(n-4) \times atan2(y,x) }{2}} \right] \\
%   u_y(x,y) &= \sum_{n=1}^{\infty} \frac{(\sqrt{x^2+y^2})^\frac{n}{2}}{2 \mu} a_n \left[ \left( k-\frac{n}{2} - (-1)^n \right) \sin{\frac{n \times atan2(y,x) }{2}} + \frac{n}{2} \sin{\frac{(n-4) \times atan2(y,x) }{2}} \right]
% \end{align}
An interesting property of these equations is that the $a_1$ and $b_1$ terms are directly related to the mode I and mode II stress intensity factors respectively. In other words if a specimen with a crack exhibits a displacement field corresponding to these equations then the stress intensity factor of the material can be determined as
\begin{align}
  K_I &= a_1 \sqrt{2 \pi} \quad \ \ \text{for mode I} \\
  K_{II} &= -b_1 \sqrt{2 \pi} \quad \text{for mode II}
\end{align}

Plate with hole
Sine wave displacement fields
crack

\section{Experimental tests}
how the performance metric is calculated for each specimen

\subsection{Tension specimen with hole}

\subsection{Arcan specimen}
The arcan specimen was designed to investigate material behaviour under mixed mode (I and II) fracture. It is a symmetric specimen with a precrack at the plane of symmetry. A special type of grip is used for these specimens which allows them to be mounted in various orientations with respect to the force being applied. Thus the plane of the precrack can be adjusted to be at different angles to the direction of the applied force.

The reason for including this specimen is to test the subset splitting algorithm at various angles.
\subsection{CT specimen}

Has some rotation - diff to arcan

\chapter{Results}
This section provides the results of the Matlab codes in order to prove that it works. The results will be presented here in accordance to the layout of the project sections.

\section{Part A}
Part A dealt with calibration only. The intrinsic parameters determined were
\begin{equation}
  \bm{K} = \begin{bmatrix}
  657.62840 & 0.30335 & 303.72868 & 0\\
  0 & 658.47823 & 245.69177 & 0\\
  0 & 0 & 0 & 0
  \end{bmatrix}.
\end{equation}
The radial distortion parameters were determined to be $k_1 = -0.24885$ and $k_2 = 0.11014$. Using these calibration parameters the image "img01.tiff" was undistorted and is provided in figure \ref{fig:undist1}. Comparing this to the reference image in figure \ref{fig:reference1} it is clear that the calibration parameters determined are sufficiently accurate however differences between the images are noticeable. These differences are likely as a result of additional distortion types that have not been taken into account due to the complexity that these additional distortions would introduce.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{reference_image.jpg}
    \caption{Reference image for comparison purposes}
    \label{fig:reference1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{partA.jpg}
    \caption{The distorted image "img01.tiff" which has been corrected for distortions}
    \label{fig:undist1}
\end{figure}

\section{Part B}
Part B deals with the correlation process only. The warp function parameters determined by the Matlab code are presented below.
\begin{align}
  u &= 5.03754\\
  \frac{\partial u}{\partial x} &= 0.04073 \\
  \frac{\partial u}{\partial y} &= -0.02176 \\
  v &= -1.03562 \\
  \frac{\partial v}{\partial x} &= -0.00915 \\
  \frac{\partial v}{\partial y} &= 0.03048
\end{align}
The corresponding correlation coefficients are
\begin{align}
  C_{ZNSSD} &= 0.00587 \\
  C_{ZNCC} &= 0.99706.
\end{align}
Both correlation coefficients have been presented here in order to illustrate that the $C_{ZNCC}$ coefficient is much easier to interpret. The determined warp function parameters were used to undeform the image "img02.tiff" to obtain figure \ref{fig:undist2}. Comparing this to the reference figure in figure \ref{fig:reference2} it can be seen that the determined warp function parameters are sufficiently accurate.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{reference_image2.jpg}
    \caption{Reference image for comparison purposes}
    \label{fig:reference2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{partB.jpg}
    \caption{The distorted image "img02.tiff" which has been corrected for distortions}
    \label{fig:undist2}
\end{figure}
\section{Part C}
Part C combine both Part A and Part B. The calibration code of Part A is used to determine the calibration parameters for the camera system. These camera parameters are then used to undistort image "img03.tif" such that it is free of lens distortion. Then the correlation code is applied to this undistorted image in order to determine the warp function parameters. These warp function parameters are then used to undeform the image. This undeformed image is shown in figure \ref{fig:undist3}. Comparing this image to the original undistorted image in figure \ref{fig:reference3} it seems that the calibration parameters and warp function parameters were sufficiently accurate. However the actual warp function parameters differ from those of Part B as shown below and the correlation coefficient is not as good as in Part B.
\begin{align}
  u &= 5.11364\\
  \frac{\partial u}{\partial x} &= 0.04348 \\
  \frac{\partial u}{\partial y} &= -0.02214 \\
  v &= -1.06509 \\
  \frac{\partial v}{\partial x} &= -0.00918 \\
  \frac{\partial v}{\partial y} &= 0.03071
\end{align}

\begin{align}
  C_{ZNSSD} &= 0.00838 \\
  C_{ZNCC} &= 0.99581
\end{align}

The reason that these warp function parameters differ from those of Part B is that the this method of using the calibration parameters to undistort the image prior to performing correlation on the image is not correct. This is because interpolation is used to undistort the image and additional interpolation is used within the correlation code. 

Thus by introducing an extra case of interpolation the reliability of the data is compromised. This is reflected in the correlation coefficients which differ from those in Part B. Note that the standard deviations of the warp function parameters for both Part B and Part C were on the order of magnitude of $10^{-13}$ sampled over one thousand correlation runs. The calibration parameters for Part C are not presented since they are the same as the values in Part A.

% It is important to note that this method of using the calibration parameters to undistort the image prior to performing correlation on the image is not correct. This is because interpolation is used to undistort the image and additional interpolation is used within the correlation code. Thus by introducing an extra case of interpolation the reliability of the data is compromised.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{reference_image2.jpg}
    \caption{Reference image for comparison purposes}
    \label{fig:reference3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{partC.jpg}
    \caption{The distorted image "img03.tiff" which has been corrected for distortions}
    \label{fig:undist3}
\end{figure}

\chapter{Conclusion}
This document outlines the Digital Image Correlation process. It presents the background information on camera optics and the mathematical relation used to model these optics in a camera model. Thereafter the calibration process is discussed which determines the parameter values for the camera model. Correlation is then reviewed with focus on the inverse compositional Lucas-Kanade algorithm. Finally the results of the Matlab codes are given in order to illustrate that the methods presented here are correct.


\bibliography{references}
\bibliographystyle{plain}
\end{document}




% website for computer vision http://dblp.uni-trier.de/db/journals/ivc/ivc29.html


% Are you a vault dweller, cause you seem pretty S.P.E.C.I.A.L. to me





% https://drive.google.com/uc?export=download&confirm=TXgS&id=0B8ChoAcEOa4HbWxmbGFPV2M0aHM
% https://drive.google.com/uc?export=download&confirm=3xOY&id=0B8ChoAcEOa4HaEljZllabTZaeEk
% https://drive.google.com/uc?export=download&confirm=Lu4C&id=0B8ChoAcEOa4HcHZJMlZLeHdwdnc
% https://drive.google.com/uc?export=download&confirm=Qic1&id=0B2LiEl8up6X_R1V5NkF2NVhwaG8
% https://drive.google.com/uc?export=download&confirm=Qic1&id=0B2LiEl8up6X_R1V5NkF2NVhwaG8
% https://drive.google.com/uc?export=download&confirm=78XC&id=0B_q296AKMRsBUGtORW1jQ0lKQzQ


% https://drive.google.com/uc?export=download&confirm=cxmO&id=0B56v3YurenhzYTRXZGZ6MzJOWnc